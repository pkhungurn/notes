\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{verse}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\likelihood}{\mathcal{L}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\new}{\mathrm{new}}

\title{The EM Algorithm}
\author{Pramook Khungurn}

\begin{document}
	\maketitle	
	
	This is the ``untangled'' version of ``A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models'' by Jeff A. Blimes. Even the tutorial is very gentle to begin with, I had a hard time reading and understanding it without writing this document up as I read.
	
	\section{The Problem}
	
	\begin{itemize}
	  \item We have a density function $p(\ve{x}|\Theta)$ governed by a set of parameters $\Theta$.\\
	  We also have a data set $\mathcal{X} = \{ \ve{x}_1, \ve{x}_2$, $\dotsc$, $\ve{x}_N \}$ drawn i.i.d.~from the distribution.
	  
	  \item The density of the sampled data $\mathcal{X}$ is given by:
	  \begin{align*}
	    p(\mathcal{X}|\Theta) = \prod_{i=1}^N p(\ve{x}_i|\Theta).
    \end{align*}
    
    \item We define $\likelihood(\Theta|\mcal{X}) = p(\mcal{X}|\Theta)$ and view it as a function of $\Theta$.\\ We call it the \emph{likelihood} of the parameter $\Theta$.
    
    \item In the \emph{maximum likelihood problem}, we wish to find $\Theta$ that maximizes $\likelihood(\Theta|\X)$. That is, we want to find
    \begin{align*}
      \Theta^* = \arg\max_\Theta \likelihood(\Theta|\X).
    \end{align*}
    
    \item However, the EM algorithm tries to maximize $\log(\likelihood(\Theta|\X))$ instead because it's much easier.
    
    \item Depending on the form of $p(\X|\Theta)$, the problem can be easy or hard. The EM algorithm at least works well when $p$ is a mixture of Gaussians or Hidden Markov Model.	  
  \end{itemize}
  
  \section{General EM}
  \begin{itemize}
    \item The EM algorithm is a general method of finding the maximum-likelihood estimate of parameters of an underlying distribution from a given data set when the data is incomplete or has missing values.
    
    \item The algorithm has two common application scenarios:
    \begin{itemize}
      \item The data is actually missing due to the observation process.
      \item The likelihood function is analytically intractable but can be simplified by assuming additional but \emph{missing} hidden parameters.
    \end{itemize}
    
    \item $\X$ denotes the observed data, which we also call the \emph{incomplete data}.
    
    \item We assume a complete data set $\Z = (\X, \Y)$ exists. That is, each data item $\z_i$ has two components: $\x_i$ and $\y_i$, where $\x_i$ is the observed data and $\y_i$ is the hidden or missing data. 
    
    \item We also assume the complete data set is generated by a joint distribution function:
    \begin{align*}
      p(\z | \Theta) = p(\x, \y | \Theta) = p(\y|\x, \Theta) p(\x|\Theta).
    \end{align*}
    
    The joint density often ``arise'' from the marginal density $p(\x|\Theta)$ and the assumption of hidden variables and parameter value guesses.
    
    \item We can now define a likelihood function:
    \begin{align*}
      \likelihood(\Theta|\Z) = \likelihood(\Theta|\X,\Y) = p(\X,\Y|\Theta).
    \end{align*}
    This function is actually a random variable because the hidden data $\Y$ is unknown. Thus, we can think of the new likelihood function as
    \begin{align*}
      \likelihood(\Theta|\Z) = h_{\X,\Theta}(\Y)
    \end{align*}
    for some some function $h_{\X, \Theta}(\cdot)$ where $\X$ and $\Theta$ are constants and $\Y$ is a random variable.
    
    \item The orignal likelihood function $\likelihood(\Theta|\X)$ is called the \emph{incomplete-data likelihood function}.
    
    \item The EM algorithm is an iterative refinement algorithm. Suppose that we already have a parameter estimate $\Theta^{(i-1)}$. It will find a better estimate $\Theta^{(i)}$ and iterate this process until convergence.
    
    \item To find a better estimate, it computes the expected value of the complete-data log-likelihood function with respect to the unknown variable $\Y$ using the observed data $\X$ and the current parameter estmate $\Theta^{(i-1)}$:
    \begin{align*}
      Q(\Theta, \Theta^{(i-1)}) = E\big[ \log(\X, \Y | \Theta)\ \big|\ \X, \Theta^{(i-1)} \big].
    \end{align*}
    Here,
    \begin{itemize}
      \item $\Theta$ is a parameter we would like to adjust.
      \item $\Theta^{(i-1)}$ and $\X$ are constants.
      \item $\Y$ is a random variable which is distributed by some marginal distribution $f(\Y | \X, \Theta^{(i-1)})$.
    \end{itemize}
    The RHS can be rewritten as:
    \begin{align*}
      E\big[ \log(\X, \Y | \Theta)\ \big|\ \X, \Theta^{(i-1)} \big] = \int_{\Y \in \mathbb{Y}} \log\big( p(\X,\Y|\Theta) \big) f(\Y|\X,\Theta^{(i-1)})\ \dee\Y
    \end{align*}
    where $\mathbb{Y}$ is the set of all values $\Y$ can take on.
    
    \item In the best case, the marginal distribution $f$ is a simple expression.
    
    In the worst case, the density is hard to obtain.
    
    Often, the density used is actually is $f(\Y,\X|\Theta^{(i-1)}) = f(\Y|\X,\Theta^{(i-1)}) f(\X|\Theta^{(i-1)})$. This distribution does not affect subsequent step because $f(\X|\Theta^{(i-1)})$ is a constant.
    
    \item The determination of the expected value $Q(\Theta, \Theta^{(i-1)})$ is called the \emph{E-step} of the algorithm.
    
    \item The second step is to maximize the expectation we just computed. That is, the new parameter estimate is given by:
    \begin{align*}
      \Theta^{(i)} = \arg\max_\Theta Q(\Theta, \Theta^{(i-1)}).
    \end{align*}
    This step is called the \emph{M-step}.
    
    \item Each iteration of the algorithm consists of an E-step followed by an M-step.
    
    \item An iteration is guaranteed to increase the log-likelihood, and the algorithm is guaranteed to converge.
  \end{itemize}
  
  \section{Learning Mixtures Models by EM}
  \begin{itemize}
    \item We assume that the distribution $p$ is given by:
    \begin{align*}
      p(\x|\Theta) = \sum_{i=1}^M \alpha_i p_i(\x|\theta_i)
    \end{align*}
    where the parameters are $\Theta = (\alpha_1, \alpha_2, \dotsc, \alpha_M, \theta_1, \theta_2, \dotsc, \theta_M)$ with the constraint that $\sum_{i=1}^M \alpha_i = 1$. In other words, we have $M$ probability distributions that are mixed together with weights $\alpha_i$.
    
    \item The incomplete-data log-likelihood function for this density is given by:
    \begin{align*}
      \log(\likelihood(\Theta|\X))
      &= \log\bigg( \prod_{i=1}^N p(\x_i|\Theta) \bigg)
      = \sum_{i=1}^N \log(p(\x_i|\Theta))
      = \sum_{i=1}^N \log\bigg( \sum_{j=1}^M \alpha_j p_j(\x_i|\theta_j) \bigg)
    \end{align*}
    which is difficult to optimize because it contains the log of the sum.
    
    \item We now think of $\X$ as incomplete. We define the hidden data $\Y = (y_1, y_2, \dotsc, y_N)$ so that $y_i$ is the ``component'' which generates $\x_i$. In other words, $y_i \in \{ 1, 2, \dotsc, M \}$, and $y_i = k$ if the $i$th observed data was generated by the $k$th mixture component. Note that the set $\mathbb{Y}$ of values $\Y$ can take on is the set $\{1, 2, \dotsc, M\}^N$. 
    
    \item If we know the values of $\Y$, the likelihood becomes:
    \begin{align*}
      \log(\likelihood(\Theta|\X,\Y)) = \sum_{i=1}^N \log(\alpha_{y_i} p_{y_i}(\x_i|\theta_{y_i})).
    \end{align*}
    Depending on the form of the component densities, the above log-likelihood might be maximized easily.
    
    \item The problem is that we don't know $\Y$, so we assume that it is a random variable.
    
    \item Let $\Theta^g = (\alpha_1^g, \alpha_2^g, \dotsc, \alpha_M^g, \theta_1^g, \theta_2^g, \dotsc, \theta_M^g)$ be the current (guessed) parameter estimation. 
    
    Given $\Theta^g$, we can compute $p_j(\x_i| \theta_j^g)$ for each $i$ and $j$. The coefficient $\alpha_j$ can be thought of as a probability for component $j$. By Bayes's rule, we have that
    \begin{align*}
      p(y_i|\x_i, \Theta^g) 
      = \frac{\alpha_{y_i}^g p_{y_i}(\x_i|\theta^g_{y_i})}{p(\x_i|\theta^G)}
      = \frac{\alpha_{y_i}^g p_{y_i}(\x_i|\theta^g_{y_i})}{\sum_{k=1}^M \alpha_k^g p_k(\x_i|\theta^g_k)}.
    \end{align*}
    Moreover,
    \begin{align*}
      p(\Y|\X, \Theta^g) = \prod_{i=1}^N p(y_i|\x_i,\Theta^g).
    \end{align*}
    
    \item We now have the expression for $Q(\Theta, \Theta^g)$:
    \begin{align*}
      Q(\Theta,\Theta^g) 
      &= E[\log(\likelihood(\Theta|\X,\Y)\ |\ \X, \Theta^g]\\
      &= \sum_{\Y \in \mathbb{Y}} \log(\likelihood(\Theta|\X,\Y)) p(\Y|\X,\Theta^g)
      = \sum_{\Y \in \mathbb{Y}} \log(p(\X,\Y|\Theta)) p(\Y|\X,\Theta^g)\\
      &= \sum_{\Y \in \mathbb{Y}} \bigg( \sum_{i=1}^N \log(\alpha_{y_i} p_{y_i}(\x_i|\theta_{y_i})) \bigg) \bigg( \prod_{j=1}^N p(y_j|x_j,\Theta^g) \bigg)\\
      &= \sum_{\Y \in \mathbb{Y}} \bigg( \sum_{i=1}^N \log(\alpha_{y_i} p_{y_i}(\x_i|\theta_{y_i})) \prod_{j=1}^N p(y_j|x_j,\Theta^g) \bigg)\\
      &= \sum_{y_1=1}^M \sum_{y_2=1}^M \dotsb \sum_{y_N=1}^M \bigg( \sum_{i=1}^N \log(\alpha_{y_i} p_{y_i}(\x_i|\theta_{y_i})) \prod_{j=1}^N p(y_j|x_j,\Theta^g) \bigg)\\
      &= \sum_{y_1=1}^M \sum_{y_2=1}^M \dotsb \sum_{y_N=1}^M \bigg( \sum_{i=1}^N \sum_{\ell=1}^M \delta_{\ell,y_i} \log(\alpha_{\ell} p_{\ell}(\x_i|\theta_{\ell})) \prod_{j=1}^N p(y_j|x_j,\Theta^g) \bigg)\\
      &= \sum_{i=1}^N \sum_{\ell=1}^M \log(\alpha_{\ell} p_{\ell}(\x_i|\theta_{\ell})) \bigg( \sum_{y_1=1}^M \sum_{y_2=1}^M \dotsb \sum_{y_N=1}^M \delta_{\ell,y_i} \prod_{j=1}^N p(y_j|x_j,\Theta^g) \bigg)
    \end{align*}
    Now, note that
    \begin{align*}
      & \sum_{y_1=1}^M \sum_{y_2=1}^M \dotsb \sum_{y_N=1}^M \delta_{\ell,y_i} \prod_{j=1}^N p(y_j|x_j,\Theta^g)\\
      &= \bigg( \sum_{y_1=1}^M \sum_{y_2=1}^M \dotsb \sum_{y_{i-1}=1}^M \sum_{y_{i+1}=1}^M \dotsb \sum_{y_N=1}^M  \prod_{j\neq i} p(y_j|x_j,\Theta^g) \bigg) p(\ell|\x_i,\Theta^g)\\
      &= p(\ell|\x_i, \Theta^g) \prod_{j\neq i} \bigg( \sum_{y_j=1}^M p(y_j|x_j, \Theta^g) \bigg)\\
      &= p(\ell|\x_i,\Theta^g)
    \end{align*}
    because $\sum_{y_j=1}^M p(y_j|x_j, \Theta^g) = 1$. Thus, we have
    \begin{align} \label{expected-log}
      Q(\Theta, \Theta^g) 
      &= \sum_{i=1}^N \sum_{\ell=1}^M \log(\alpha_{\ell} p_{y_i}(\x_i|\theta_{\ell})) p(\ell|\x_i,\Theta^g) \notag \\
      &= \sum_{i=1}^N \sum_{\ell=1}^M \log(\alpha_{\ell}) p(\ell|\x_i,\Theta^g) + \sum_{i=1}^N \sum_{\ell=1}^M  \log(p_{\ell}(\x_i|\theta_{\ell}))p(\ell|\x_i,\Theta^g)
    \end{align}
    
    \item Now, we have to optimize the above expression. We can optimize the expression involving $\alpha_\ell$ and the one involving $\theta_\ell$ separately.
    
    \item To optimize $\alpha_\ell$, we introduce Lagrange multiplier $\lambda$ with the constraint that $\sum_\ell \alpha_\ell = 1$,
    and solve the following equation:
    \begin{align*}
      \frac{\partial}{\partial \alpha_\ell} \bigg[ \sum_{\ell=1}^M \sum_{i=1}^N \log(\alpha_\ell) p(\ell|\x_i, \Theta^g) + \lambda\bigg( \sum_\ell \alpha_\ell - 1\bigg) \bigg] = 0
    \end{align*}
    or
    \begin{align*}
      \sum_{i=1}^N \frac{1}{\alpha_\ell} p(\ell|\x_i,\Theta^g) + \lambda &= 0\\
      \sum_{i=1}^N p(\ell|\x_i,\Theta^g) + \alpha_\ell \lambda &= 0\\
      \sum_{\ell = 1}^M \sum_{i=1}^N p(\ell|\x_i,\Theta^g) + \sum_{\ell=1}^M \alpha_\ell \lambda &= 0\\
      \sum_{i=1}^N \sum_{\ell = 1}^M  p(\ell|\x_i,\Theta^g) + \lambda \sum_{\ell=1}^M \alpha_\ell  &= 0\\
      \sum_{i=1}^N 1 + \lambda &= 0\\
      \lambda &= -N.
    \end{align*}
    This means that
    \begin{align*}
      \alpha_\ell = \frac{1}{N} \sum_{i=1}^N p(\ell|\x_i,\Theta^g).
    \end{align*}
    
    \item Optimizing for $\theta_\ell$ depends on the form of $p_\ell$. An important special case where we can find $\theta_\ell$ analytically is given in the next section.
  \end{itemize}
  
  \section{Learning Gaussian Mixture Models with EM}
  
  \begin{itemize}
    \item In the Gaussian mixture model, the component density assumes the form:
    \begin{align*}
      p_\ell(\ve{x} |\mu_\ell, \Sigma_\ell) = \frac{1}{(2\pi)^{d/2}(\det \Sigma_\ell)^{1/2}} \exp\bigg( -\frac{1}{2} (\x - \mu_\ell)^T \Sigma_\ell^{-1} (\x - \mu_\ell) \bigg).
    \end{align*}
    Here, the parameters $\theta_\ell$ is $(\mu_\ell, \Sigma_\ell)$.
    
    \item We now wish to find the update expression for $\mu_\ell$ and $\Sigma_\ell$.
    
    \item We have that
    \begin{align} \label{gaussian-expectation}
      & \sum_{\ell=1}^M \sum_{i=1}^N  \log(p_{\ell}(\x_i|\theta_{\ell}))p(\ell|\x_i,\Theta^g) \notag\\
      &= \sum_{\ell=1}^M \sum_{i=1}^N \bigg( -\frac{1}{2} \log(\det \Sigma_\ell) - \frac{1}{2} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell) - \frac{d}{2}\log(2\pi) \bigg)p(\ell|\x_i,\Theta^g).
    \end{align}
    
    \item For $\mu_\ell$, we compute the following partial derivative:
    \begin{align*}
      & \frac{\partial}{\partial \mu_\ell} \bigg( \sum_{\ell=1}^M \sum_{i=1}^N \bigg( -\frac{1}{2} \log(\det \Sigma_\ell) - \frac{1}{2} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell) - \frac{d}{2}\log(2\pi) \bigg)p(\ell|\x_i,\Theta^g) \bigg) \\
      &= \frac{\partial}{\partial \mu_\ell} \bigg( \sum_{i=1}^N \bigg( - \frac{1}{2} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell)  \bigg)p(\ell|\x_i,\Theta^g) \bigg)\\
      &= - \frac{1}{2} \sum_{i=1}^N \bigg( \frac{\partial}{\partial \mu_\ell} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell) \bigg) p(\ell|\x_i,\Theta^g).
    \end{align*}
    Using Lemma~\ref{bilinear-deriv}, we have that
    \begin{align*}
      \frac{\partial}{\partial \mu_\ell} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell)
      &= (\Sigma^{-1} + \Sigma^{-T})(\x_i - \mu_\ell) \frac{\partial(\x_i - \mu_\ell)}{\partial \mu_\ell} = -2\Sigma_\ell^{-1}(\x_i - \mu_\ell).
    \end{align*}
    The simplification $\Sigma_\ell^{-1} + \Sigma_\ell^{-T} = 2\Sigma_\ell^{-1}$ comes from the fact that $\Sigma_\ell$ is a symmetric matrix, so is its inverse. So, we have that the partial derivative is given by:
    \begin{align*}
      - \frac{1}{2} \sum_{i=1}^N \bigg( \frac{\partial}{\partial \mu_\ell} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell) \bigg) p(\ell|\x_i,\Theta^g)
      &= \sum_{i=1}^N \Sigma_\ell^{-1}(\x_i - \mu_\ell) p(\ell|\x_i,\Theta^g).
    \end{align*}
    Setting the above expression equal to zero, we have:
    \begin{align*}
      \sum_{i=1}^N \Sigma_\ell^{-1}(\x_i - \mu_\ell) p(\ell|\x_i,\Theta^g) &= 0\\
      \Sigma_\ell^{-1} \sum_{i=1}^N (\x_i - \mu_\ell) p(\ell|\x_i,\Theta^g) &= 0\\
      \sum_{i=1}^N (\x_i - \mu_\ell) p(\ell|\x_i,\Theta^g) &= 0\\
      \sum_{i=1}^N \x_i p(\ell|\x_i,\Theta^g) - \sum_{i=1}^N  \mu_\ell p(\ell|\x_i,\Theta^g) &= 0\\
      \mu_\ell \sum_{i=1}^N p(\ell|\x_i,\Theta^g) &= \sum_{i=1}^N \x_i p(\ell|\x_i,\Theta^g)\\
      \mu_\ell &= \frac{\sum_{i=1}^N \x_i p(\ell|\x_i,\Theta^g)}{\sum_{i=1}^N   p(\ell|\x_i,\Theta^g)}.
    \end{align*}
    
    \item For $\Sigma_\ell$, we rewrite \eqref{gaussian-expectation}, dropping the term involing $d/2$ because it doesn't show up in the derivative.
    \begin{align*}
      & \sum_{\ell=1}^M \sum_{i=1}^N \bigg( -\frac{1}{2} \log(\det \Sigma_\ell) - \frac{1}{2} (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i - \mu_\ell) \bigg)p(\ell|\x_i,\Theta^g)\\
      &= \frac{1}{2}\sum_{\ell=1}^M \bigg( \log(\det(\Sigma_\ell^{-1})) \sum_{i=1}^N p(\ell|\x_i, \Theta^g) - \sum_{i=1}^N p(\ell|\x_i,\Theta^g) (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i- \mu_\ell)\bigg).
    \end{align*}    
    Now, using Lemma~\ref{bilinear-as-trace}, we rewrite the above expression further:
    \begin{align*}
      &\frac{1}{2}\sum_{\ell=1}^M \bigg( \log(\det(\Sigma_\ell^{-1})) \sum_{i=1}^N p(\ell|\x_i, \Theta^g) - \sum_{i=1}^N p(\ell|\x_i,\Theta^g) (\x_i - \mu_\ell)^T \Sigma_\ell^{-1} (\x_i- \mu_\ell)\bigg)\\
      &= \frac{1}{2}\sum_{\ell=1}^M \bigg( \log(\det(\Sigma_\ell^{-1})) \sum_{i=1}^N p(\ell|\x_i, \Theta^g) - \sum_{i=1}^N p(\ell|\x_i,\Theta^g) \tr\Big(\Sigma_\ell^{-1}(\x_i - \mu_\ell)(\x_i - \mu_\ell)^T\Big)\bigg)\\
      &= \frac{1}{2}\sum_{\ell=1}^M \bigg( \log(\det(\Sigma_\ell^{-1})) \sum_{i=1}^N p(\ell|\x_i, \Theta^g) - \sum_{i=1}^N p(\ell|\x_i,\Theta^g) \tr\Big(\Sigma_\ell^{-1}N_{\ell,i}\Big)\bigg)
    \end{align*}
    where $N_{\ell,i} = (\x_i - \mu_\ell)(\x_i - \mu_\ell)^T$.
    
    We take the derivative of the above expression with respect to $\Sigma_\ell^{-1}$, making use of Lemma~\ref{log-det-deriv} and Lemma~\ref{trace-deriv}:
    \begin{align*}
      &\frac{1}{2} \bigg( \sum_{i=1}^N p(\ell|\x_i,\Theta^g)(2\Sigma_\ell - \diag(\Sigma_\ell)) - \sum_{i=1}^N p(\ell|\x_i, \Theta^g)(2N_{\ell,i} - \diag(N_{\ell,i})) \bigg)\\      
      &= \frac{1}{2} \bigg(2\sum_{i=1}^N p(\ell|x_i,\Theta^g)(\Sigma_\ell - N_{\ell,i}) - \diag\bigg( \sum_{i=1}^N p(\ell|x_i,\Theta^g)(\Sigma_\ell - N_{\ell,i}) \bigg) \bigg)\\
      &= \frac{1}{2} (2S - \diag(S))
    \end{align*}
    where $S = \sum_{i=1}^N p(\ell|x_i,\Theta^g)(\Sigma_\ell - N_{\ell,i})$. Setting the derivative equal to zero, we have the equation $2S - \diag(S) = 0$ and consequently $S = 0$. Hence,
    \begin{align*}
      \sum_{i=1}^N p(\ell|x_i,\Theta^g)(\Sigma_\ell - N_{\ell,i}) 
      &= 0\\
      \Sigma_\ell \sum_{i=1}^N p(\ell|x_i,\Theta^g) &= \sum_{i=1}^N p(\ell|x_i,\Theta^g) N_{\ell,i}\\
      \Sigma_\ell &= \frac{\sum_{i=1}^N p(\ell|x_i,\Theta^g) N_{\ell,i}}{\sum_{i=1}^N p(\ell|x_i,\Theta^g)}
      = \frac{\sum_{i=1}^N p(\ell|x_i,\Theta^g) (\x_i - \mu_\ell)(\x_i - \mu_\ell)^T}{\sum_{i=1}^N p(\ell|x_i,\Theta^g)}.
    \end{align*}
    
    \item Hence, the update rules for learning mixture of Gaussians is:
    \begin{align*}      
      \alpha^{\new}_\ell &= \frac{1}{N} \sum_{i=1^N} p(\ell|\x_i, \Theta^g)\\
      \mu^{\new}_\ell &= \frac{\sum_{i=1}^N \x_i p(\ell|\x_i,\Theta^g)}{\sum_{i=1}^N p(\ell|\x_i,\Theta^g)}\\
      \Sigma^{\new}_\ell &= \frac{\sum_{i=1}^N p(\ell|x_i,\Theta^g) (\x_i - \mu_\ell^\new)(\x_i - \mu_\ell^\new)^T}{\sum_{i=1}^N p(\ell|x_i,\Theta^g)}.
    \end{align*}
    
  \end{itemize}
  
  \section{Some Matrix Identities}
  \begin{itemize}
    \item The last section used some matrix identities which might not be obvious. We prove them in this section.
    \item \begin{lemma} \label{xAx}
      $\x^T A \x = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j$
    \end{lemma}
    \begin{proof}
      \begin{align*}
        \x^T A X 
        &= \begin{bmatrix}
          x_1 & x_2 & \cdots & x_n
        \end{bmatrix}
        \begin{bmatrix}
          a_{11} & a_{12} & \cdots & a_{1n}\\
          a_{21} & a_{22} & \cdots & a_{2n}\\
          \vdots & \vdots & \ddots & \vdots\\
          a_{n1} & a_{n2} & \cdots & a_{nn}\\
        \end{bmatrix}        
        \begin{bmatrix}
          x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}\\
        &= \begin{bmatrix}
          x_1 & x_2 & \cdots & x_n
        \end{bmatrix}
        \begin{bmatrix}
          \sum_{j=1}^n a_{1j}x_j\\
          \sum_{j=1}^n a_{2j}x_j\\
          \vdots\\
          \sum_{j=1}^n a_{nj}x_j\\
        \end{bmatrix}\\
        &= x_1 \sum_{j=1}^n a_{1j}x_j + x_2 \sum_{j=1}^n a_{2j}x_j + \dotsb + x_n \sum_{j=1}^n a_{nj}x_j\\
        &= \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
      \end{align*}          
    \end{proof}
    
    \item \begin{lemma} \label{bilinear-deriv}
      $\frac{\partial (\x^T A \x)}{\partial \x} = (A + A^T) \x$
    \end{lemma}
    \begin{proof}
      Using Lemma~\ref{xAx}, we have that, for a fixed $k$,
      \begin{align*}
        \x^T Z \x = \sum_{i=1}^N \sum_{j=1}^N a_{ij}x_i x_j
        = a_{kk}x_k^2 + \sum_{j\neq k} a_{kj} x_k x_j + \sum_{i\neq k} \sum_{j=1}^n a_{ij} x_i x_j
      \end{align*}
      Thus,
      \begin{align*}
        \frac{\partial(\x^T A \x)}{\partial x_k} 
        &= 2a_{kk}x_k + \sum_{j \neq k} a_{kj} x_j + \sum_{i\neq k} a_{ik}x_i
        = \sum_{j=1}^n a_{kj} x_j + \sum_{i=1}^n a_{ik}x_i 
        = \sum_{i=1}^n (a_{ki} + a_{ik})x_i.
      \end{align*}
      Moreover, we have that
      \begin{align*}
        \frac{\partial(\x^T A \x)}{\partial \x} 
        &= \begin{bmatrix}
          \partial(\x^T A \x)/\partial x_1\\
          \partial(\x^T A \x)/\partial x_2\\
          \vdots\\
          \partial(\x^T A \x)/\partial x_n
        \end{bmatrix}
        = \begin{bmatrix}
          \sum_{i=1}^n (a_{1i} + a_{i1})x_i\\
          \sum_{i=1}^n (a_{2i} + a_{i2})x_i\\
          \vdots\\
          \sum_{i=1}^n (a_{ni} + a_{in})x_i
        \end{bmatrix}\\
        &= \begin{bmatrix}
        a_{11}+a_{11} & a_{12}+a_{21} & \cdots & a_{1n}+a_{n1}\\
        a_{21}+a_{12} & a_{22}+a_{22} & \cdots & a_{2n}+a_{n2}\\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1}+a_{1n} & a_{n2}+a_{2n} & \cdots & a_{nn}+a_{nn}\\
        \end{bmatrix}        
        \begin{bmatrix}
          x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}\\
        &= (A + A^T)\x.
      \end{align*}      
    \end{proof}    
    
    \item \begin{lemma} \label{bilinear-as-trace}
      $\x^T A \x = \tr(A \x \x^T)$
    \end{lemma}                   
    \begin{proof}
      We just have to show that $tr(A\x\x^T) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j$. Note that
      \begin{align*}
        A\x\x^T
        &= \begin{bmatrix}
          a_{11} & a_{12} & \cdots & a_{1n}\\
          a_{21} & a_{22} & \cdots & a_{2n}\\
          \vdots & \vdots & \ddots & \vdots\\
          a_{n1} & a_{n2} & \cdots & a_{nn}\\
        \end{bmatrix}
        \begin{bmatrix}
          x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}
        \begin{bmatrix}
          x_1 & x_2 & \cdots & x_n
        \end{bmatrix}\\
        &= \begin{bmatrix}
          a_{11} & a_{12} & \cdots & a_{1n}\\
          a_{21} & a_{22} & \cdots & a_{2n}\\
          \vdots & \vdots & \ddots & \vdots\\
          a_{n1} & a_{n2} & \cdots & a_{nn}\\
        \end{bmatrix}
        \begin{bmatrix}
          x_1 x_1 & x_1x_2 & \cdots & x_1x_n\\
          x_2 x_1 & x_2x_2 & \cdots & x_2x_n\\
          \vdots & \vdots & \ddots & \vdots\\
          x_n x_1 & x_nx_2 & \cdots & x_nx_n\\
        \end{bmatrix}.
      \end{align*}
      The diagonal members of the above matrix, from top to bottom, are:
      \begin{align*}
        \begin{matrix}
          a_{11}x_1x_1 + a_{12}x_2x_1 + \dotsb + a_{1n}x_nx_1\\
          a_{21}x_1x_2 + a_{22}x_2x_2 + \dotsb + a_{2n}x_nx_2\\
          \vdots\\
          a_{n1}x_1x_n + a_{n2}x_2x_n + \dotsb + a_{nn}x_nx_n.
        \end{matrix}
      \end{align*}
      If we add all of these up, we get $\sum_{i=1}^n \sum_{j=1}^n a_{ij}x_i x_j$ as required.
    \end{proof}  
    
    \item Let $f(A)$ be a function sending matrix $A \in \mathbb{R}^{n\times n}$ to a real number.\\ 
    We define the partial derivative $\partial f(A) / \partial A$ to be a matrix whose $(i,j)$-entry is equal to $\partial f(A) / \partial a_{ij}$.
    
    \item \begin{lemma}
      If $A$ is a general matrix (no relationship between the entries), then $\partial \det(A)/\partial a_{ij} = C_{ij}$ where $C_{ij}$ is the $(i,j)$-cofactor of $A$.
    \end{lemma}
    \begin{proof}
      Recall that $\det(A) = \sum_{i=1}^n a_{ij} C_{ij}$. Notice that no terms except $a_{ij}C_{ij}$ can contain $a_{ij}$. Moreover, $C_{ij}$ does not contain $a_{ij}$ itself. Hence, $\partial \det(A)/\partial a_{ij} = C_{ij}$.
    \end{proof}    
    
    \item \begin{lemma}
      If $A$ is a symmetric matrix, then
      \begin{align*}
        \frac{\partial \det(A)}{\partial a_{ij}}
        = \begin{cases}
          C_{ij}, & \mbox{if }i=j\\
          2C_{ij}, & \mbox{if }i \neq j
        \end{cases}        
      \end{align*}
      where $C_{ij}$ is the $(i,j)$-cofactor of $A$.
    \end{lemma}
    
    \begin{proof}      
      If $i = j$, recall that $\det(A) = \sum_{i=1}^n a_{ij} C_{ij}$. Notice that no terms $a_{ij}C_{ij}$ in the sum can contain $a_{ii}$ except for $a_{ii}C_{ii}$. Moreover, $C_{ii}$ does not contain $a_{ii}$ in itself. Therefore, $\partial \det(A) / \partial a_{ii} = C_{ii} = C_{ij}.$
      
      If $i \neq j$, we have that
      \begin{align*}
        \frac{\partial \det(A)}{\partial a_{ij}} 
        &= \frac{\partial}{\partial a_{ij}} \bigg( \sum_{\pi \in \mathfrak{S}_n} \sgn(\pi) \prod_{k=1}^n a_{k\pi(k)} \bigg)\\
        &= \frac{\partial}{\partial a_{ij}} \bigg( \sum_{\substack{\pi(i) = j\\\pi(j)=i}} \sgn(\pi) a_{ij}^2 \prod_{k\neq i,j} a_{k\pi(k)} 
        + \sum_{\substack{\pi(i) = j\\\pi(j)\neq i}} \sgn(\pi) a_{ij} \prod_{k\neq i} a_{k\pi(k)}\\
        &\phantom{\ = \ } + \sum_{\substack{\pi(i) \neq j\\\pi(j) =  i}} \sgn(\pi) a_{ij} \prod_{k\neq j} a_{k\pi(k)} +  \sum_{\substack{\pi(i) \neq j\\\pi(j) \neq  i}} \sgn(\pi) \prod_{k = 1}^n a_{k\pi(k)} \bigg)\\
        &= 2 \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) a_{ij} \prod_{k\neq i,j} a_{k\pi(k)} + \sum_{\substack{\pi(i) = j\\\pi(j)\neq i}} \sgn(\pi) \prod_{k\neq i} a_{k\pi(k)} + \sum_{\substack{\pi(i) \neq j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq j} a_{k\pi(k)}.
      \end{align*}
      where $\mathfrak{S}_n$ is the set of permutations over $\{1,2,\dotsc,n\}$. Note that
      \begin{align*}
        2 \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) a_{ij} \prod_{k\neq i,j} a_{k\pi(k)}
        &= \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq i} a_{k\pi(k)} + \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq j} a_{k\pi(k)}.
      \end{align*}
      Hence,
      \begin{align*}
      &2 \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) a_{ij} \prod_{k\neq i,j} a_{k\pi(k)} + \sum_{\substack{\pi(i) = j\\\pi(j)\neq i}} \sgn(\pi) \prod_{k\neq i} a_{k\pi(k)} + \sum_{\substack{\pi(i) \neq j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq j} a_{k\pi(k)}\\
      &= \bigg( \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq i} a_{k\pi(k)} + \sum_{\substack{\pi(i) = j\\\pi(j)\neq i}} \sgn(\pi) \prod_{k\neq i} a_{k\pi(k)} \bigg)\\
      &\phantom{\ =\ } + 
      \bigg( \sum_{\substack{\pi(i) = j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq j} a_{k\pi(k)} + \sum_{\substack{\pi(i) \neq j\\\pi(j) =  i}} \sgn(\pi) \prod_{k\neq j} a_{k\pi(k)} \bigg)\\
      &= \sum_{\pi(i) = j} \sgn(\pi) \prod_{k\neq j} a_{k\pi(k)}
      + \sum_{\pi(j) = i} \sgn(\pi) \prod_{k\neq i} a_{k\pi(k)}\\
      &= C_{ij} + C_{ji} = 2C_{ij}.
      \end{align*}
      The last line follows from the fact that $A$ is symmetric, so the cofactors are symmetric as well.
    \end{proof}
    
    \item \begin{corollary} \label{log-det-deriv-elem}
      If $A$ is a symmetric matrix, then
      \begin{align*}
        \frac{\partial \log(\det(A))}{\partial a_{ij}} =
        \begin{cases}
          C_{ij}/\det(A), & \mbox{if }i = j\\
          2C_{ij}/\det(A), & \mbox{if }i \neq j\\
        \end{cases}        
      \end{align*}
    \end{corollary}
    
    \item \begin{corollary} \label{log-det-deriv}
      If $A$ is a symmetric matrix, then 
      \begin{align*}
        \frac{\partial \log(\det(A))}{\partial A} = 2A^{-1}- \diag(A^{-1}).
      \end{align*}          
    \end{corollary}    
        
    \item \begin{lemma} \label{trace-deriv}
      If $A$ is symmetric, then
      $$\frac{\partial \tr(AB)}{\partial A} = B + B^T - \diag(B).$$
    \end{lemma}
    \begin{proof}
      Let the rows of $A$ be $a_1^T, a_2^T, \dotsc a_n^T$. The the columns of $B$ be $b_1, b_2, \dotsc, b_n$. We have that $\tr(AB) = \sum_{i=1}^n a_i^T b_i = \sum_{i=1}^n \sum_{j=1}^n a_{ij} b_{ji}$.
      
      Consider the expression $\frac{\partial \tr(AB)}{\partial a_{ij}}$. There are two cases:
      \begin{itemize}
        \item If $i = j$, then the coefficient of $a_{ij}$ in $\tr(AB)$ is $b_{ji} = b_{ii}$. Thus, the derivative with respect to $a_{ij}$ is $b_{ii}$.
        
        \item If $i \neq j$, then the coefficient of $a_{ij} = a_{ji}$ is $b_{ji} + b_{ij}$. Thus, the derivative with respect to $a_{ij}$ is $b_{ij} + b_{ji}$.
      \end{itemize}
      All in all, we have that the derivative with respect to $A$ is equal to $B + B^T - \diag(B)$.
    \end{proof}    
  \end{itemize}
  
\bibliographystyle{plain}
\bibliography{em}	

\end{document}