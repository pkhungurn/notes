\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\def\sc#1{\dosc#1\csod}
\def\dosc#1#2\csod{{\rm #1{\small #2}}}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\ves}[1]{\boldsymbol{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{A Primer on Markov Chain Monte Carlo Algorithms}
\author{Pramook Khungurn}

\begin{document}
\maketitle

This note is a primer on Markov chain Monte Carlo (MCMC) algorithms. I take materials from \cite{Andrieu:2003}, \cite{Gareth:2004}, and \cite{Neal:2011}.

\section{Introduction}

\begin{itemize}
  \item We are given an unnormalized density function $\pi_u$ defined on a measurable space $(\Omega,\Sigma)$ such that $0 < \int_\Omega \pi_u(x)\, \dee x < \infty$.
  
  \item The above density gives rise to a probability measure $P$, which is given by
  \begin{align*}
    P(A) = \frac{\int_A \pi_u( x)\, \dee x}{\int_\Omega \pi_u( x)\, \dee x}
  \end{align*}
  for any $A \in \Sigma$. The corresponding probability density function is
  \begin{align*}
    \pi( x) = \frac{\pi_u( x)}{\int_\Omega \pi_u( x)\, \dee x}.
  \end{align*}
  We collectively call $P$ and $\pi$ the {\bf target distribution}.

  \item MCMC algorithms allow us to perform the following two following tasks:
  \begin{itemize}
    \item Sample elements from $\Omega$ according to $P$.
    \item Compute an estimate of 
    \begin{align*}
    \pi(f) 
    = E_{ x \sim \pi}[f( x)]
    = \int_\Omega f( x) \pi( x)\, \dee  x
    = \frac{\int_\Omega f( x) \pi_u( x)\, \dee  x}{\int_\Omega \pi_u( x)\, \dee x}.
    \end{align*}
  \end{itemize}

  \item There are a number of motivations for doing this.
  \begin{itemize}
    \item In statistictical mechanics, the probability density of state $ x$ is given by
    \begin{align*}
      \pi( x) = \frac{1}{Z} \exp\bigg(-\frac{E( x)}{kT}\bigg)
    \end{align*}
    where $E( x)$ is the Hamiltonian of $s$, $k$ is the Boltzmann's constant, and $T$ is the temperature of the system. The constant $Z$ is called the {\bf partition function}, and it is given by
    \begin{align*}
      Z = \int_{\Omega} \exp\bigg( -\frac{E( x)}{kT} \bigg)\, \dee  x.
    \end{align*}    
    We see that this setting is exactly the same as the one we just discuss after taking $\pi_u( x) = e^{-E( x)/(kT)}$. So, MCMC is useful for computing expectations arising from these probabilities.

    \item In Bayesian parameter estimation, we are given data $x$, and we want to estimate parameter $\theta$ of our model, which includes the how to compute the prior $p(\theta)$, and the likelihood $p( x|x)$. The posterior is given by
    \begin{align*}
      p(\theta | x) = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta)\, \dee \theta}
    \end{align*}
    In many cases, we also want to compute expectations such as
    \begin{align*}
      E_{\theta \sim p(\theta|x)}[f(\theta)] 
      = \int f(\theta)p(\theta|x)\, \dee \theta
      = \frac{\int f(\theta)p(x|\theta)p(\theta)\, \dee\theta}{\int p(x|\theta)p(\theta)\, \dee \theta}.
    \end{align*}
    So, if we let $\Theta$ be the set of the $\theta$'s, then we may set $\Omega = \{ (\theta,x) : \theta \in \Theta \}$, and $\pi_u(\theta,x) = p(x|\theta)p(\theta)$ be our unnormalized density.
  \end{itemize}

  \item In the above situations, what prevents us from computing probabilities and integrals is the difficulty of computing the integral $\int_\Omega \pi_u( x)\, \dee x$.
  
  \item MCMC gives a way to sample $ x$ according to the target distribution without explicity normalizing $\pi_u( x)$. The idea is to constructs a Markov chain $\{ X_i \}_{i=0}^\infty$ whose stationary distribution is the target distribution $\pi(x)$. To do so, we need to find a transition kernel $K$, whose density is $k$, such that
  \begin{align*}
    \pi(y) = \int_\Omega \pi(x) k(x, y)\, \dee x.
  \end{align*}
  
  \item Then, we can run the Markov chain for a long time (starting from somewhere). When $n$ is large, the distribution of $X_n$ will be approximately $\pi$. We can now restart the Markov chain from $X_n$, run the chain for $M$ iterations to obtain $M$ samples $x_1$, $ x_2$, $\dotsc$, $ x_M$, and then compute the estimate
  \begin{align*}
    \int_\Omega f( x) \pi( x)\, \dee x \approx \widehat{\pi}(f) = \frac{1}{M} \sum_{i=1}^M f( x_i).
  \end{align*}
  This is an unbiased estimate, and the error $(\pi(f) - \widehat{\pi}(f))/\sqrt{M}$ has Gaussian distribution. We have this property despite the fact that the samples are correlated.\footnote{\url{https://en.wikipedia.org/wiki/Markov_chain_central_limit_theorem}}

  \item We say that a Markov chain $\{ X_i \}_{i=0}^\infty$ with kernel density function $k$ is {\bf reversible} with respect to probability density function $\pi$ if
  \begin{align} \label{eqn:detailed-balance}
    \pi(x) k(x,y) &= \pi(y) k(y,x) 
  \end{align}
  for all $x,y \in \Omega$. The condition \eqref{eqn:detailed-balance} is called {\bf detailed balance}.  

  \item Note that when $\pi$ satisfies detailed balance, we have that $\pi$ is a stationary distribution. This is because
  \begin{align*}
    \int_\Omega \pi(x) k(x, y)\, \dee x = \int_\Omega \pi(y)k(y, x)\, \dee x = \pi(y) \int_\Omega k(y, x)\, \dee x = \pi(y).
  \end{align*}

  \item Hence, the goal would be to find a kernel that satisfies detailed balance with respect to the target distribution. The most popular way to do so is the Metropolis--Hastings algorithm.
\end{itemize}

\section{The Metropolis--Hastings Algorithm}

\begin{itemize}
  \item The algorithm presupposes a Markov chain over $\Omega$. Let $Q$ denote its transition kernel, and let $q$ denote the transition kernel's density. This density can be unnormalized, just like $\pi_u$, but we require a way to sample $y$ according to $q(x,y)$ for any $x$. We often call $q$ the {\bf proposal distribution}.
  
  \item The algorithm proceeds as follows.
  \begin{enumerate}
    \item Choose some $X_0$ as the starting point.
    \item Given $X_n$, generates a proposal $Y_{n+1}$ according to $q(X_n, \cdot)$.
    \item Compute $\alpha(X_n, Y_{n+1})$ where
    \begin{align*}
      \alpha(x,y) = \min\bigg( 1, \frac{\pi_u(y)q(y,x)}{\pi_u(x)q(x,y)} \bigg)
    \end{align*}
    is the {\bf acceptance probability}.
    Here, if $\pi(x)q(x,y) = 0$, we set $\alpha(x,y) = 1$.
    \item Sample $\xi$ uniformly from $[0,1)$.
    \item If $\xi < \alpha(x,y)$, then set $X_{n+1} = Y_{n+1}$. Otherwise, set $X_{n+1} = X_n$.
    \item Repeat Step 2 to Step 5 until you are satisfied.
  \end{enumerate}

  \item The algorithm creates a Markov chain whose transition density is given by $k(x,y) = q(x,y)\alpha(x,y)$. 
  
  \item To see that $\pi$ is the stationary distribution of this above Markov chain, we need to show that $q^*$ satisfies detailed balance with respect to $\pi$. In other words,
  \begin{align*}
    \pi(x)k(x,y) = \pi(y)k(y,x).
  \end{align*}
  To see this, let $\pi(x) = C^{-1} \pi_u(x,y)$ where $C = \int_\Omega p_u(x)\, \dee x$ is the normalizing constant. We have that
  \begin{align*}
    \pi(x) k(x,y)
    &= \pi(x) q(x,y) \alpha(x,y) \\
    &= C^{-1}\pi_u(x) q(x,y) \min\bigg( 1, \frac{\pi_u(y)q(y,x)}{\pi_u(x)q(x,y)} \bigg)\\
    &= C^{-1} \min\bigg( \pi_u(x) q(x,y) , \pi_u(y)q(y,x) \bigg).    
  \end{align*}
  Notice that the expression on the RHS is symmetric in $x$ and $y$. Hence, we have that $\pi(x)k(x,y) = \pi(y)k(y,x)$ as required.

  \item The main question for employing the Metropolis--Hastings algorithm is how to choose the proposal distribution $q(\cdot, \cdot)$.
  
  \item Of course, $q$ needs to be such that the Markov chain is irreducible and aperiodic so that it would converge to the stationary distribution regardless of where we start.
    
  \item Note, though, that $q^*$ allows for rejection, so the chain is aperiodic by default. As a result, we only need to make sure that it covers the whole support of $\pi$. In other words, if $\pi_u(x) > 0$ and $\pi_u(y) > 0$, then $q(x,y)$ should be greater than $0$ too.  

  \item There are many approaches to construct the proposal distribution.
  \begin{itemize}
    \item {\bf Symmetric Metropolis algorithm.} Here, $q$ is chosen such that $q(x,y) = q(y,x)$. The acceptance probability simplifies to
    \begin{align*}
      \alpha(x,y) = \min\bigg( 1, \frac{\pi_u(y)}{\pi_u(x)} \bigg).
    \end{align*}
    One way to arrange for this is to use $y \sim \mcal{N}(x, \sigma^2)$.

    \item {\bf Random walk Metropolis algorithm.} Here, $q(x,y) = q(y-x)$. For example, we may have $y \sim \mrm{Uniform}(x-1, x+1)$.
    
    \item {\bf Independence sampler.} Here, $q(x,y) = q(y)$. The acceptance probability thus reduces to
    \begin{align*}
      \alpha(x,y) 
      = \min\bigg( 1, \frac{\pi_u(y)q(x)}{\pi_u(x)q(y)} \bigg)
      = \min\bigg( 1, \frac{w(y)}{w(x)} \bigg)
    \end{align*}
    where $w(x) = \pi_u(x)/q(x)$ for all $x$.

    \item {\bf Langevin algorithm.} The proposal distribution is given by
    \begin{align*}
      \ve{y} \sim \mcal{N}\Big(\ve{x} + \frac{\sigma}{2} \nabla \log \pi_u(\ve{x}), \sigma^2 I \Big) 
    \end{align*}
    for some small $\sigma > 0$.    
  \end{itemize}

  \item Heuristics on the standard deviation of the proposal distribution.
  \begin{itemize}
    \item If it is too low (i.e., the proposal distribution is too narrow), then only a few modes of the target distribution $\pi(\cdot)$ would be visited.
    \item If it is too wide, then there's a high chance of new samples being rejected, resulting in high correlation between samples.
  \end{itemize}

  \item If all modes of the target distribution is visited while the acceptance probability is high, we say that the chain {\bf mixes well}.
\end{itemize}

\section{Combining Markov Chains}

\subsection{Mixture MCMC}

\begin{itemize}
  \item If transition kernel density $k_1$ and $k_2$ both result in the same stationary distribution $\pi$, then so does the kernel $$\nu k_1 + (1 - \nu)k_2$$ for any $0 \leq \nu \leq 1$.
  
  \item The update step of the MCMC algorithm that uses the mixed kernel would be as follows:
  \begin{itemize}
    \item[] Sample $\xi \sim \mrm{Uniform}(0,1)$.
    \item[] {\bf if} $\xi < \nu$ {\bf then}
    \item[] $\qquad$ Generate the next state $X_{n+1}$ using kernel $k_1$.
    \item[] {\bf else}
    \item[] $\qquad$ Generate the next state $X_{n+1}$ using kernel $k_2$.
    \item[] {\bf end if}
  \end{itemize}

  \item Mixing more than two kernels is, of course, possible.

  \item Mixture MCMC is useful when the target distribution has many narrow peaks. In this case, we can have a ``global'' kernel that has large variance and a ``local'' kernel that has small variance. The global proposal would jump between peaks, and the local proposals would allow one to explore the space around each peak.
\end{itemize}

\subsection{Cycle MCMC}

\begin{itemize}
  \item If we have multiple transition kernel densities $k_1$, $k_2$, $\dotsc$, $k_m$ that have $\pi$ as the stationary distribution of their Markov chains, then the Markov chain obtained by applying the kernels in the round robin fashion, would also have $\pi$ as its stationary distribution.
  
  \item Another use of this type of MCMC algorithm is when a sample $\ve{x}$ can be divided into $m$ blocks $\ve{x} = (\ve{x}^{(1)} | \ve{x}^{(2)} | \dotsb | \ve{x}^{(m)})$. If the kernel $k_i$ only change the $\ve{x}^{(i)}$ block, then applying all the kernels in succession would give us a kernel that change all components of the sample, and this can be engineered to have $\pi$ as the statinoary distribution.
  
  \item The update step of the cycle MCMC algorithm be as follows:
  \begin{itemize}
    \item[] Use $k_1$ to get a state $Y^{(1)}_{n+1}$ from $X_n$.
    \item[] Use $k_2$ to get a state $Y^{(2)}_{n+1}$ from $Y^{(1)}_{n+1}$.
    \item[] $\qquad \vdots$
    \item[] Use $k_m$ to get a state $Y^{(m)}_{n+1}$ from $Y^{(1)}_{n+1}$. 
    \item[] Set $X_{n+1}$ to $Y^{(m)}_{n+1}$.
  \end{itemize}

  \item The expression for the overall kernel $k$ would be
  \begin{align*}
    k(x,y) = \int \int \dotsm \int k_1(x, x^{(1)})k_2(x^{(1)}, x^{(2)}) \dotsb k_m(x^{(m-1)}, y)\, \dee x^{(1)}\dee x^{(2)} \dotsm \dee x^{(m)}.
  \end{align*}
  However, if $k_i$ only changes the $i$th block independent of the other blocks, then we can write
  \begin{align*}
    k(\ve{x},\ve{y}) = \prod_{i=1}^m k_i(\ve{x}^{(i)},\ve{y}^{(i)}).
  \end{align*}
  Weirdly, all the papers that I read write the overall kernel as
  \begin{align*}
    k = k_1 k_2 \dotsb k_m.
  \end{align*}
\end{itemize}

\section{The Gibbs Sampler}

\begin{itemize}
  \item Suppose that $\Omega = \Real^d$. So, we denote an elements of $\Real^d$ by $\ve{x} = (x_1, x_2, \dotsc, x_d)$.
  
  \item Let $\ve{x}_{-i}$ denote $\ve{x}$ with the component $x_i$ removed. In other words,
  \begin{align*}
    \ve{x}_{-i} = (x_1, \dotsc, x_{i-1}, x_{i+1}, \dotsc, x_d) = (\ve{x}[1:i-1], \ve{x}[i+1:d]).
  \end{align*}

  \item Suppose that, in addition to an expression for $\pi_u(\ve{x})$, we are also given, for each $i$, an algorithm for sampling $x_i$ given all the other components (i.e., given $\ve{x}_{-i}$) according to the conditional probability distribution
  \begin{align*}
    \pi(x_i | \ve{x}_{-i}) = \frac{\pi_u(\ve{x})}{\int \pi_u(\ve{x}[1:i-1], x, \ve{x}[i+1,d])\, \dee x}.
  \end{align*}
  
  \item For further derivativation, let us denote the marginal distribution of $\ve{x}_{-i}$ by
  \begin{align*}
    \pi(\ve{x}_{-i}) = \frac{1}{C} \pi_u(\ve{x}_{-i}) = \frac{1}{C} \int \pi_u(\ve{x}[1:i-1], x, \ve{x}[i+1,d])\, \dee x.
  \end{align*}
  where $C = \int \pi_u(\ve{x}')\, \dee \ve{x}'$ is the normalizing constant. It follows that $$\pi(\ve{x}) = \pi(x_i|\ve{x}_{-i}) \pi(\ve{x}_{-i}),$$ which is just the standard Bayes' rule. Moreover, $$\pi_u(\ve{x}) = C\pi(\ve{x}) = C \pi(x_i|\ve{x}_{-i}) \pi(\ve{x}_{-i}) = \pi(x_i|\ve{x}_{-i}) \pi_u(\ve{x}_{-i}).$$
  
  \item The $i$th component Gibbs sampler is a kernel $k_i$ that leaves all components besides $x_i$ unchanged and replaces $x_i$ by a sample drawn from the above expression. In other words, the proposal distribution $q_i(\ve{x},\ve{y})$ is defined only when $\ve{x}_{-i} = \ve{y}_{-i}$, and it is given by
  \begin{align*}
    q(\ve{x},\ve{y}) = \pi(y_i | \ve{x}_{-i}).
  \end{align*}

  \item When used with the Metropolis--Hastings algorithm, the acceptance probability is
  \begin{align*}
    \alpha_i(\ve{x}, \ve{y}) 
    &= \min\bigg( 1, \frac{\pi_u(\ve{y}) q_i(\ve{y},\ve{x}) }{\pi_u(\ve{x})q_i(\ve{x},\ve{y}) } \bigg)
    = \min\bigg( 1, \frac{\pi_u(\ve{y}) \pi(x_i|\ve{y}_{-i}) }{\pi_u(\ve{x}) \pi(y_i|\ve{x}_{-i}) } \bigg) \\
    &= \min\bigg( 1, \frac{\pi_u(\ve{y}_{-i}) \pi(y_i|\ve{y}_{-i}) \pi(x_i|\ve{y}_{-i}) }{ \pi_u(\ve{x}_{-i}) \pi(x_i|\ve{x}_{-i}) \pi(y_i|\ve{x}_{-i}) } \bigg)
  \end{align*}
  Again, this is defined only when $\ve{x}_{-i} = \ve{y}_{-i}$. So,
  \begin{align*}
    \alpha_i(\ve{x}, \ve{y}) 
    = \min\bigg( 1, \frac{\pi_u(\ve{x}_{-i}) \pi(y_i|\ve{x}_{-i}) \pi(x_i|\ve{x}_{-i}) }{ \pi_u(\ve{x}_{-i}) \pi(x_i|\ve{x}_{-i}) \pi(y_i|\ve{x}_{-i}) } \bigg)
    = 1.
  \end{align*}
  This means that $k_i(\ve{x},\ve{y}) = q_i(\ve{x},\ve{y}) = \pi(y_i|\ve{x}_{-i})$ given that $\ve{x}_{-i} = \ve{y}_{-i}$.

  \item With the above property, we have that, if $\ve{x}_{-i} = \ve{y}_{-i}$, 
  \begin{align*}
    \pi(\ve{x}) k_i(\ve{x},\ve{y}) 
    &= \pi(x_i|\ve{x}_{-i}) \pi(\ve{x}_{-i}) \pi(y_i|\ve{x}_{-i}) 
    = \pi(x_i|\ve{y}_{-i}) \pi(\ve{y}_{-i}) \pi(y_i|\ve{y}_{-i})
    = \pi(x_i|\ve{y}_{-i}) \pi(\ve{y}) \\
    &= \pi(\ve{y}) k_i(\ve{y},\ve{x}).
  \end{align*}
  So, the kernel $k_i$ is reversiable with respect to the conditional probability distribution $\pi(\ve{x}|\ve{x}_{-i})$.
  
  \item The above derivation implies that $k_1$, $k_2$, $\dotsc$, $k_d$ can be combined to obtain a kernel $k$ that has $\pi$ as the stationary distribution. There are at least two ways to combine them.
  \begin{itemize}
    \item The {\bf deterministic-scan Gibbs sampler} is
    \begin{align*}
      k = k_1 k_2 \dotsm k_d.
    \end{align*} 

    \item The {\bf random-scan Gibbs sampler} is
    \begin{align*}
      k = \frac{k_1 + k_2 + \dotsb + k_d}{d}.
    \end{align*}
  \end{itemize}
  We note that the random-scan Gibbs sampler is reversible, but the deterministic-scan one is not. Moreover, for the deterministic scan one, $X_n$ is usable only when $d$ divides $n$.
\end{itemize}


\section{Hamiltonian Monte Carlo}

\begin{itemize}
  \item Hamiltonian Monte Carlo (HMC) is an algorithm that uses the gradient of the target distribution to create proposals, which allows a new proposal $Y_{n+1}$ to be far away from $X_n$ while still attaining high acceptance probability.
\end{itemize}

\subsection{Physical Background}

\begin{itemize}
  \item The algorithm uses ideas from analytical mechanics and statistical physics.
  
  \item In analytical mechanics, the state of a system can be described by its {\bf position vector} $\ve{x} \in \Real^d$ and its {\bf momentum vector} $\ve{p} \in \Real^d$. 
  
  \item The two vectors are related by
  \begin{align*}
    \ve{p} = M \frac{\dee \ve{x}}{\dee t} = M\dot{\ve{x}}.
  \end{align*}
  where $M$ is a positive definite matrix called the {\bf mass matrix}. Usually, the matrix is diagonal, and the mass value is often the same for all dimension. In such a case, we have that $$\ve{p} = m\dot{\ve{x}},$$ which is just the standard definition of momentum in physics.

  \item When the system is time symmetric, it has a conserved quantity, which is called the {\bf energy} or the {\bf Hamiltonian}.
\end{itemize}


\bibliographystyle{apalike}
\bibliography{mcmc-primer}  
\end{document}