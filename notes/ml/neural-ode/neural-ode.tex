\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mtt}[1]{\mathtt{#1}}
\newcommand{\ves}[1]{\boldsymbol{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Neural Ordinary Differential Equations}
\author{Pramook Khungurn}

\begin{document}
\maketitle

This is a note on the paper ``Neural Ordinary Differential Equations'' by Chen \etal \cite{Chen:2018}.

\section{Introduction}

\begin{itemize}
  \item Many existing neural networks models creates a sequence of hidden states $\ve{h}_0$, $\ve{h}_1$, $\ve{h}_2$, $\dotsc$ $\ve{h}_T$ by adding something to the previous state:
  \begin{align*}
    \ve{h}_{t+1} = \ve{h}_t + \ve{f}(\ve{h}_t, t, \ves{\theta})
  \end{align*}
  Such models include such as residual networks \cite{He:2015}, recurrent neural networks, and normalizing flows \cite{Rezende:2015,Dinh:2014}.

  \item What if we take the limit as the number of time step goes to infinity? We will have a differential equation:
  \begin{align*}
    \frac{\dee\ve{h}(t)}{\dee t} = \ve{f}(\ve{h}(t), t, \ves{\theta}).
  \end{align*}

  \item To use the network, we simply say that $\ve{h}(0)$ is the input layer, and the output is $\ve{h}(T)$ at some time $T$. The output can be found by solving the initial value problem, and this can be done by any black-box differential equation solver. 
\end{itemize}

\section{How to train a neural ODE model}

\begin{itemize}
  \item The problem with the above approach is that it is unclear how to train such a neural ODE model.
  \begin{itemize}
    \item The computation of the solution can require a lot of time steps. Differentiating through these time steps to compute the gradient would requires saving a lot of information in memory.
  \end{itemize}

  \item The good news is that there is a method to compute the gradient using constant memory (i.e., does not depend on the number of time steps). This is called the {\bf adjoint sensitivity method}. It requires, however, an ODE solve, which can be done, again, by any ODE solver.
\end{itemize}  
  
\subsection{Problem Setup}

\begin{itemize}
  \item Let the hidden state be a vector in $\Real^n$. We typically denote it by $\ve{z}$.
  
  \item Let the neural network's parameters be a vector in $\Real^m$, and we typically denote it by $\ves{\theta}$.
  
  \item We will work on a state space vector $\ve{r} = (\ve{z}, t, \ves{\theta}) \in \Real^{n+1+m}$.
  
  \item We will want to see how $\ve{r}$ evolves through time. We denote the $\ve{r}$ at time $t$ with $\ve{r}_t = (\ve{z}_t, t, \ves{\theta})$. Note that $\ves{\theta}$ does not vary with $t$.
  
  \item It also makes sense to talk about the function that sends $t$ to $\ve{r}_t$. We denote this by $\ve{R}: \Real \rightarrow \Real^{n+1+m}$, and we can write
  \begin{align*}
    \ve{r}_t = \ve{R}(t) = (\ve{Z}(t), T(t), \ves{\Theta}(t)) = (\ve{z}_t, t, \ves{\theta}).
  \end{align*}
  Note that $T$ is the identity function, and $\ves{\Theta}$ is a constant function.

  \item The act of solving the neural ODE is a function that maps $\ve{r}_{t}$ to some $\ve{r}_{t + \Delta t}$ for some $\Delta t \geq 0$. Let us denote this function by $\ve{s}_{\Delta t}^+: \Real^{n+1+m} \rightarrow \Real^{n+1+m}.$ (The letter $\ve{s}$ stands for ``solve.'') We have that
  \begin{align*}
    \ve{s}^+_{\Delta t}(\ve{z}_t, t, \ves{\theta})     
    = (\ve{z}_{t+\Delta}, t, \ves{\theta})
    = \begin{bmatrix}
      \ve{z}_{t + \Delta t} \\
      t + \Delta t \\
      \ves{\theta}
    \end{bmatrix}
    = \begin{bmatrix}
      \ve{z}_t + \int_{t}^{t+\Delta t} \ve{f}(\ve{z}_u, u, \ves{\theta})\, \dee u \\
      t + \Delta t \\
      \ves{\theta}
    \end{bmatrix}.
  \end{align*}

  \item The above function runs the ODE for a fixed time internal $\Delta t$. However, we can also talk about running the ODE until a fixed time $t_1$. We denote this by
  \begin{align*}
    \ve{s}^+_{\rightarrow t_1}(\ve{z}_t, t, \ves{\theta}) 
    = \ve{s}^+_{t_1 - t}(\ve{z}_t, t, \ves{\theta})
    = \begin{bmatrix}
      \ve{z}_t + \int_{t}^{t_1} \ve{f}(\ve{z}_u, u, \ves{\theta})\, \dee u \\
      t + \Delta t \\
      \ves{\theta}
    \end{bmatrix}.
  \end{align*}

  \item When optimizing a neural network, we need a loss function. In our case, the loss function is given by $L: \Real^{n+1+m} \rightarrow \Real$ that maps a state vector to a real number. When we write $L(\ve{r}) = L(\ve{z}, t, \ves{\theta})$, it is typical to say that the function only depends on $\ve{z}$, the produced hidden state. So, $$L(\ve{r}) = L(\ve{z},t,\ves{\theta}) = L(\ve{z}).$$ 
  
  \item When training a neural ODE, we start with the input state vector $\ve{r}_{t}$. We then solve the ODE to get the state $\ve{r}_{t_1}$. We then evaluate $L(\ve{r}_{t_1})$ to compute the loss. Let $\mcal{L}: \Real^{n+1+m} \rightarrow \Real$ be the function that maps the input state to the final loss. This function is thus given by
  \begin{align*}
    \mcal{L}(\ve{z}_t, t, \ves{\theta}) = L(\ve{s}^+_{\rightarrow t_1}(\ve{z}_t, t, \ves{\theta})).
  \end{align*}

  \item To train the neural network, we need the gradient
  \begin{align*}
    \nabla_{\S3} \mcal{L}(\ve{z}_{t_0}, t_0, \ves{\theta})
  \end{align*}
  where $t_0$ is the time we designate for the input, typically $0$. Here, we use the notations for multivariable derivatives from \cite{KhungurnDeriv} to avoid confusion. $\nabla_{\S3}\mcal{L}$ denotes the gradient with respect to the third block of arguments of $\mcal{L}$, which is the network parameters $\ves{\theta}$.  
\end{itemize}

\subsection{Adjoint Sensitivity Method}

\begin{itemize}
  \item Define the {\bf adjoint} to be the function $\ve{a}: \Real \rightarrow \Real^{1 \times (n+1+m)}$ such that
  \begin{align*}
    \ve{a}: t \mapsto \nabla \mcal{L}(\ve{z}_t, t, \ves{\theta}).
  \end{align*}
  In other words,
  \begin{align*}
    \ve{a}(t) = \mcal{L}(\ve{R}(t)) = L(\ve{s}^+_{\rightarrow t_1}(\ve{R}(t)))
  \end{align*}
  or $\ve{a} = \mcal{L} \circ \ve{R} = L \circ s_{\rightarrow t_1}^+ \circ \ve{R}$.

  \item With the adjoint function, our end goal is to evaluate $$\ve{a}_{\S3}(t_0) 
  = \ve{a}(t_0)[:,\S3] 
  = \nabla \mcal{L}(\ve{z}_{t_0}, t_0, \ves{\theta})[:, \S3] 
  = \nabla_{\S3}\mcal{L}(\ve{z}_{t_0}, t_0, \ves{\theta}).$$

  \item The adjoint sensivity method relies on the fact that we can express $\dee\ve{a} / \dee t$ in terms for $\ve{a}$ and $\ve{f}$.
  \begin{theorem}
  We have that
  \begin{align*}
    \frac{\dee \ve{a}(t)}{\dee t}
    = -\ve{a}(t)
    \begin{bmatrix}
      \nabla_{\S1}\ve{f}(\ve{z}_t, t, \ves{\theta})
      & \nabla_{\S2}\ve{f}(\ve{z}_t, t, \ves{\theta})
      & \nabla_{\S3}\ve{f}(\ve{z}_t, t, \ves{\theta}) \\
      \ve{0} & 0 & \ve{0} \\
      \ve{0} & \ve{0} & \ve{0}
    \end{bmatrix}
  \end{align*}
  In particular,
  \begin{align*}
    \frac{ \dee \ve{a}_{\S 1}(t)}{\dee t} &= -\ve{a}_{\S1}(t) \nabla_{\S1}\ve{f}(\ve{z}_t, t, \ves{\theta}), \\
    \frac{ \dee \ve{a}_{\S 3}(t)}{\dee t} &= -\ve{a}_{\S1}(t) \nabla_{\S3}\ve{f}(\ve{z}_t, t, \ves{\theta}). \\
  \end{align*}
  \end{theorem}

  \begin{proof}
    We have that 
    \begin{align*}
      \frac{\dee \ve{a}(t)}{\dee t} = \lim_{\varepsilon \rightarrow 0} \frac{\ve{a}(t + \varepsilon) + \ve{a}(t)}{\varepsilon}.
    \end{align*}
    To prove the theorem, we shall write $\ve{a}(t)$ in terms of $\ve{a}(t+\varepsilon)$.

    Consider the function $\mcal{L}$. We have that, for any $\varepsilon > 0$ such that $t + \varepsilon < t_1$, 
    \begin{align*}
      \mcal{L}(\ve{z}_t, t, \ves{\theta}) = \mcal{L}(\ve{z}_{t+\varepsilon}, t+\varepsilon, \ves{\theta}).
    \end{align*}
    This is because both $(\ve{z}_t, t, \ves{\theta})$ and $(\ve{z}_{t+\varepsilon}, t+\varepsilon, \ves{\theta})$ are on the trajectory to the final state vector $(\ve{z}_{t_1}, t_1, \ves{\theta})$. So, starting running the ODE from either points would lead to the same result. As a result, we may say that
    $$\mcal{L} = \mcal{L} \circ \ve{s}^+_\varepsilon$$
    if $\varepsilon$ is small enough. Applying the chain rule, we have that
    \begin{align*}
      \nabla \mcal{L}(\ve{z}_t, t, \ves{\theta})
      &= \nabla \mcal{L}(\ve{s}^+_\varepsilon(\ve{z}_t, t, \ves{\theta})) \nabla \ve{s}_{\varepsilon}^+(\ve{z}_t, t, \ves{\theta}) \\
      \nabla \mcal{L}(\ve{z}_t, t, \ves{\theta})
      &= \nabla \mcal{L}(\ve{z}_{t+\varepsilon}, t+\varepsilon, \ves{\theta}) \nabla \ve{s}_{\varepsilon}^+(\ve{z}_t, t, \ves{\theta}) \\
      \ve{a}(t) &= \ve{a}(t+\varepsilon) \nabla \ve{s}_{\varepsilon}^+(\ve{z}_t, t, \ves{\theta}).
    \end{align*}
    Now,
    \begin{align*}
      \ve{s}^+_\varepsilon(\ve{z}_t, t, \ves{\theta})
      &= \begin{bmatrix}
        \ve{z}_t + \int_{t}^{t+\varepsilon} \ve{f}(\ve{z}_u, u, \ves{\theta}) \, \dee u \\
        t + \varepsilon \\
        \ves{\theta}
      \end{bmatrix} 
      = \begin{bmatrix}
        \ve{z}_t + \varepsilon \ve{f}(\ve{z}_t, t, \ves{\theta}) + O(\varepsilon^2) \\
        t + \varepsilon \\
        \ves{\theta}
      \end{bmatrix} \\
      &= \begin{bmatrix}
        \ve{z}_t \\ t \\ \ves{\theta}
      \end{bmatrix} 
      + \varepsilon
      \begin{bmatrix}
        \ve{f}(\ve{z}_t, t, \ves{\theta}) \\
        1 \\
        \ve{0}
      \end{bmatrix} 
      + O(\varepsilon^2).      
    \end{align*}
    So,
    \begin{align*}
      \nabla \ve{s}^+_\varepsilon(\ve{z}_t, t, \ves{\theta})
      &= I + \varepsilon \begin{bmatrix}
        \nabla_{\S1}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S2}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S3}\ve{f}(\ve{z}_t, t, \ves{\theta}) \\
        \ve{0} & 0 & \ve{0} \\
        \ve{0} & \ve{0} & \ve{0}
      \end{bmatrix} + O(\varepsilon^2).
    \end{align*}
    This gives
    \begin{align*}
      \ve{a}(t) 
      &= \ve{a}(t + \varepsilon) 
      + \varepsilon \ve{a}(t + \varepsilon) \begin{bmatrix}
        \nabla_{\S1}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S2}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S3}\ve{f}(\ve{z}_t, t, \ves{\theta}) \\
        \ve{0} & 0 & \ve{0} \\
        \ve{0} & \ve{0} & \ve{0}
      \end{bmatrix} + O(\varepsilon^2),
    \end{align*}
    and so
    \begin{align*}
      \frac{\ve{a}(t+\varepsilon) - \ve{a}(t)}{\varepsilon} 
      &= -\ve{a}(t + \varepsilon) \begin{bmatrix}
        \nabla_{\S1}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S2}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S3}\ve{f}(\ve{z}_t, t, \ves{\theta}) \\
        \ve{0} & 0 & \ve{0} \\
        \ve{0} & \ve{0} & \ve{0}
      \end{bmatrix} + O(\varepsilon).
    \end{align*}
    Taking the limit as $\varepsilon \rightarrow 0$, we have that
    \begin{align*}
      \frac{\dee \ve{a}(t)}{\dee t}
      &= -\ve{a}(t) \begin{bmatrix}
        \nabla_{\S1}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S2}\ve{f}(\ve{z}_t, t, \ves{\theta})
        & \nabla_{\S3}\ve{f}(\ve{z}_t, t, \ves{\theta}) \\
        \ve{0} & 0 & \ve{0} \\
        \ve{0} & \ve{0} & \ve{0}
      \end{bmatrix}
    \end{align*}
    as required.
  \end{proof}
\end{itemize}

\bibliographystyle{alpha}
\bibliography{neural-ode}  
\end{document}