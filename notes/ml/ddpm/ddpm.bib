@article{SohlDickstein:2015,
  author    = {Jascha Sohl{-}Dickstein and
               Eric A. Weiss and
               Niru Maheswaranathan and
               Surya Ganguli},
  title     = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  journal   = {CoRR},
  volume    = {abs/1503.03585},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.03585},
  eprinttype = {arXiv},
  eprint    = {1503.03585},
  timestamp = {Mon, 13 Aug 2018 16:47:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Sohl-DicksteinW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ho:2020,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Song:2020,
  title={Denoising Diffusion Implicit Models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv:2010.02502},
  year={2020},
  month={October},
  abbr={Preprint},
  url={https://arxiv.org/abs/2010.02502}
}

@article{Vincent:2011,
author = {Vincent, Pascal},
title = {A Connection between Score Matching and Denoising Autoencoders},
year = {2011},
issue_date = {July 2011},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {23},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/NECO_a_00142},
doi = {10.1162/NECO_a_00142},
abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
journal = {Neural Comput.},
month = {jul},
pages = {1661â€“1674},
numpages = {14}
}

@misc{Mohamed:2016,
doi = {10.48550/ARXIV.1610.03483},

url = {https://arxiv.org/abs/1610.03483},

author = {Mohamed, Shakir and Lakshminarayanan, Balaji},

keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Learning in Implicit Generative Models},

publisher = {arXiv},

year = {2016},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Song:2022:sde,
  doi = {10.48550/ARXIV.2011.13456},
  
  url = {https://arxiv.org/abs/2011.13456},
  
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
