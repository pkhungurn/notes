\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\data}{\mathrm{data}}

\title{Flow Matching for Generative Modeling}
\author{Pramook Khungurn}

\begin{document}
\maketitle

This note was written as I read the ``Flow Matching for Generative Modeling'' paper by Lipman \etal~\cite{Lipman:2023}. In this document, we use both standard notations for partial derivatives and my notations for partial derivatives \cite{Khungurn:Notation}. The latter is used when we need want to avoid all ambiguities.


\section{Background}

\subsection{Flow and Neural ODE}

\begin{itemize}
  \item A data item is denoted by $x = (x^1, x^2, \dotsc, x^d) \in \Real^d$.  

  \item \begin{definition} 
    A {\bf time-dependent vector field} is a function $v: [0,1] \times \Real^d \rightarrow \Real^d$. 
  \end{definition}

  \item For a time dependent function $f: [0,1] \times \Real^d \rightarrow R$ for some range set $R$, we may write $f(t,x)$ as $f_t(x)$ to emphasize time dependence. Moreover, we can refer to $f_t: \Real^d \rightarrow R$ as a function in its own right.
  \begin{itemize}
    \item With this, for a time-dependent vector field $v$, we may write $v_t$ to mean the vector field obtained from $v$ when restricted to a given time.
  \end{itemize}  

  \item \begin{definition}
    A vector function $f: \Real^d \ra \Real^d$ is called a {\bf diffeomorphism} if $f$ is differentiable and bijective.
  \end{definition}
  \begin{itemize}
    \item If $f$ is a diffeomorphism, then so is $f^{-1}$.
    \item If $f$ is a diffeomorphism, then, for all $x \in \Real^d$, its Jacobian matrix $\nabla f(x)$ (my notation \cite{Khungurn:Notation}) is invertible. This means that $\det \nabla f(x) \neq 0$. 
  \end{itemize}

  \item \begin{definition}
    A {\bf flow} is a time-dependent vector field $\phi: [0,1] \times \Real^d \ra \Real^d$ such that
    \begin{itemize}      
      \item $\phi$ is differentiable.
      \item $\phi_0(x) = x$ for all $x$, and
      \item $\phi_t$, when viewed as a function of signature $\Real^d \ra \Real^d$, is a diffeomorphism for all $t$.      
    \end{itemize}
  \end{definition}    
  
  \item We note that we call such a function a flow because, for any $x$, the function $\Phi_{x}: t \mapsto \phi_t(x)$ traces a path of a particle that starts at $x$ and moves through space. So, $\phi$ indicates how particles ``flow'' through space. 
  
  \item If $\phi$ is a flow, we can interpret $\phi_t(x)$ as the position at time $t$ of the particle that starts at $x$.
  
  \item \begin{definition}
    Let $v$ be a time-dependent vector field and $\phi$ be a flow. We say that $v$ {\bf generates} $\phi$ if
    \begin{align}
      \frac{\partial}{\partial t} \phi_t(x) = v_t(\phi_t(x)). \label{eqn:flow}
    \end{align}
  \end{definition}
  In other words, $v_t$ acts as a velocity field that governs the direction and the speed that a particle at each position move.

  \item Note that we can rewrite Equation~\eqref{eqn:flow} using my notation \cite{Khungurn:Notation} as:
  \begin{align}
    \nabla_1 \phi_t(x) = v_t(\phi_t(x)). \label{eqn:flow-my-notation}
  \end{align}

  \item We note that, for any time-dependent vector field $v$, it generates a flow $\phi$ that is obtained by solving the differential equation $\partial \phi_t(x) / \partial t = v_t(\phi_t(x))$ with the initial condition $\phi_0(x) = x$.

  \item Chen \etal\ proposed the {\bf neural ordinary differential equation} model \cite{Chen:2019}. The idea is to model the vector field $v$ with a neural network $v_t(x;\theta)$. The vector field generates a flow $\phi_t$. The goal is to make $\phi_1$ have properties that we want.
  \begin{itemize}        
    \item If you want a refresher on neural ODE, then read my previous note on it \cite{Khungurn:neuralODE}.    
    \item Chen \etal's paper proposes a way to train $v_t(x;\theta)$. However, this training method involves integrating the vector field in each iteration, which means that optimization takes a long time. This is still incredible because the gradient of this process can be computed kind of easily.
  \end{itemize}

  \item The flow matching paper is here to offer another way to train a neural ODE without integrating the vector field. Howerver, the flow matching algorithm is specialized to the task of generative modeling, so its scope is narrower than the neural ODE paper.
\end{itemize}

\subsection{Generative Modeling as Transformation of Probability Distribution}

\begin{itemize}
  \item At a high level, generative modeling is about transforming a noise distribution $p_{\mrm{noise}}$ to a distribution of data items $p_{\data}$.
  
  \item Perhaps the easiest form of transforming one probability distribution to another is the following process.
  \begin{itemize}
    \item Sample a data item from the starting probability distribution.
    \item Transform the data item in some way.
    \item Return the transformed result to the usesr.
  \end{itemize}
  This process has a name.
  \begin{definition}
    Let $p: \Real^d \rightarrow \Real^{+}\cup \{0\}$ be a probability distribution. Let $f: \Real^d \rightarrow \Real^d$ be a diffeomorphism. A {\bf push-forward} or a {\bf change of variable} of $p$ according to $f$ is the probability distribution $q$ of elements $y$ created through the following process:
    \begin{itemize}
      \item Sample $x \sim p$.
      \item Compute $y = f(x)$.
    \end{itemize}
    Notationally, we write $q = [f]_* p$.
  \end{definition}

  \item \begin{lemma}
    Let $q = [f]_*p$. It follows that
    \begin{align*}
      q(y) = [f]_*p(y) = p(f^{-1}(y)) | \det \nabla f^{-1}(y) |,
    \end{align*}
    or
    \begin{align*}
      q(f(x)) = [f]_*p(f(x)) = \frac{p(x)}{|\det \nabla f(x)|}.
    \end{align*}    
  \end{lemma}
  Here, $\nabla f$ is a notation for the derivative (the Jacobian matrix) of $f$ according to my notation \cite{Khungurn:Notation}. A non-rigourous proof of this lemma can be found in my note on probability density under transformation \cite{Khungurn:ProbXform}. It is analogous to the change of variable formula in multi-variable calculus.

  \item \begin{definition} A {\bf probability density path} is a function $p: [0,1] \times \Real^d \rightarrow \Real^{+} \cup \{0\}$ such that each $p(t,\cdot)$ is a probabilty density function on $\Real^d$. In other words, it holds that
    \begin{align*}
      \int p(t,x)\, \dee x = 1
    \end{align*}
    for all $t \in [0,1]$.
    \end{definition}

  \item A flow $\phi_t$ can be used to transform one probability to another in a gradual sort of way.
  \begin{lemma}
    Let $p_0: \Real^d \ra \Real^{+} \cup \{0\}$ be a probability distribution and $\phi$ be a flow. The {\bf push-forward} or the {\bf change of variable} of $p_0$ according to $\phi$ is a probability path $p: [0,1] \times \Real^d \ra \Real^+ \cup \{0\}$ that is the result of the following process:
    \begin{itemize}
      \item Sample $x_0 \sim p_0$.
      \item Apply the flow to get $x_t = \phi_t(x)$.
      \item Let $p_t$ be the distribution of $x_t$.
    \end{itemize}
    It follows that $p_t = [\phi_t]_* p_0$. In other words,
    \begin{align}
      p_t(x') = p_0(\phi_t^{-1}(x'))\, |\det \nabla \phi_t^{-1}(x')| \label{eqn:cnf-prob-xform}
    \end{align}
    for all $x' \in \Real^d$.    
  \end{lemma}
  So, instead of getting just one probability distribution from $p_0$, we get an infinite number of distributions.  

  \item The formula in Equation~\eqref{eqn:cnf-prob-xform} is not that great because there is an issue about arity. We said earlier that $\phi_t$ can be viewed as a function of signature $\Real^d \ra \Real^d$. However, $\phi$ itself has signature $[0,1] \times \Real^{d} \ra \Real^d$, which means that it is function that maps a $(d+1)$-dimensional space to a $d$-dimensional space. So, we can treat it in the same way as a function of signature $\Real^{d+1} \ra \Real^d$. In other words, we can say that $\phi$ takes $d+1$ inputs. We can then divide the $d+1$ inputs into two blocks.
  \begin{itemize}
    \item The first block is the first argument alone. Using Python slice notation, it is ``$1:2$.'' Using my ``chapter'' notation, it can be abbreviated as $\S 1$.
    
    \item The second block is the rest of the arguments. Using Python slice notation, it is ``$2:d+2$.'' Using my ``chapter'' notation, it can be abbreviate as $\S 2$.
  \end{itemize}
  Hence, using my notation for partial derivatives, we can rewrite the equation as:
  \begin{align}
    p_t(x') = p_0(\phi_t^{-1}(x'))\, | \det \nabla_{\S 2} \phi^{-1}_t(x') |. \label{eqn:cnf-prob-xform-my-notation}
  \end{align}  

  \item \begin{definition} 
    If $p_t$ is a probability path that is a push-forward of $p_0$ according to flow $\phi_t$, and $v_t$ generates $\phi_t$, we say that $v_t$ {\bf generates} $p_t$.
  \end{definition}
  
  \item The neural ODE framework can be used to do generative modeling in the following way. We train a neural network to model a vector field $v_t(x;\theta)$ such that it generates a flow $\phi_t$ so that the push-forward of a noise distribution $p_0 = p_{\mrm{noise}}$ results in a probability path $p_t$ such that $p_1 = p_{\data}$.
  
  A neural ODE used in this way is called a {\bf continous normalizing flow} model.
\end{itemize}

\section{Flow Matching}

\subsection{Flow Matching Objective}

\begin{itemize}
  \item We want to use the above framework to transform a simple noise distribution $p_0 = p_{\mrm{noise}}$ to a data distribution $p_1 = p_{\mrm{data}}$.
  \begin{itemize}
    \item $p_{\mrm{noise}}$ is typically a Gaussian distribution $p_{\mrm{noise}} = \mcal{N}(0,I)$.
    \item As in most ML settings, we do not have access to the density function $p_{\mrm{data}}$, but we only have samples from the distribution.
  \end{itemize}

  \item Suppose we know a probability path $p_t$ and a time-dependent vector field $u_t$ that has the following property:
  \begin{itemize}
    \item $p_0$ is the desired noise distribution, and $p_1$ is the desired data distribution.
    \item $u_t$ generates $p_t$.
  \end{itemize}
  Suppose again that we want to model $u_t$ with a neural network $v_t(x; \theta)$. Then, we may do it my minimizing the {\bf flow matching objective}:
  \begin{align}
    \mcal{L}_{\mrm{FM}}(\theta) = E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}} \big[ \| u_t(x) - v_t(x; \theta) \|^2 \big]. \label{eqn:flow-matching-loss}
  \end{align}

  \item The flow maching objective is usable if we know $p_t$ and $u_t$ before hand. However, in our settings, we do not know anything about $u_t$, and we only know $p_0 = p_{\mrm{noise}}$ and $p_1 = p_{\mrm{data}}$ but nothing in between.
\end{itemize}

\subsection{Special Case: Single Item Dataset}

\begin{itemize}
  \item One of the difficulty we are facing right now is that $p_{\mrm{data}}$ can be quite complicated and that we only have access to its samples, not a function that can evaluate the density of sample from the distribution.
  
  \item So, let's start with a special case where the distribution can generate exactly one data item. Let us call this item $x_{\mrm{data}}$. 
  \begin{itemize}
    \item You know where this is going. We will later approximate $p_{\mrm{data}}$ as a mixture of the distributions of individual samples. So, stay tuned and work with this special case first.
  \end{itemize}
  
  \item The distribution $p_{\mrm{data}}$ is given by $p_{\mrm{data}}  = \delta(x_{\mrm{data}})$ where $\delta$ is the Dirac delta function.
  
  \item We want to derive a vector field $u_t$ that generates a probability distribution $p_t$ so that (1) $p_0 = p_{\mrm{noise}} = \mcal{N}(0,I)$ and $p_1 = p_{\mrm{data}} = \delta(x_{\mrm{data}})$.
  
  \item Unfortunately, I don't think there is a finite-time process that can turn a Gaussian distribution into a delta distribution. So, we will settle for an approximation. We instead require that $$p_1 = \mcal{N}(x_{\mrm{data}}, \sigma_{\min}^2 I )$$ where $\sigma_{\min}$ is a small positive constant.

  \item Since $u_t$ and $p_t$ we shall derive is specific to $x_{\mrm{data}}$, we may write them as ``conditional'' vector field and probability density, using the notation $u_t(\cdot|x_{\mrm{data}})$ and $p_t(\cdot|x_{\mrm{data}})$.
  \begin{itemize}
    \item Of course, this will be used later when we approximate of $p_{\mrm{data}}$ as a mixture of single item distributions.
  \end{itemize}

  \item In other words, we want to find a time-dependent vector field $u_t(\cdot|x_{\data})$ that generates a probability path $p_t(\cdot|x_{\data})$ such that
  \begin{itemize}
    \item $p_0(x|x_{\mrm{data}}) = \mcal{N}(x; 0, I)$, and
    \item $p_1(x|x_{\mrm{data}}) = \mcal{N}(x; x_{\data}, \sigma_{\min}^2 I)$.
  \end{itemize}

  \item We let $p_t(\cdot|x_{\data})$ take the form
  \begin{align}
    p_t(x|x_{\data}) = \mcal{N}(x;\mu_t(x_{\data}), \sigma_t(x_{\data})^2 I) \label{eqn:mu-sigma}
  \end{align}
  where $\mu: [0,1] \times \Real^d \ra \Real^d$ and $\sigma: [0,1] \times \Real \ra \Real^{+}.$ We will specify these two functions later, but there are many choises of them.

  \item To satsify the requirement on $p_0$, it must be the case that
  \begin{itemize}
    \item $\mu_0(x_{\data}) = 0$ for all $x_{\data}$, and
    \item $\sigma_0(x_{\data}) = 1$ for all $x_{\data}$.
  \end{itemize}
  Morever, to satisfy the requirment on $p_1$, it must be the case that
  \begin{itemize}
    \item $\mu_1(x_{\data}) = x_{\data}$ for all $x_{\data}$, and
    \item $\sigma_1(x_{\data}) = \sigma_{\min}$ for all $x_{\data}$.
  \end{itemize}

  \item Now that we have specified the form of $p_t(\cdot|x_{\data})$, it is now time to figure out the vector field $u_t(\cdot|x_{\data})$ that generates it. 
  
  \item We do so by first specifying a flow $\psi_t$ such that $p_t(\cdot|x_{\data}) = [\psi_t]_* p_0(\cdot|x_{\data})$. Then, we can define $u_t$ according to the equation
  \begin{align*}
    \nabla_1 \psi_t(x) = u_t(\psi_t(x)| x_{\data}).
  \end{align*}
  In other words,
  \begin{align}
    u_t(x'| x_{\data}) = \nabla_1 \psi_t(\psi_t^{-1}(x')). \label{eqn:single-item-vector-field}
  \end{align}

  \item Now, let's specify $\psi_t$. We use a very simple flow:
  \begin{align*}
    \psi_t(x) = \sigma_t(x_{\data}) x + \mu_t(x_{\data})
  \end{align*}
  Let's do some sanity check.
  \begin{itemize}
    \item At $t = 0$, we have that $\phi_0(x) = x$. So, $\phi_t$ satisfies the initial condition. The distribution $p_0$ is the distribution of $x$, which is $\mcal{N}(0,I)$ as required.
    \item At $t = 1$, we have that $\phi_1(x) = \sigma_{\min} x + x_{\data}$. Because $x \sim \mcal{N}(0,I)$, we have that $p_1 \sim \mcal{N}(x_{\data}, \sigma_{\min}^2I)$ as required too.
    \item At other values of $t$, we have that $\phi_t(x) = \sigma_t(x_{\data}) x + \mu_t(x_{\data})$. Again, because $x \sim \mcal{N}(0,I)$, we have that $\phi_t(x) \sim \mcal{N}(\mu_t{x_{\data}},\sigma_t(x_{\data})^2 I)$ as required again.
  \end{itemize}

  \item Let's derive $u_t(x'|x_{\data})$.
  \begin{itemize}
    \item First, we need to derive $\psi^{-1}_t(x')$. Let $x = \psi^{-1}_t(x')$. We have that
    \begin{align*}
      x' = \psi_t(x) = \sigma_{t}(x_{\data}) x + \mu_t(x_{\data}).
    \end{align*}
    So,
    \begin{align*}
      x = \frac{x' - \mu_t(x_{\data})}{\sigma_t(x_{\data})}.
    \end{align*}
    In other words,
    \begin{align}
      \psi^{-1}_t(x') = \frac{x' - \mu_t(x_{\data})}{\sigma_t(x_{\data})}. \label{eqn:single-item-flow-inverse}
    \end{align}

    \item Second, we need to derive the time-derivative $\nabla_1 \psi_t(x)$. This is also simple:
    \begin{align}
      \nabla_1 \psi_t(x) 
      &= \frac{\partial}{\partial t} \big[ \sigma_t(x_{\data}) x + \mu_t(x_{\data}) \big] \notag \\
      &= \bigg( \frac{\partial}{\partial t} \sigma_t(x_{\data}) \bigg) x + \frac{\partial}{\partial t} \mu_t(x_{\data}) \notag \\
      &= x \nabla_1 \sigma_t(x_{\data}) + \nabla_1 \mu_t(x_{\data}). \label{eqn:single-item-time-deriv}
    \end{align}
  \end{itemize}

  \item Substituting \eqref{eqn:single-item-flow-inverse} and \eqref{eqn:single-item-time-deriv} into \eqref{eqn:single-item-vector-field}, we have that
  \begin{align*}
    u_t(x'|x_{\data}) 
    &= \bigg( \frac{x' - \mu_t(x_{\data})}{\sigma_t(x_{\data})} \bigg) \nabla_1 \sigma_t(x_{\data}) + \nabla_1 \mu_t(x_{\data}) \\
    &= \frac{\nabla_1 \sigma_t(x_{\data})}{\sigma_t(x_{\data})}(x' - \mu_t(x_{\data})) + \nabla_1 \mu_t(x_{\data}).
  \end{align*}

  \item \begin{theorem}
    Suppose that we are given the following functions.
    \begin{itemize}
      \item Let $\sigma: [0,1] \times \Real^d \ra \Real^+$ be a differentable function such that $\sigma_0(x) = 1$ and $\sigma_1(x) = \sigma_{\min}$ for all $x$.
      \item Let $\mu: [0,1] \times \Real^d \ra \Real^d$ be a differentiable time-dependent vector field such that $\mu_0(x) = 0$ and $\mu_1(x) = x$ for all $x$.
    \end{itemize}
    Then, the vector field
    \begin{align*}
      u_t(x|x_{\data}) = \frac{\nabla_1 \sigma_t(x_{\data})}{\sigma_t(x_{\data})}(x - \mu_t(x_{\data})) + \nabla_1 \mu_t(x_{\data})
    \end{align*}
    generates the flow
    \begin{align*}
      \phi_t(x) = \sigma_t(x_{\data}) x + \mu_t(x_{\data})
    \end{align*}
    and a probablity path $p_t(\cdot|x_{\data})$ such that that $$p_t(\cdot|x_{\data}) \sim \mcal{N}(\mu_t(x_{\data}), \sigma_t(x_{\data})^2 I)$$ for all $t$. In particular, we have that
    \begin{enumerate}
      \item $p_0(\cdot|x_{\data}) \sim \mcal{N}(0,I)$, and 
      \item $p_1(\cdot|x_{\data}) \sim \mcal{N}(x_{\data}, \sigma_{\min}^2 I)$.      
    \end{enumerate}
    So, $u_t(\cdot|x_{\data})$ transforms a Gaussian noise distribution into an approximation of a single data distribution that only contains $x_{\data}$.
  \end{theorem}
\end{itemize}

\subsection{From single item distribution to multi-item distribution}

\begin{itemize}
  \item Now, we get back to the case where $p_{\data}$ is not a distrbution that output only a single item.
  
  \item Using the law of total proability, we can define the {\bf marginal probability path} as
  \begin{align}
    p_t(x) = \int p_t(x|x_1) p_{\mrm{data}}(x_{\data})\, \dee x_{\data}. \label{eqn:marginal-probability-path}
  \end{align}
  
  \item It follows that $p_0 = \mcal{N}(0,I)$, and 
  \begin{align*}
    p_1 = p_{\data} * \mcal{N}(0,\sigma_{\min}^2I)  \approx p_{\mrm{data}}.
  \end{align*}
  where $*$ is the convolution operation. So, we can use $p_1$ in place of $p_{\data}$ in many cases.

  \item Our task is now to find a time-dependent vector field $u_t$ that generates $p_t$. The paper argues that the following {\bf marginal vector field},
  \begin{align}
    u_t(x) = \int u_t(x|x_{\data}) \frac{p_t(x|x_{\data}) p_{\mrm{data}}(x_{\data})}{p_t(x)}\, \dee x_{\data}, \label{eqn:marginal-vector-field}
  \end{align}
  works.  

  \item \begin{theorem}
    The marginal vector field $u_t$ defined in Equation~\eqref{eqn:marginal-vector-field} generates the marginal probability path $p_t$ in Equation~\eqref{eqn:marginal-probability-path}.
  \end{theorem}
  \begin{proof}
    This proof makes heavy use of the continuity equation \eqref{eqn:continuity-no-ambiguity} and Theorem~\ref{thm:continuity-equation}.

    We showed in the last section that the conditional vector field $u_t(\cdot|x_{\data})$ generates the condtional probability path $p_t(\cdot|x_{\data})$. To make derivation easier, we shall write $p_t(x|x_{\data})$ as $p_{|x_{\data}}(t,x)$ and $u_t(x|x_{\data})$ as $u_{|\data}(t,x)$. With this, we have that these two functions satisfy the continuity equation
    \begin{align*}
      \nabla_1 p_{|x_{\data}}(t,x) + \sum_{i=1}^d \nabla_{i+1} (p_{|x_{\data}}u^i_{|x_{\data}})(t,x) = 0.
    \end{align*}
    In other words,
    \begin{align*}
      \nabla_1 p_{|x_{\data}}(t,x) = -\sum_{i=1}^d \nabla_{i+1} (p_{|x_{\data}}u^i_{|x_{\data}})(t,x).
    \end{align*}

    Now, recall the definition of $p(t,x)$.
    \begin{align*}
      p(t,x) = \int p_{|x_{\data}}(t,x) p_{\data}(x_{\data})\, \dee x_{\data}.
    \end{align*}
    Differentiating both sides with respect to the first argument ($t$), we have that
    \begin{align*}
      \nabla_1 p(t,x) 
      &= \int \nabla_1 p_{|x_{\data}}(t,x) p_{\data}(x_{\data})\, \dee x_{\data} \\
      &= \int \bigg( -\sum_{i=1}^d \nabla_{i+1} (p_{|x_{\data}}u^i_{|x_{\data}})(t,x) \bigg) p_{\data}(x_{\data})\, \dee x_{\data} \\
      &= -\sum_{i=1}^d \int \nabla_{i+1} (p_{|x_{\data}}u^i_{|x_{\data}})(t,x) p_{\data}(x_{\data})\, \dee x_{\data} \\
      &= -\sum_{i=1}^d \nabla_{i+1} \bigg( \int  p_{|x_{\data}}(t,x) u_{|x_{\data}}(t,x) p_{\data}(x_{\data})\, \dee x_{\data}\bigg) \\
      &= -\sum_{i=1}^d \nabla_{i+1} \bigg( \int  p_t(x|x_{\data}) u_t(x|x_{\data}) p_{\data}(x_{\data})\, \dee x_{\data}\bigg).
    \end{align*}
    By Equation~\eqref{eqn:marginal-vector-field}, we have that
    \begin{align*}
      u_t(x) p_t(x) = \int p_t(x|x_{\data}) u_t(x|x_{\data}) p_{\data}(x_{\data})\, \dee x_{\data}.
    \end{align*}
    As a result,
    \begin{align*}
      \nabla_1 p(t,x) &= -\sum_{i=1}^d \nabla_{i+1} \big( u_t(x) p_t(x) \big) \\
      \nabla_1 p(t,x) &= -\sum_{i=1}^d \nabla_{i+1} (u p)(t,x) \\
      \nabla_1 p(t,x) + \sum_{i=1}^d \nabla_{i+1} (u p)(t,x) &= 0.
    \end{align*}
    This shows that $u_t$ and $p_t$ satisfies the continuity equation, which implites that $u_t$ generates $p_t$.
  \end{proof}
\end{itemize}

\subsection{Conditional Flow Matching}

\begin{itemize}
  \item We have just identified the vector field $u_t$ that we can used in the flow matching loss \eqref{eqn:flow-matching-loss}. However, the problem is that $u_t$ is defined as an integral, and we do not want to compute it directly.
  
  \item Instead, we optimize the following {\bf conditional flow matching objective} where we sample $x_{\data}$ and trying to match $v_t(x;\theta)$ against $u_t(x|x_{\data})$. Here, $x$ is sampled from the conditional distribution $p_t(x|x_{\data})$.
  \begin{align*}
    \mcal{L}_{\mrm{CFM}}(\theta) = E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} [ \| u_t(x|x_{\data}) - v_t(x;\theta)\|^2].
  \end{align*}

  \item We can sample $x_{\data}$ easily because we can sample uniformly from the collections of samples we have at hand.
  
  \item We can also sample from $p_t(x|x_{\data})$ easily because $p_t(x|x_{\data}) = \mcal{N}(x; \mu_t{\data}(x_{\data}), \sigma_t(x_{\data})^2I)$.
  
  \item The only concern is whether the conditional flow matching objective $\mcal{L}_{\mrm{CFM}}(\theta)$ would yield the same $\theta$ as $\mcal{L}_{\mrm{FM}}(\theta)$ after optimization. The answer is yes.
  
  \item \begin{theorem}
    Assuming that $p_t(x) > 0$ for all $x \in \Real^d$ and $t \in [0,1]$, then,
    $$\mathcal{L}_{\mrm{FM}}(\theta) = \mcal{L}_{\mrm{CFM}}(\theta) + C$$ where $C$ is a constant independent of $\theta$. As a result, $$\nabla_{\theta} \mathcal{L}_{\mrm{FM}}(\theta) = \nabla_{\theta} \mcal{L}_{\mrm{CFM}}(\theta).$$
  \end{theorem}

  \begin{proof}
    We assume that all functions are well-behaved so that we can say that all integrals exist use the standard trick such as exchanging the order of integration (Fubini's theorem).

    We have that
    \begin{align*}
      \| v_t(x;\theta) - u_t(x) \|^2 &= \| v_t(x;\theta) \|^2 - 2 \langle v_t(x;\theta), u_t(x) \rangle + \| u_t(x) \|^2 \\
      \| v_t(x;\theta) - u_t(x|x_{\data}) \|^2 &= \| v_t(x;\theta) \|^2 - 2 \langle v_t(x;\theta), u_t(x|x_{\data}) \rangle + \| u_t(x|x_{\data}) \|^2.
    \end{align*}
    So,
    \begin{align*}
      \mcal{L}_{\mrm{FM}}(\theta) &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\| v_t(x;\theta) \|^2] - 2 E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\langle v_t(x;\theta), u_t(x) \rangle ] + E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\| u_t(x) \|^2] \\
      \mcal{L}_{\mrm{CFM}}(\theta) &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}}[\| v_t(x;\theta) \|^2] -2 E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} [\langle v_t(x;\theta), u_t(x|x_{\data}) \rangle] + E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} [\| u_t(x|x_{\data}) \|^2].
    \end{align*}

    Note that $u_t(x)$ and $u_t(x|x_{\data})$ does not depend on $\theta$. As a result, we can treat them as constants. In other words,
    \begin{align*}
      \mcal{L}_{\mrm{FM}}(\theta) &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\| v_t(x;\theta) \|^2] - 2 E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\langle v_t(x;\theta), u_t(x) \rangle ] + C_1 \\
      \mcal{L}_{\mrm{CFM}}(\theta) &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}}[\| v_t(x;\theta) \|^2] -2 E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} [\langle v_t(x;\theta), u_t(x|x_{\data}) \rangle] + C_2.
    \end{align*}

    Next, note that
    \begin{align*}
      E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\| v_t(x;\theta) \|^2] = E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}}[\| v_t(x;\theta) \|^2]
    \end{align*}
    because we still sample $x$ from the same distribution $p_t(x)$. On the LHS, we sample $x$ directly, but, on the RHS, we sample $x_{\data}$ before sampling $x$ given $x_{\data}$.

    Lastly,
    \begin{align*}
      & E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}}[\langle v_t(x;\theta), u_t(x) \rangle ] \\
      &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}} \Bigg[\bigg\langle v_t(x;\theta), \int u_t(x|x_{\data}) \frac{p_t(x|x_{\data})p_{\data}(x_{\data})}{p_t(x)}\, \dee x_{\data} \bigg\rangle \Bigg] \\
      &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x \sim p_t(x)}} \Bigg[ \int \langle v_t(x;\theta), u_t(x|x_{\data}) \rangle \frac{p_t(x|x_{\data})p_{\data}(x_{\data})}{p_t(x)}\, \dee x_{\data} \Bigg] \\
      &= E_{t \sim \mcal{U}([0,1])} \Bigg[ \int p_{t}(x) \bigg(\int \langle v_t(x;\theta), u_t(x|x_{\data}) \rangle \frac{p_t(x|x_{\data})p_{\data}(x_{\data})}{p_t(x)}\, \dee x_{\data}\,\bigg) \dee x \Bigg] \\
      &= E_{t \sim \mcal{U}([0,1])} \Bigg[ \int \int \langle v_t(x;\theta), u_t(x|x_{\data}) \rangle p_t(x|x_{\data})p_{\data}(x_{\data})\, \dee x_{\data}\, \dee x \Bigg] \\
      &= E_{t \sim \mcal{U}([0,1])} \Bigg[ \int p_{\data}(x_{\data}) \bigg( \int p_t(x|x_{\data}) \langle v_t(x;\theta), u_t(x|x_{\data}) \rangle \, \dee x\bigg) \, \dee x_{\data} \Bigg] \\
      &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} [\langle v_t(x;\theta), u_t(x|x_{\data}) \rangle].
    \end{align*}
    
    We are done.
  \end{proof}
\end{itemize}

\section{Conditional Vector Fields}

\begin{itemize}
  \item We said earlier that there are multiple ways to define $\mu_t$ and $\sigma_t$ in Equation~\eqref{eqn:mu-sigma}. We discuss them in this section.
\end{itemize}

\subsection{Diffusion Conditional Vector Fields}

\begin{itemize}
  \item For a diffusion model, time is the reverse of what we have been using in this note.
  \begin{itemize}
    \item $p_0$ is $p_{\data}$ or an approximation of it.
    \item $p_1$ is a noise distribution.
  \end{itemize}
  However, to make things simple, we will reverse the time so that it complies with what we have in this note.

  \item Variance-exploding case.
  \begin{itemize}
    \item Given $x_{\data}$, we have that $p_t(x|x_{\data}) = \mcal{N}(x; x_{\data}, \sigma_t^2 I )$ where $\sigma_1 = 0$ and $\sigma_0 \gg 1$. 
    \item Hence, $\mu_t(x_{\data}) = x_{\data}$, and $\sigma_t(x_{\data})$ does not depend on $x_{\data}$.
    \item As a result, the conditional vector field is given by
    \begin{align*}
      u_t(x|x_\data) = \frac{\sigma'_t}{\sigma_t} (x - x_{\data}).
    \end{align*}
  \end{itemize}

  \item Variance-preserving case.
  \begin{itemize}
    \item Given $x_{\data}$, we have that $p_t(x|x_{\data}) = \mcal{N}(x; \alpha_t x_{x_{\data}}, (1 - \alpha_t^2)I)$ where $\alpha_t: [0,1] \rightarrow \Real^{+} \cup \{ 0 \}$ is a real function such that $\alpha_0 = 0$ and $\alpha_1 = 1$.
    
    \item In other words, $\mu_t(x_{\data}) = \alpha_t x_{\data}$ and $\sigma_t(x_{\data}) = \sqrt{1-\alpha_t^2}$.
    
    \item The conditonal vector field is given by
    \begin{align*}
      u_t(x|x_{\data}) 
      &= \frac{\{ \sqrt{1 - \alpha_t^2} \}' }{\sqrt{1-\alpha_t^2}} (x - \alpha_t x_{\data}) + \alpha_t' x_{\data} \\
      &= \frac{-\alpha_t \alpha_t'}{1 - \alpha_t^2}(x - \alpha_t x_{\data}) + \alpha' x_{\data}.
    \end{align*}
  \end{itemize}
\end{itemize}

\subsection{Optimal Transport Conditional Vector Field}

\begin{itemize}
  \item We simply make $\mu_t$ and $\sigma_t$ linear functions of time.
  \begin{align*}
    \mu_t(x_{\data}) &= t x_{\data}, \\
    \sigma_t(x_{\data}) &= 1 - (1-\sigma_{\min})t.
  \end{align*}
  This leads to the conditional vector field
  \begin{align*}
    u_t(x|x_{\data}) = \frac{-(1-\sigma_{min})}{1 - (1-\sigma_{\min}) t} (x - t x_{\data}) + x_{\data} = \frac{x_{\data} - (1-\sigma_{\min})x}{1 - (1-\sigma_{\min}) t}.
  \end{align*}
  The conditional flow is given by:
  \begin{align*}
    \psi_t(x) 
    &= \sigma_t(x_{\data})x + \mu_t(x_{\data})    
    = (1 - (1-\sigma_{\min})t) x + tx_{\data}.
  \end{align*}
  The conditional flow matching objective is given by
  \begin{align*}
    \mcal{L}_{\mrm{CFM}}(\theta) 
    &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} [\| u_t(x|x_{\data}) - v_t(x;\theta) \|^2] \\
    &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x \sim p_t(x|x_{\data})}} \bigg[\bigg\| \frac{x_{\data} - (1-\sigma_{\min})x}{1 - (1-\sigma_{\min})t} - v_t(x;\theta) \bigg\|^2 \bigg].
  \end{align*}
  
  \item Let's rewrite the loss so that it becomes simpler. We have that $x \sim p_t(x|x_{\data})$ simply means that $x = (1-(1-\sigma_{\min})t)x_0 + tx_{\data}$ where $x_0 \sim p_0$. So,
  \begin{align*}
    &\mcal{L}_{\mrm{CFM}}(\theta) \\    
    &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x_0 \sim p_0}} \bigg[\bigg\| \frac{x_{\data} - (1-\sigma_{\min})((1 - (1-\sigma_{\min})t) x_0 + t{x_{\data}})}{1 - (1-\sigma_{\min})t} - v_t( (1-(1-\sigma_{\min}t))x_0 + t x_{\data};\theta) \bigg\|^2 \bigg] \\
    &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x_0 \sim p_0}} \bigg[\bigg\| \frac{(1 - (1-\sigma_{\min})t)(x_{\data} - (1-\sigma_{\min})x_0)}{1 - (1-\sigma_{\min})t} - v_t( (1-(1-\sigma_{\min}t))x_0 + t x_{\data};\theta) \bigg\|^2 \bigg] \\
    &= E_{\substack{t \sim \mcal{U}([0,1]),\\ x_{\data} \sim p_{\data},\\ x_0 \sim p_0}} \bigg[\bigg\| x_{\data} - (1-\sigma_{\min})x_0 - v_t( (1-(1-\sigma_{\min}t))x_0 + t x_{\data};\theta) \bigg\|^2 \bigg].
  \end{align*}

  \item The paper seems to make a great deal with the flow $\psi_t$ is the ``optimal transport displacement map'' between two Gaussians.
  \begin{itemize}
    \item Honestly, I found this to be very difficult to parse.
    \item The paper cites a paper by McCann \cite{McCann:1997}, but McCann's paper does not contain the word ``optimal transport'' or the word ``displacement map.''
  \end{itemize}

  \item McCann's paper defines something called the ``displacement interpolation.''
  \begin{definition}
    Let $p$ and $p'$ be two probability distributions on $\Real^d$ such that there exists a diffeomorphism $\varphi$ such that $p' = [\varphi]_* p$. Let $\mrm{id}: x \mapsto x$ denotes the identity mapping on $\Real^d$. The {\bf displacemenet interpolation} between $p$ and $p'$ is defined to be the probability path
    $$ p_t := [(1-t)\mrm{id} + t\varphi]_* p.$$
  \end{definition}
  In other words, to sample from $p_t$, one does the following
  \begin{itemize}
    \item Sample $x$ form $p$.
    \item Compute $x' = \varphi(x)$.
    \item Compute $x_t = (1-t) x + tx'$.
    \item Return $x_t$ to the user.
  \end{itemize}

  \item Look at our conditional flow $\psi_t(x) = (1-(1-\sigma_{\min})t) x + tx_{\data}$ again. We note that it transforms a Gaussian distribution $p_0 = \mcal{N}(0,I)$ to another Gaussian distribution $p_1 = \mcal{N}(x_{\data},\sigma_{\min}^2 I)$. Moreover, we can write it as a displacement interpolant between $p_0$ and $p_1$ because we can define
  \begin{align*}
    \varphi(x) = \sigma_{\min} x + x_{\data}.
  \end{align*}
  It is clear that $\varphi$ is a diffeomorphism if $\sigma_{\min} \neq 0$. It is also clear that $p_1 = [\varphi]_* p_0$. Lastly,
  \begin{align*}
    \psi_t(x) = (1 - (1-\sigma_{\min})t) x + tx_{\data} = (1 - t)x + t(\sigma_{\min}x + x_{\data}) = (1-t)x + t\varphi(x).
  \end{align*}
  So, indeed, $\psi_t$ is a displacement interpolation between two Gaussians.

  \item Where does optimal transport come from then?
  
  \item First, let us understand what an optimal transport problem is.
  
  \begin{definition}
    Given two probability distributions $p$ and $p'$ on $\Real^d$, a diffeomorphism $f: \Real^d \ra \Real$ is called an {\bf optimal transport plan} from $p$ to $p'$ if the following two conditions are satisfied.
    \begin{itemize}
      \item $p'$ is the push-forward of $p$ according to $f$. In other word, $p' = [f]_* p$.
      \item $f$ is a function that minimizes
      \begin{align*}
        E_{x \sim p}[c(x, f(x))] = \int p(x) c(x,f(x))\, \dee x
      \end{align*}
      for some cost function $c: \Real^d \times \Real^d \ra \Real^+ \cup \{0\}$.
    \end{itemize} 
  \end{definition}

  \item For the cost function $c(x,y) = \| x-y \|^2$, the optimal transport plan exists and is unique for non-pathological $p$ and $p'$.
  \begin{theorem}[Brenier's \cite{Brenier:1991}]
    Let the cost function $c(x,y) = \| x-y \|^2$. Let $p$ and $p'$ be well-behaved probability distributions (i.e., have finite moments and do not assign mass to sets with measure zero). Then, there exists a unique optimal transport plan $f$ from $p$ to $p'$. Moreover, there exists a convex scalar function $F:\Real^d \ra \Real$ such that $\nabla F = f$.
  \end{theorem}
  I'm using the form of the theorem from a Tweet by Gabriel Peyr\'{e} \cite{Peyre:BreinerTheoremTweet}.

  \item For two Gaussians, our diffeomorphism is $\varphi(x) = \sigma_{\min} x + x_{\data}$. Let
  \begin{align*}
    \Phi(x) = \frac{1}{2} \sum_{i=1}^d \bigg(\sqrt{\sigma_{\min}} x^i + \frac{x^i_{\data}}{\sqrt{\sigma_{\min}}} \bigg)^2.
  \end{align*}
  We have that
  \begin{align*}
    \nabla_i \Phi(x) = \sigma_{\min} x^i + x^i_{\data}.
  \end{align*}
  So,
  \begin{align*}
    \nabla \Phi(x) = \begin{bmatrix}
      \nabla_1 \Phi(x) \\
      \nabla_2 \Phi(x) \\
      \vdots \\
      \nabla_d \Phi(x)
    \end{bmatrix}
    = \begin{bmatrix}
      \sigma_{\min} x^1 + x^1_{\data} \\
      \sigma_{\min} x^2 + x^2_{\data} \\
      \vdots \\
      \sigma_{\min} x^d + x^d_{\data} 
    \end{bmatrix}
    = \sigma_{\min} x + x_{\data}
    = \varphi(x).
  \end{align*}
  By Brenier's theorem, $\varphi$ is the unique optimal transport plan that minimizes $\int p(x) \| x - \varphi(x) \|^2\, \dee x$.

  \item So, when the paper says $\phi_t$ is the ``optimal transport displacement map'' between two Gaussians, it means that
  \begin{itemize}
    \item There exists the unique optimal transport map $\varphi$ from the Gaussian $p_0 = \mcal{N}(0,I)$ to another Gaussian $p_1 = \mcal{N}(x_{\data},\sigma_{\min}^2 I)$.
    \item $\phi_t$ is the displacement interpolation between $p_0$ and $p_1$ with respect to $\varphi$.
  \end{itemize}

  \item The conditional flow $\phi_t(x) = (1 - (1-\sigma_{\min})t) x + tx_{\data}$ has nice properties.
  \begin{itemize}
    \item These properties are kind of evident by the form of the function itself. Don't be fooled by the fact that $\phi_t$ is ``optimal transport'' or anything. I think it's just a marketing gimmick to make it sound more impressive than it is.
    
    \item The first property is that, according to the flow, a particle that starts from $x$ moves in a straight line from $x$ towards $x_{\data}$.
    \begin{itemize}
      \item This is also true for variance-preserving diffusion conditional flows.
    \end{itemize}

    \item The second property is that the velocity at which each particle moves is constant throughout the movement duration. A particle that starts at $x$ always move at velocity $x_{\data} - (1-\sigma_{\min}) x$.
    \begin{itemize}
      \item This property is not true for diffusion conditional flows in general.
    \end{itemize}
  \end{itemize}
  
  \item The paper claims that ``sampling trajectory from diffusion paths can `overshoot' the final sample, resulting in unnecessary backtracking, whilst the OT paths are guaranteed to stay straight.''
  \begin{itemize}
    \item WTF are the authors talking about?
    \item Diffusion conditional paths are straight too. They don't overshoot whatsover. See Figure 2. Every paths are straight.
    \item If they talk about unconditonal sampling paths, then there is no gaurantee that the uncondional paths according to this formulation is going to be straight. See the rectified flow paper \cite{Liu:2022}.
  \end{itemize}

  \item The paper claims that ``An interesting observation is that the OT VF has a constant direction in time, which arguably leads to a simpler regression task.''
  \begin{itemize}
    \item This is plausible, but take it with grain of salt.
  \end{itemize}

  \item Also, keep in main that ``we note that although the conditional flow is optimal, this by no means imply that the marginal VT is an optimal transport solution.''
  \begin{itemize}
    \item The rectified flow paper \cite{Liu:2022} has more things to say in terms of optimal transport properties of the marginal VT.
  \end{itemize}
\end{itemize}

\section{Experiments}

\begin{itemize}
  \item The paper trains generative models on the following datasets.
  \begin{itemize}
    \item CIFAR-10 at $32\times32$.
    \item ImageNet at $32 \times 32$, $64 \times 64$ and $128 \times 128$.
  \end{itemize}
  
  \item The following generative models were trained:
  \begin{itemize}
    \item Vanilla DDPM \cite{Ho:2020}. I believe this is a noise-predicting network with linear-$\beta$ schedule.
    \item Score matching \cite{Song:SDE:2021}. I believe this is a diffusion model with variance-exploding noise schedule.
    \item ScoreFlow \cite{Song:ScoreFlow:2021}. I have not read this paper, so I don't have a good idea what it is about.
    \item Flow matching with diffusion conditional path (variance preserving).
    \item Flow matching with optimal transport conditional path.
  \end{itemize}

  \item It seems all the generative models use the ADM architecture because the hyperparameter table is that of the ADM architecture.
  
  \item Evaluation metrics.
  \begin{itemize}
    \item FID
    \item NLL $=$ natgative log likelihood
    \item NFE = ``number of function evaluations for an adaptive solver to reach its prespecified numerical tolerance.''
    \begin{itemize}
      \item Actually, I don't know exactly what this means. This is the first time I see this metric.
      \item My guess is that, for a solver like Euler's method, the output converges as you increase the number of steps.
      \item This might measure the number of steps $N$ such that the difference between a sample generated with $N$ steps and another sample generated with $N+1$ step is below a certain threshold.
      \item This metrics would take a very long time to evaluate.
    \end{itemize}
  \end{itemize}

  \item In Table 1 of the paper, it seems that FM with OT conditional path beats all other generative models in all metrics for all datsets.
  
  \item Paper comments that flow matching models converge much faster than other models on ImageNet 64.
  
  \item The paper presents evidence that flow matching models attent higher FID scores with lower NFEs. (Figure 7.)
  
  \item Lastly, the paper trains an image super-resolution model with flow matching and compares it to SR3. They found that they beated SR3 on the FID score but not image similarity metrics (PSNR and SSIM).
\end{itemize}

\appendix

\section{Continuity Equation}

\begin{itemize}
  \item The continuity equation is an equation from fluid dynamics that shows up a lot in fields that involves transport pheonomena.
  
  \item Here, we start with a probability density function $p_0$, which tells us how the probability mass is distributed over $\Real^d$. Then, we have a vector field $v_t$ that gives us the time-dependent velocity field that governs how the mass at each point should move. The velocity field generates a probability path $p_t$.
  
  \item Note that, because the velocity field just move the mass around, there is no new mass added or no new mass being dropped. It follows that the total probability mass remains constant. It is \emph{conserved}. This means that $p_t$ is a probability distribution for all $t$. If you integrate it over $\Real^d$, you should get $1$.
  
  \item Given a probability path $p_t$ and a time-dependent velocity field $v_t$ that generates it, the {\bf flux density} $j: [0,1] \times \Real^d \ra \Real^d$ is defined as:
  \begin{align*}
    j_t(x) = p_t(x) v_t(x).
  \end{align*}
  It is just the velocity field weighted by the probability density.

  \item Let $f: \Real^d \ra \Real^d$ be a vector field. The {\bf divergence} of $f$ is a scalar function $\nabla \cdot f: \Real^d \ra \Real$ define by
  \begin{align*}
    (\nabla \cdot f)(x) = \sum_{i=1}^d \nabla_i f^i(x)
  \end{align*}
  where $f^i(x)$ denotes the $i$th component of $f(x)$.

  \item The {\bf continuity equation} of $p_t$ and $v_t$ is given by
  \begin{align*}
    \frac{\partial}{\partial t} p_t(x) + (\nabla \cdot j_t)(x) = 0.
  \end{align*}
  In the above equation, we view $j_t$ with $t$ fixed as a vector field of signature $\Real^d \ra \Real^d$.

  \item Let's rewrite the continuity equation with my notation and taking into account correct indexing with no ambiguity whatsoever. We have
  \begin{align}
    \nabla_1 p(t,x) + \sum_{i=1}^{d} \nabla_{i+1} (p v^i)(t,x) = 0. \label{eqn:continuity-no-ambiguity}
  \end{align}
  
  \item In our context, the continuity equation is useful because of the following theorem.
  \begin{theorem} \label{thm:continuity-equation}
    Let $p: [0,1] \times \Real^d \rightarrow \Real^+ \cup \{0\}$ be a differentiable probability path. Let $v: [0,1] \times \Real^d \ra \Real^d$ be a differentiable time-dependent vector field. Then, $v_t$ generates $p_t$ if and only if the continuity equation holds.
  \end{theorem}

  \item We shall not prove the theorem in this note, but I will be studying it more.
  
\begin{comment}
  \item Let $h: \Real^d \rightarrow \Real$ be any compactly supported differentiable function. Consider the time derivative of the following expectation
  \begin{align*}
    \frac{\partial}{\partial t} E_{x \sim p_0} [h(\phi_t(x))]
  \end{align*}
  where $\phi: [0,1] \times \Real^d \ra \Real^d$ is the flow generated by $v_t$. We have that
  \begin{align*}
    \frac{\partial}{\partial t} E_{x \sim p_0} [h(\phi_t(x))]
    &= \frac{\partial}{\partial t} \int p_0(x) h(\phi_t(x)) \, \dee x \\
    &= \int p_0(x) \frac{\partial}{\partial t} h(\phi_t(x)) \, \dee x \\
    &= \int p_0(x) \bigg( \sum_{i=1}^d \nabla_i h(\phi_t(x)) \nabla_1 \phi^i_t(x) \bigg) \, \dee x \\
    &= \int p_0(x) \bigg( \sum_{i=1}^d \nabla_i h(\phi_t(x)) v_t^i(\phi_t(x)) \bigg) \, \dee x \\
  \end{align*}
\end{comment}

\end{itemize}


\bibliographystyle{acm}
\bibliography{flow-matching}  
\end{document}