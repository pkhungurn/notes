\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\ves}[1]{\boldsymbol{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Score-Based Generative Models}
\author{Pramook Khungurn}

\begin{document}
\maketitle

In 2021, I read a paper on denoising diffusion models, which proposes a new type of generative models \cite{Ho:2020}. Now, it turns out that there are parallel works by Yang Song and Stefano Ermon on the so-called ``score-based models,'' which is later discovered to be deeply connected (in other words, pretty much equivalent) to the former approach \cite{Song:2019}.\footnote{FYI, my time in grad school overlapped with that of Stefano's, but we never interacted.}The approaches share the advantanges of being very stable to train and being capable of generating high quality samples, and they also share the disadvantage of being slow when sampling. Researchers have claimed that these models beat GANs in image generation \cite{Dhariwal:2021}, and so my interested is piqued.

I read the introductory blog post by Yang Song \cite{Song:2022}, but it seems that I lack the background to understand this body of work. This note aims to fill this understanding gap by summarizing relavant research papers.

\section{Preliminary}

\begin{itemize}
  \item We are given $n$ data items $\ve{x}_1$, $\ve{x}_2$, $\dotsc$, $\ve{x}_N$ that are sampled i.i.d. from a probability distribution $p_{\mrm{data}}(\ve{x})$, which is unknown to us.
  
  \item We are interested in modeling $p_{\mrm{data}}(\ve{x})$ by finding a model $p_{\boldsymbol{\theta}}(\ve{x})$ with parameters $\boldsymbol{\theta}$ that best approximates it.
  \begin{itemize}
    \item To reduce levels of subscription, we will sometimes write $p_{\boldsymbol{\theta}}(\ve{x})$ as $p(\ve{x};\boldsymbol{\theta})$.
  \end{itemize}

  \item For the models in this note, we cannot compute the probability $p_{\boldsymbol{\theta}}(\ve{x})$ directly.
  
  \item However, we would still be able to sample from it, which is something that is of practical use.
  
  \item Hence, the focus would be on (1) how to estimate the parameters $\boldsymbol{\theta}$, and (2) how to sample from the model given the parameters.
\end{itemize}

\section{Score Matching [Hyv\"{a}rinen, 2005]}

\begin{itemize}
  \item First, however, we need to take a detour from generative modeling and study a related problem: parameter estimation of unnormalized models.
  
  \item For some probabilistic models, the form of the probability distribution is known up to the normalization constant:
  \begin{align*}
      p(\ve{x}) \propto q(\ve{x})
  \end{align*}
  So, we have that
  \begin{align*}
      p(\ve{x}) = \frac{q(\ve{x})}{Z}.
  \end{align*}
  where $$Z = \int p(\ve{x})\, \dee\ve{x}$$ is the normalization constant.

  \item A common class of such model is the {\bf energy-based model}, where $$p(\ve{x}) \propto e^{-E(\ve{x})}.$$ Here, $E(\ve{x})$ is called the {\bf energy function}, and the normalization constant
  \begin{align*}
  Z = \int e^{-E(\ve{x})}\, \dee\ve{x}
  \end{align*}
  is called the {\bf partition function}. Energy-based models show up a lot in statistical physics.

  \item Another class of such models is the {\bf graphical model} where $$p(\ve{x}) \propto \prod_{\ve{a} \in \mathcal{F}} p_\ve{a}(\ve{x}).$$ Here, we assume that $\ve{x} = (x_1, x_2, \dotsc, x_n)$, $\ve{F}$ is a set of subsets of $\{1,2,\dotsc,n)$, and $p_\ve{a}(\ve{x})$ is a function of components of $\ve{x}$ whose set of indices are exactly $\ve{a}$. You can read more about such models in another note of mine \cite{KhungurnCrf}.
  
  \item We are interested in such probability distributions with paremeters. In other words,
  \begin{align*}
      p(\ve{x};\boldsymbol{\theta}) = \frac{q(\ve{x};\boldsymbol{\theta})}{Z(\boldsymbol{\theta})}
  \end{align*}
  where
  \begin{align*}
      Z(\boldsymbol{\theta}) = \int q(\ve{x};\boldsymbol{\theta})\, \dee\ve{x}.
  \end{align*}

  \item We are particularly interested in estimating $\ves{\theta}$ from sampled data $\ve{x}_1$, $\ve{x}_2$, $\dotsc$, $\ve{x}_N$.
  
  \item The standard approch would be to perform maximum likelihood estimation (MLE):
  \begin{align*}
      \argmax_{\boldsymbol{\theta}} \sum_{i=1}^N \log p(\ve{x}_i;\boldsymbol{\theta}) = \argmax_{\boldsymbol{\theta}} \bigg\{ \sum_{i=1}^N \log q(\ve{x}_i;\boldsymbol{\theta}) -N \log Z(\boldsymbol{\theta}) \bigg\}
  \end{align*}

  \item The general problem is that computing $Z(\boldsymbol{\theta})$ is often infeasible.
  
  \item The common way to deal with this is to estimate $Z(\boldsymbol{\theta})$ with Monte Carlo integration.
  \begin{itemize}
    \item Markov chain Monte Carlo (MCMC) methods are often employed to generate samples that yield low variances.
    \item Nevertheless, MCMC methods are slow because they need to generate many samples before convergence.
  \end{itemize}
  
  \item In 2005, Aapo Hyv\"{a}rinen proposed {\bf score matching} as a way to estimate $\boldsymbol{\theta}$ without explicitly dealing with $Z(\boldsymbol{\theta})$ \cite{Hyvarinen:2005}.
  
  \item The idea is to ``match'' the ``score function'' instead of doing the optimization on the probabilities directly.
  
  \item The (Stein) {\bf score function} of a probability distribution $p(\ve{x};\boldsymbol{\theta})$ is the gradient with respect to $\ve{x}$ of its logarithm:
  \begin{align*}
      \boldsymbol{\Psi}(\ve{x};\boldsymbol{\theta})
      = \begin{bmatrix}
          \Psi_1(\ve{x};\boldsymbol{\theta}) \\
          \Psi_2(\ve{x};\boldsymbol{\theta}) \\
          \vdots \\
          \Psi_n(\ve{x};\boldsymbol{\theta})
      \end{bmatrix}
      = \begin{bmatrix}
          \partial(\log p(\ve{x};\boldsymbol{\theta})) / \partial x_1 \\
          \partial(\log p(\ve{x};\boldsymbol{\theta})) / \partial x_2 \\
          \vdots \\
          \partial(\log p(\ve{x};\boldsymbol{\theta})) / \partial x_n
      \end{bmatrix}
      = \nabla_{\ve{x}} \log p(\ve{x};\boldsymbol{\theta}).
  \end{align*}
  Viewing the distribution $p$ as a function with signature $\Real^n \rightarrow \Real$, we have that $\boldsymbol{\Psi}$ has signature $\Real^n \rightarrow \Real^n$.

  \item Note that the good thing about the score function is that it allows us to bypass the partition function:
  \begin{align*}
      \nabla_{\ve{x}} \log p(\ve{x};\boldsymbol{\theta})
      &= \nabla_{\ve{x}} \log \frac{q(\ve{x};\boldsymbol{\theta})}{Z(\boldsymbol{\theta})}
      = \nabla_{\ve{x}} \big( \log q(\ve{x};\boldsymbol{\theta}) - \log Z(\boldsymbol{\theta}) \big)
      = \nabla_{\ve{x}} \log q(\ve{x};\boldsymbol{\theta}) - \nabla_{\ve{x}} \log Z(\boldsymbol{\theta}) \\
      &= \nabla_{\ve{x}} \log q(\ve{x};\boldsymbol{\theta}).
  \end{align*}
  This is because $Z(\boldsymbol{\theta})$ is a constant with respect to $\ve{x}$.

  \item Recall that we are given $\ve{x}_1$, $\ve{x}_2$, $\dotsc$, $\ve{x}_N$ sampled from an unknown distribution $p_{\mrm{data}}$. We can define the score function of the data distribution:
  \begin{align*}
      \boldsymbol{\Psi}_{\mrm{data}}(\ve{x}) = \nabla_{\ve{x}} \log p_{\mrm{data}}(\ve{x}).
  \end{align*}

  \item The ``matching'' in score matching is trying to find $\boldsymbol{\theta}$ that makes $\boldsymbol{\Psi}(\ve{x},\boldsymbol{\theta})$ as close as possible to $\boldsymbol{\Psi}_{\mrm{data}}(\ve{x})$. Operationally, Hyv\"{a}rinen proposes minimizing the expected squared Euclidean distance between the scores:
  \begin{align*}
      J(\theta) 
      = \frac{1}{2} E_{\ve{x} \sim p_{\mrm{data}}} \Big[ \big\| \boldsymbol{\Psi}(\ve{x};\boldsymbol{\theta}) - \boldsymbol{\Psi}_{\mrm{data}}(\ve{x}) \big\|_2^2 \Big]
      = \frac{1}{2} \int p_{\mrm{data}}(\ve{x}) \big\| \boldsymbol{\Psi}(\ve{x};\boldsymbol{\theta}) - \boldsymbol{\Psi}_{\mrm{data}}(\ve{x}) \big\|_2^2\, \dee\ve{x}.
  \end{align*}
  The estimator is thus given by by:
  \begin{align*}
      \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}}\ J(\boldsymbol{\theta}). 
  \end{align*}

  \item The function $J(\boldsymbol{\theta})$ is commonly known as the {\bf Fisher divergence} between two distributions. Given two distributions $p_0$ and $p_1$, the Fisher divergence is defined as:
  \begin{align*}
      F(p_0\|p_1) = E_{\ve{x} \sim p_0} \Big[ \big\| \nabla_{\ve{x}} \log p_0(\ve{x}) - \nabla_{\ve{x}} \log p_1(\ve{x}) \big\|_2^2 \Big] = \int p_0(\ve{x}) \big\| \nabla_{\ve{x}} \log p_0(\ve{x}) - \nabla_{\ve{x}} \log p_1(\ve{x}) \big\|_2^2\, \dee\ve{x}.
  \end{align*}
  In other words, we are minimizing $F(p_{\mrm{data}}(\ve{x}) \| p(\ve{x};\boldsymbol{\theta}))$.

  \item Recall again that we are only given $\ve{x}_1$, $\ve{x}_2$, $\dotsc$, $\ve{x}_N$. We don't know what $p_{\mrm{data}}$ is. So, how do we do the optimization then? The good news is that we can rewrite the Fisher divergence in a form that does not involve $p_{\mrm{data}}$ instance the expectation operator.
  
  \item \begin{theorem} \label{thm:score-matching}
  We have that
  \begin{align}
      J(\ves{\theta}) 
      &= \int p_{\mrm{data}}(\ve{x}) \sum_{i=1}^n \bigg[ \frac{\partial \Psi_i(\ve{x};\boldsymbol{\theta})}{\partial x_i} + \frac{1}{2} \big( \Psi_i(\ve{x};\boldsymbol{\theta}) \big)^2 \bigg]\, \dee\ve{x} + C \notag\\
      &= \int p_{\mrm{data}}(\ve{x}) \sum_{i=1}^n \bigg[ 
      \frac{\partial^2 (\log p(\ve{x};\boldsymbol{\theta}))}{\partial x_i^2}
      + \frac{1}{2} \bigg( \frac{\partial(\log p(\ve{x};\boldsymbol{\theta}))}{\partial x_i} \bigg)^2
      \bigg]\, \dee\ve{x} + C \label{fisher-divergence-rewrite}
  \end{align}
  where $C$ is a constant that does not depend on $\boldsymbol{\theta}$. The theorem holds under the following conditions:
  \begin{itemize}
    \item $\boldsymbol{\Psi}$ is differentiable.
    \item $p_{\mrm{data}}(\ve{x})$ is differentiable.
    \item $E_{\ve{x} \sim p_{\mrm{data}}}[\| \boldsymbol{\Psi}(\ve{x};\boldsymbol{\theta}) \|^2]$ and $E_{\ve{x} \sim p_{\mrm{data}}}[\| \boldsymbol{\Psi}_{\mrm{data}}(\ve{x}) \|^2]$ are finite for every $\boldsymbol{\theta}$.
    \item $p_{\mrm{data}}(\ve{x})\boldsymbol{\Psi}(\ve{x};\boldsymbol{\theta})$ has bounded support.
  \end{itemize}  
  \end{theorem}
  The proof can be found in Hyv\"{a}rinen's paper \cite{Hyvarinen:2005}, and it is based on applying integration by parts.

  \item Note that the RHS of \eqref{fisher-divergence-rewrite} can be rewritten as:
  \begin{align*}
      J(\ves{\theta}) 
      &= E_{\ve{x} \sim p_{\mrm{data}}} \bigg[ \nabla_{\ve{x}} \cdot \ves{\Psi}(\ve{x};\ves{\theta}) + \frac{1}{2} \| \ves{\Psi}(\ve{x};\ves{\theta}) \|_2^2 \bigg]
      = E_{\ve{x} \sim p_{\mrm{data}}} \bigg[ \Delta_\ve{x} \log p(\ve{x};\ves{\theta}) + \frac{1}{2} \| \nabla_{\ve{x}} \log p(\ve{x};\ves{\theta}) \|_2^2 \bigg].
  \end{align*}
  Here, $\nabla_{\ve{x}} \cdot$ is the {\bf divergence operator}
  \begin{align*}
      \nabla_{\ve{x}} \cdot \ve{f} = \sum_{i=1}^n \frac{\partial f_i}{\partial x_i},
  \end{align*}
  and $\Delta_{\ve{x}} = \nabla_{\ve{x}} \cdot \nabla_{\ve{x}}$ is the {\bf Laplace operator}
  \begin{align*}
      \Delta_{\ve{x}} f = \nabla_{\ve{x}} \cdot \nabla_{\ve{x}} f = \sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2}.
  \end{align*}

  \item Theorem~\ref{thm:score-matching} allows us to approximate $J(\theta)$ by Monte Carlo integrations using the samples we have:
  \begin{align*}
      J(\ves{\theta}) \approx \tilde{J}(\ves{\theta}) 
      = \frac{1}{N} \sum_{j=1}^N \bigg[ \nabla_{\ve{x}} \cdot \ves{\Psi}(\ve{x}_j;\ves{\theta}) + \frac{1}{2} \| \ves{\Psi}(\ve{x}_j;\ves{\theta}) \|_2^2 \bigg].
  \end{align*}
  This means that we can now optimize for $\ves{\theta}$.

  \item Hyv\"{a}rinen also shows that optimizing for $J(\ves{\theta})$ would yield the right distribution.
  
  \begin{theorem} \label{thm:score-matching-uniqueness}
    Assume that there is a unique $\ves{\theta}^*$ such that $p_{\mrm{data}}(\ve{x}) = p(\ve{x};\ves{\theta}^*)$ and that $q(\ve{x};\ves{\theta}) > 0$ for all $\ve{x}$ and $\ves{\theta}$. Then, $J(\ves{\theta}) = 0$ if and only if $\ves{\theta} = \ves{\theta}^*$.
  \end{theorem}

  \begin{corollary}
    Under the assumption of Theorem~\ref{thm:score-matching-uniqueness}, the estimator $\tilde{J}(\ves{\theta})$ is consistent. In other words, it converges in probability towards the true value of $J(\ves{\theta})$ when the sample size approaches infinity.
  \end{corollary}
\end{itemize}

\section{Sampling from a Score-Based Model}

\begin{itemize}
  \item 
\end{itemize}

\section{Training a Score-Based Model Cheaply [Vincent 2011]}

\begin{itemize}
  \item 
\end{itemize}

\section{Noise-Conditional Score-Based Model [Song and Ermon, 2019]}

\begin{itemize}
  \item 
\end{itemize}

\bibliographystyle{alpha}
\bibliography{score-based-generative-models}  
\end{document}