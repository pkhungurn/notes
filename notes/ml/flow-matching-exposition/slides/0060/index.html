<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>0060</title>

    <!-- Bootstrap -->
    <link href="../../../../../css/bootstrap.min.css" rel="stylesheet">
    <link href="../../../../../css/theme.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: {autoNumber: "AMS"} }
    });
    </script>
    <script type="text/javascript"
            src="../../../../../MathJax/MathJax.js?config=TeX-AMS_HTML-full">
    </script>

    <script type="text/javascript" src="../../../../../js/jquery-3.4.1.min.js"></script>
    <script type="text/javascript" src="../../../../../js/bigfoot.min.js"></script>

    <link rel="stylesheet" type="text/css" href="../../../../../css/bigfoot-default.css">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.1/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.1/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>
<body>
<div class="container">
    <span style="visibility: hidden;">
        \(
        \def\sc#1{\dosc#1\csod}
        \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\ves}[1]{\boldsymbol{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \DeclareMathOperator*{\argmax}{arg\,max}
        \DeclareMathOperator*{\argmin}{arg\,min}

        \newcommand{\data}{\mathrm{data}}
        \newcommand{\N}{\mathcal{N}}
        \newcommand{\Hil}{\mathcal{H}}
        \)
    </span>

    <h1>Continuous Normalizing Flow</h1>
    
    <hr>
    <p>Let's get back to generative modeling problem.</p>

    <ul>
      <li>We are given $p_{\mrm{noise}}$.</li>
      <li>We are given $N$ samples from $p_{\data}$, but we do not know what $p_{\data}$ looks like.</li>
      <li>We want to find a probability path $p_t$ such that $p_0 = p_{\mrm{noise}}$ and $p_1 = p_{\data}$.</li>
    </ul>

    <hr>
    <p>The neural ODE paper <a href="https://arxiv.org/abs/1806.07366">[Chen et al. 2018]</a> proposes doing the following.</p>
    <ul>
      <li>Use a neural network $v(t,x;\theta)$ to model a time-dependent vector field $v_t(x)$.</li>
      <li>The neural network would give rise to a flow. We can use this flow to push forward $p_{\mrm{noise}}$, and this will give us some probability distribution $p_1$.</li>
      <li>We optimize $p_1$ to approximate $p_{\data}$ by minimizing the negative log likelihodd of the data items from the distribution. In other words, we minimize
      \begin{align*}
        \mcal{L}_{\mrm{NLL}}(\theta) = E_{x_{\data} \sim p_{\data}} \bigg[ -\log p_1(x_{\data}) \bigg].
      \end{align*}
      </li>
      <li>We can compute $\log p_1(x_{\data})$ with the algorithm we discussed in the last slide.</li>
    </ul>    

    <hr>
    <p>The neural network model above is called a <b>continuous normalizing flow</b> (CNF).</p>

    <hr>
    <p>Why is CNF good?</p>

    <ul>
      <li>Unlike discrete normalizing flow, there is no restriction on the form of the network $v(t,x;\theta)$.</li>
    </ul>

    <hr>
    <p>Why is CNF bad?</p>

    <ul>
      <li>Each training example is expensive. We must simulate the dynamics of the vector field in each training iteration.</li>
      <li>Sampling is also expensive because we need to integrate the vector field.
        <ul>
          <li>This is also one of cons of diffusion models.</li>
        </ul>
      </li>
    </ul>
      
    <hr>
    <p>When we train diffusion models, we do not have to integrate anything. Each training iteration is much cheaper.</p>

    <p>Can we train a CNF as cheaply as we train a diffusion model?</p>

    <hr>
    <p>
        <a href="../0059/index.html">[&lt;&lt;]</a> 
        <a href="../../index.html">[Top]</a>
        <a href="../0061/index.html">[&gt;&gt;]</a> 
    </p>
  </div>

<script type="text/javascript">
$.bigfoot();
</script>

</body>
</html>