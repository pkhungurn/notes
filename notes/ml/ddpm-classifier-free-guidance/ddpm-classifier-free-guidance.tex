\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\ves}[1]{\boldsymbol{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\N}{\mathcal{N}}
\newcommand{\data}{\mathrm{data}}
\newcommand{\SNR}{\mathrm{SNR}}
\newcommand{\sigmoid}{\mathrm{sigmoid}}

\title{Classifer-Free Diffusion Guidance}
\author{Pramook Khungurn}

\begin{document}
\maketitle

This note is written as I read the paper ``Classifer-Free Diffusion Guidance'' by Ho and Salimans \cite{Ho:2022}.

\section{Introduction}

\begin{itemize}
\item Classifier guidance.
\begin{itemize}
  \item First derived by Sohl-Dicksteing \etal\ in 2015 \cite{SohlDickstein:2015}.
  
  \item Popularized by Dhariwal and Nichol in 2021 \cite{Dhariwal:2021}.
  
  \item The technique allows us to force a DDPM to generate images of a certain class.
  
  \item It also allows a DDPM to trade diversity of generated data items for their fidelity.
  \begin{itemize}
    \item This helps improve metrics such as FID, sFID, and precision.
    \item It is similar to the way GANs can improve their metrics by utilizing the ``trucation trick.''
  \end{itemize}

  \item We train a DDPM as usual. However, we also need to train a classifier $p_{\ves{\phi}}(y|\ves{x}^{(t)})$ on degraded samples $\ve{x}^{(t)}$.
  
  \item In the backward process, when sampling $\ve{x}^{(t-1)}$ from $\ve{x}^{(t)}$, we use the distribution
  \begin{align*}
    p_{\ves{\theta}, \ves{\phi}}(\ve{x}^{(t-1)}|\ve{x}^{(t)}, y)
    &= \mcal{N}\big(\ve{x}^{(t-1)}; \ves{\mu} + s \Sigma \nabla \log p_{\ves{\theta}}(y | \ves{\mu})\big)
  \end{align*}
  where
  \begin{itemize}
    \item $\ves{\mu} = \ves{\mu}_{\ves{\theta}}(\ve{x}^{(t)},t)$ is the mean of the backward transition,
    \item $\Sigma = \Sigma_{\ves{\theta}}(\ve{x}^{(t)},t)$ is the covariance matrix of the backward transition, and
    \item $s$ is a scaling factor.
  \end{itemize}
  
  \item See more details in another note of mine \cite{Khungurn:2022a}.
\end{itemize}

\item Research question: can guidance be performed without a classifier?
\begin{itemize}
  \item To use a classifier guidance, we need to train a classifier from scratch.
  \begin{itemize}
    \item Existing ones cannot be used because we need to train them on degraded samples.
  \end{itemize}

  \item Their use in DDPM sampling is also cumbersome.
  \begin{itemize}
    \item Need to evaluate the gradient of the log of the probability.
    
    \item The most convenient way to do this is to use automatic differentiation in deep learning packages. Still, this is still a hassle.
  \end{itemize}

  \item Ho and Salimans also worry that classifier guidance makes DDPMs similar to GANs.
    \begin{itemize}
      \item Classifier guidance is an ``attack'' on a certain classifier because it tries to fool that classifier to think that the generated samples are from a certain class.
      
      \item So, in a sense, classifier guidance is similar to adversarial training.
      
      \item Do we have a non-GAN-like way to trade diversity for fidelity?
    \end{itemize}
\end{itemize}
\end{itemize}

\section{Background}

\begin{itemize}
  \item Ho and Salimans use the formalism developed in Kingma \etal's paper \cite{Kingma:2021} with some modifications.  

  \item $\ve{x}$ denoted a data item that is sampled from a data distribution $p(\ve{x})$.
  
  \item The time parameter $t$ is dropped in favor of a new parameter $\lambda$. A degraded data item is denoted by $\ve{z}_\lambda$, which is indexed by $\lambda \in [\lambda_{\min}, \lambda_{\max}] \subseteq \Real$ like this $\ve{z}_\lambda$. Here, $\lambda$ can be negative.
  
  \item We define
  \begin{align*}
    \alpha_{\lambda}^2 
    &= \frac{1}{1 + e^{-\lambda}} = \sigmoid(\lambda), &
    \\
    \sigma_\lambda^2       
    &= 1 - \alpha^2_\lambda = \frac{e^{-\lambda}}{1 + e^{-\lambda}} = \frac{1}{1 + e^{\lambda}} = \sigmoid(-\lambda), 
    &
    \\
    \alpha^2_{\lambda_0|\lambda_1} 
    &= \frac{\alpha^2_{\lambda_0}}{\alpha^2_{\lambda_1}} = \frac{1 + e^{-\lambda_1}}{1 + e^{-\lambda_0}},
    & 
    \\
    \sigma^2_{\lambda_0|\lambda_1}
    &= \sigma^2_{\lambda_0} - \alpha^2_{\lambda_0|\lambda_1} \sigma^2_{\lambda_1} 
    = (1 - e^{\lambda_0-\lambda_1}) \sigma_{\lambda_0}^2.
    & (\mbox{Proposition~\ref{thm:sigma-lambda-simplified}})
  \end{align*}
  Note that the formula for $\sigma_{\lambda_0|\lambda_1}^2$ works only when $\lambda_0 \leq \lambda_1$. Otherwise, we would have $\sigma^2_{\lambda_0|\lambda_1} < 0$.

  \item The signal-to-noise ratio (SNR) is given by
  \begin{align*}
    \SNR(\lambda) = \frac{\alpha^2_\lambda}{\sigma^2_\lambda} = \frac{1 + e^{\lambda}}{1 + e^{-\lambda}} = \frac{e^{\lambda}(1 + e^{-\lambda})}{1 + e^{-\lambda}} = e^{\lambda}.
  \end{align*}
  So,
  \begin{align*}
    \lambda = \log \frac{\alpha_\lambda^2}{\sigma^2_\lambda} = \log \SNR(\lambda).
  \end{align*}

  \item The forward process is denoted by $q$, so we have probabilities like $q(\ve{z}_\lambda | \ve{x})$ and $q(\ve{z}_{\lambda} | \ve{z}_{\lambda'})$.
  \begin{itemize}
    \item The forward process starts from $\lambda_{\max}$ (highest SNR, least noise) and goes to $\lambda_{\min}$ (lower SNR, most noise). The backward process does the opposite.
    
    \item As usual, we set
    \begin{align*}
      q(\ve{z}_\lambda | \ve{x}) &= \N(\alpha_\lambda \ve{x}, \sigma_\lambda^2 I), \\
      q(\ve{z}_{\lambda_0}|\ve{z}_{\lambda_1}) &= \N(\alpha_{\lambda_0|\lambda_1} \ve{x}, \sigma^2_{\lambda_0|\lambda_1} I).
    \end{align*}
    for any $\lambda$, $\lambda_0$, and $\lambda_1$ with the condition that $\lambda_0 < \lambda_1$.
  
    \item Given $\lambda_0 < \lambda_1$, we can show that
    \begin{align*}
      q(\ve{z}_{\lambda_1} | \ve{z}_{\lambda_0}, \ve{x})
      &= \N(\ve{z}_{\lambda_1}; \ves{\mu}_{\lambda_1|\lambda_0}(\ve{z}_{\lambda_0}, \ve{x}), \widetilde{\sigma}^2_{\lambda_1|\lambda_0}I)
    \end{align*}
    where
    \begin{align*}
      \ves{\mu}_{\lambda_1|\lambda_0}(\ve{z}_{\lambda_0}, \ve{x})
      &= \frac{\alpha_{\lambda_0|\lambda_1} \sigma_{\lambda_1}^2}{\sigma_{\lambda_0}^2} \ve{z}_{\lambda_0} + \frac{\alpha_{\lambda_1} \sigma^2_{\lambda_0|\lambda_1}}{\sigma^2_{\lambda_0}} \ve{x}
      = e^{\lambda_0 - \lambda_1} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}} \ve{z}_{\lambda_0} + (1 - e^{\lambda_0 - \lambda_1})\alpha_{\lambda_1} \ve{x}, \\
      \widetilde{\sigma}^{2}_{\lambda_1 | \lambda_0} 
      &= \frac{ \sigma^2_{\lambda_0|\lambda_1}\sigma^2_{\lambda_1}}{\sigma^2_{\lambda_0}} 
      = (1 - e^{\lambda_0 - \lambda_1})\sigma^2_{\lambda_1}.
    \end{align*}
    See Proposition~\ref{thm:backward-conditional-prob-simplified} for the proof.  
  \end{itemize}

  \item The backward process is denoted by $p_{\ves{\theta}}$.
  \begin{itemize}
    \item With $\lambda_{\min}$ low enough, we may set $p_{\ves{\theta}}(\ve{z}_{\lambda_{\min}}) = \N(\ve{z}_{\lambda_{\min}}; \ve{0}, I).$
    
    \item The transition probability from $\ve{z}_{\lambda_0}$ to $\ve{z}_{\lambda_1}$ is given by
    \begin{align*}
      p_{\ves{\theta}}(\ve{z}_{\lambda_1} | \ve{z}_{\lambda_0})
      &= \N(\ve{z}_{\lambda_1}; \mu_{\lambda_1|\lambda_0}(\ve{z}_{\lambda_0}, \ve{x}_{\ves{\theta}}(\ve{z}_{\lambda_0}, \lambda_0)), (\widetilde{\sigma}^2_{\lambda_0|\lambda_1})^{1-v}(\sigma^2_{\lambda_1|\lambda_0})^v I)
    \end{align*}
    where $\ve{x}_{\ves{\theta}}(\ve{z}_{\lambda_0}, \lambda_0)$ is the denoising model the tries to predict $\ve{x}$ from $\ve{z}_{\lambda_0}$.

    \item The variance of the backward transition is taken from Dhariwal and Nichol \cite{Dhariwal:2021}, which specifies that it is an interpolated value in log space between $\widetilde{\sigma}^2_{\lambda_0|\lambda_1}$ and $\sigma^2_{\lambda_1|\lambda_0}$.\
    \begin{itemize}
      \item The paper found it effective to use a constant hyperparameter $v$ instance of a learned $v$ that depends on $\ve{z}_{\lambda_0}$.
    \end{itemize}

    \item During sampling, we apply the transition along an increasing sequence $$\lambda_{\min} = \lambda_1 < \lambda_2 < \dotsb < \lambda_T = \lambda_{\max}.$$
    This is the standard ancestral sampling used in \cite{Ho:2020}.

    \item The denosing model $\ve{x}_{\ves{\theta}}$ is parameterized in the terms of the noise prediction model $\ves{\xi}_{\ves{\theta}}$ such that there relationship is given by
    \begin{align*}
      \ve{x}_{\ves{\theta}}(\ve{z}_{\lambda}, \lambda)
      &= \frac{\ve{z}_\lambda - \sigma_\lambda \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda)}{\alpha_\lambda}.
    \end{align*}

    \item The noise prediction model is trained to minimize the loss
    \begin{align*}
      \mcal{L} = E_{\ve{x} \sim p(\ve{x}), \lambda \sim p(\lambda), \ves{\xi} \sim \N(\ve{0},I)} \big[
        \| \ves{\xi} - \ves{\xi}_{\ves{\theta}}(\alpha_\lambda \ve{x} + \sigma_\lambda \ves{\xi}, \lambda) \|^2
      \big]
    \end{align*}
    where $\ve{x}$ is drawn from the data distribution $p(\ve{x})$. 
    
    \item $\lambda$ is drawn from the distribution $p(\lambda)$ where $\lambda$ is sampled through the expression
    \begin{align*}
      \lambda := -2 \log \tan(au + b(1-u))
    \end{align*}
    where $u$ is sampled unformly from interval $[0,1]$, $a = \arctan(e^{-\lambda_{\min}/2})$, and $b = \arctan(e^{-\lambda_{\max}/2})$.

    \item If the noise prediction model is optimized well enough, it would predict the scaled version of the score of $p_{\lambda}$. In other words,
    \begin{align*}
      \ves{\xi}_{\ves{\theta}}(\ve{z}_{\lambda}, \lambda) \approx -\sigma_\lambda \nabla \log q(\ve{z}_\lambda).
    \end{align*}
  \end{itemize}  

  \item For conditional generation, we are given conditioning information $\ve{c}$. We are required to sample from $p(\ve{x}|\ve{c})$. The derivation above remains the same except for the fact that we need to add the conditioning information to everything. In particular,
  \begin{align*}
    p(\ve{x}) &\mapsto p(\ve{x}|\ve{c}), \\
    q(\ve{z}_\lambda) &\mapsto q(\ve{z}_{\lambda}|\ve{c}), \\
    \ve{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda) &\mapsto \ve{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}). 
  \end{align*}

  \item Classifier guidance, when written using the notation of the present paper is equivalent to predicting the noise with
  \begin{align*}
    \widetilde{\ves{\xi}}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c})
    = \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda) - w\sigma_\lambda \nabla \log p(\ve{c}|\ve{z}_\lambda)
  \end{align*}
  where $p(\ve{c}|\ve{z}_\lambda, \lambda)$ denote the classifier, and $w$ is a scaling factor.
  \begin{itemize}
    \item We are not using the mean $\mu_{\lambda + \Delta\lambda|\lambda}(\ve{z}_\lambda, \ve{x}_{\ves{\theta}}(\ve{z}_\lambda,\lambda))$ to compute the classifier gradient here because (1) it's what people do even if the derivation tell them other words, and (2) it makes sense when you take the limit as the step size approaches $0$, making the model a continuous-time model.
  \end{itemize}

  \item The paper notes that the best result in \cite{Dhariwal:2021} is obtained by applying classifier-guidance on top of an already conditional model. In other words, the noise prediction model can be written as:
  \begin{align} \label{eq:classifier-guidance}
    \widetilde{\ves{\xi}}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c})
    = \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) - w\sigma_\lambda \nabla \log p(\ve{c}|\ve{z}_\lambda).
  \end{align}
\end{itemize}

\section{Classifier-Free Guidance}

\begin{itemize}
  \item Instead of training a separate classifier model, we train an unconditional model $p_{\ves{\theta}}(\ve{x})$ togother with a conditional model $p_{\ves{\theta}}(\ve{x}|\ve{c})$.
  
  \item We use a single neural network $\ve{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c})$ to serve as our noise prediction model. When we want the network to predict unconditional noise, we simply set the conditioning information to $\ve{c} = \mbox{\O}$ where $\mbox{\O}$ is a special token.
  
  \item During training, $\ve{c}$ is set to $\mbox{\O}$ with some probability $p_{\mrm{uncond}}$, which is a hyperparameter.
  
  \item Sampling is performed using the following linear combination of noise estimate:
  \begin{align*}
    \widetilde{\ves{\xi}}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c})
    = (1 + w) \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) - w \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda).
  \end{align*}

  \item The above noise estimate is derived from the fact that
  \begin{align*}
    q(\ve{c}| \ve{z}_\lambda) \approx \frac{q(\ve{z}_\lambda | \ve{c})}{q(\ve{z}_\lambda)}
  \end{align*}
  As a result,
  \begin{align*}
    \nabla \log q(\ve{c}|\ve{z}_\lambda) = \nabla \log q(\ve{z}_\lambda|\ve{c}) - \nabla \log q(\ve{z}_\lambda).
  \end{align*}
  Hence, using the classifier guidance equation \eqref{eq:classifier-guidance}, we have that
  \begin{align*}
    \widetilde{\ves{\xi}}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c})
    &= \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) - w\sigma_\lambda \nabla \log p(\ve{c}|\ve{z}_\lambda) \\
    &= \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) - w\sigma_\lambda (\nabla \log q(\ve{z}_\lambda|\ve{c}) - \nabla \log q(\ve{z}_\lambda) ) \\
    &= \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) + w ( \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) - \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda) ) \\
    &= (1 + w) \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda, \ve{c}) - w \ves{\xi}_{\ves{\theta}}(\ve{z}_\lambda, \lambda).
  \end{align*}
\end{itemize}

\section{Experiments}

\begin{itemize}
  \item The experiments were performed on the scaled-down ($64 \times 64$ and $128 \times 128$) versions of ImageNet.

  \item The paper uses the same architecture as that of Dhariwal and Nichol \cite{Dhariwal:2021} with some modifications.
  \begin{itemize}
    \item The interpolating parameter $v$ is not learned, but fixed.
    \begin{itemize}
      \item For the $64 \times 64$ model, the paper fixes $v = 0.3$.
      \item For the $128 \times 128$ model, the paper fixes $v = 0.2$.
    \end{itemize}

    \item $\lambda_{\min} = -20$ and $\lambda_{\max} = 20$.
    \item The models were trained for $2.7$ million steps.
  \end{itemize}

  \item The paper experimented with $w \in \{0, 0.1, 0.2, \dotsc, 4\}$ and $p_{\mrm{uncond}} = \{ 0.1, 0.2, 0.5 \}$. 
  \begin{itemize}
    \item For FID, it dropped until $w = 0.1$ where the best score was atttained with $p_{\mrm{uncond}} = 0.1$. Then, FID increased as $w$ increase.
    \item IS increases with $w$. The best IS was attained at $w = 4$ and $p_{\mrm{uncond}} = 0.1$.
    \item $p_{\mrm{uncond}}$ of $0.1$ and $0.2$ performed about equally well, but $0.5$ was the worst.
  \end{itemize}

  \item The author draws two conclusion.
  \begin{itemize}
    \item Classifier-free guidance can trade diversity for fidelity like classifier guidance can.
    \begin{itemize}
      \item This is evident from the fact that IS scores (which depends much on fidelity) improved as $w$ increases.
      \item FID is more complicated as it depends on both fidelity and diversity.
    \end{itemize}

    \item Only a small portion of the model capacity is needed for unconditional generation.
  \end{itemize}
\end{itemize}

\appendix

\section{Derivations}

\begin{itemize}
\item \begin{proposition} \label{thm:sigma-lambda-simplified}
  For $\lambda_0 < \lambda_1$, it holds that
  $\sigma^2_{\lambda_0|\lambda_1} = (1 - e^{\lambda_0 - \lambda_1})\sigma_{\lambda_0}^2.$
\end{proposition}

\begin{proof} We have that
  \begin{align*}
    \sigma^2_{\lambda_0|\lambda_1}
    &= \sigma^2_{\lambda_0} - \alpha^2_{\lambda_0|\lambda_1} \sigma^2_{\lambda_1} 
    = 1 - \alpha^2_{\lambda_0} - \frac{\alpha_{\lambda_0}^2}{\alpha_{\lambda_1}^2} (1 - \alpha^2_{\lambda_1}) 
    = 1 - \alpha_{\lambda_0}^2 - \frac{\alpha_{\lambda_0}^2}{\alpha_{\lambda_1}^2} + \alpha_{\lambda_0}^2
    = 1 - \frac{\alpha_{\lambda_0}^2}{\alpha_{\lambda_1}^2} 
    \\
    &= 1 - \frac{1 + e^{-\lambda_1}}{1 + e^{-\lambda_0}} 
    = \frac{e^{-\lambda_0} - e^{-\lambda_1}}{1 + e^{-\lambda_0}}
    = \frac{e^{\lambda_0}(e^{-\lambda_0} - e^{-\lambda_1})}{e^{\lambda_0}(1 + e^{-\lambda_0})}
    = \frac{1 - e^{\lambda_0-\lambda_1}}{1 + e^{\lambda_0}} 
    \\
    &= (1 - e^{\lambda_0-\lambda_1}) \sigma_{\lambda_0}^2
  \end{align*}
  as required.
\end{proof}

\item \begin{proposition} \label{thm:backward-conditional-prob-simplified}
  For $\lambda_0 < \lambda_1$, it holds that
  \begin{align*}
    \ves{\mu}_{\lambda_0|\lambda_1} (\ve{z}_{\lambda_0}, \ve{x})
    &= e^{\lambda_0 - \lambda_1} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}} \ve{z}_{\lambda_0} + (1 - e^{\lambda_0 - \lambda_1})\alpha_{\lambda_1} \ve{x}.
  \end{align*}
\end{proposition}

\begin{proof}
  We have that
  \begin{align*}
    \ves{\mu}_{\lambda_0|\lambda_1} (\ve{z}_{\lambda_0}, \ve{x})
    &= \frac{\alpha_{\lambda_0|\lambda_1} \sigma_{\lambda_1}^2}{\sigma_{\lambda_0}^2} \ve{z}_{\lambda_0} + \frac{\alpha_{\lambda_1} \sigma^2_{\lambda_0|\lambda_1}}{\sigma^2_{\lambda_0}} \ve{x}.
  \end{align*}
  Now,
  \begin{align*}
    \frac{\alpha_{\lambda_0|\lambda_1} \sigma_{\lambda_1}^2}{\sigma_{\lambda_0}^2}
    &= \frac{\alpha_{\lambda_0} \sigma_{\lambda_1}^2}{\alpha_{\lambda_1} \sigma^2_{\lambda_0}}
    = \frac{\alpha^2_{\lambda_0} \sigma_{\lambda_1}^2}{\alpha_{\lambda_1}^2 \sigma^2_{\lambda_0}} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}}
    = \frac{\alpha^2_{\lambda_0} (1 - \alpha_{\lambda_1}^2)}{\alpha_{\lambda_1}^2 (1-\alpha^2_{\lambda_0})} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}}
    = \frac{\alpha^{-2}_{\lambda_1} - 1}{\alpha^{-2}_{\lambda_0}-1} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}}
    = \frac{e^{-\lambda_1}}{e^{-\lambda_0}} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}}
    = e^{\lambda_0 - \lambda_1} \frac{\alpha_{\lambda_1}}{\alpha_{\lambda_0}},
  \end{align*}
  and
  \begin{align*}
    \frac{\alpha_{\lambda_1} \sigma^2_{\lambda_0|\lambda_1}}{\sigma^2_{\lambda_0}}
    &= \frac{\alpha_{\lambda_1}(1-e^{\lambda_0 - \lambda_1}) \sigma_{\lambda_0}^2}{\sigma^2_{\lambda_0}}
    = \alpha_{\lambda_1}(1-e^{\lambda_0 - \lambda_1})
    = (1-e^{\lambda_0 - \lambda_1})\alpha_{\lambda_1}.
  \end{align*}
  We are done.
\end{proof}
\end{itemize}

\bibliographystyle{alpha}
\bibliography{ddpm-classifier-free-guidance}  
\end{document}