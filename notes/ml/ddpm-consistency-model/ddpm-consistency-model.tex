\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{clrscode3e}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\ves}[1]{\boldsymbol{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\data}{\mathrm{data}}
\newcommand{\SNR}{\mathrm{SNR}}

\DeclareMathOperator*{\argmin}{arg\,min}

\title{Consistency Models}
\author{Pramook Khungurn}

\begin{document}
\maketitle

This note is written as I read ``Consistency Models'' by Song \etal~\cite{Song:2023}.

\section{Introduction}

\begin{itemize}
  \item Diffusion models are slow to sample, which means that they cannot be used in real-time applications like GANs, VAEs, and other models that can sample in one step.
  
  \item Diffusion models can be sped up by distillation: training ``student'' models to mimic the behavior of a ``teacher'' model. There are two existing techniques in literature.
  \begin{itemize}
    \item Standard one-model-to-one-model distillation by Luhman and Luhman \cite{Luhman:2021}.
    \item Progressive distillation by Salimans and Ho \cite{Salimans:2022}.
  \end{itemize}

  \item The Consistenty Models paper proposes a new generative model that can generate high-quality sample fast.

  \item A consistency model can be trained in two ways.
  \begin{itemize}
    \item As a way to distill an existing diffusion model.
    \item From scratch, as a stand-alone model.
  \end{itemize}

  \item Performance of consistency models depend on the way you train it.
  \begin{itemize}
    \item When trained by distillation, it achieved SOTA FID on CIFAR-10 and ImageNet $64\times64$ when compared to other distilled diffusion models.
    \begin{itemize}
      \item This method consistency outperforms progressive distillation.
    \end{itemize}
    \item When trained as a standalone model, it outperformed single-step non-adversarial models on CIFAR-10, ImageNet $64 \times 64$ and LSUN $256 \times 256$.
    \begin{itemize}      
      \item This method performs on par with progressive distillation.       
    \end{itemize}    
  \end{itemize}
  However, the method still loses to the best GANs in one-step generation.

  \item A consistency model has a number of other advantages.
  \begin{itemize}
    \item With it, you can also sample in one step or multiple steps. Using multiple steps get you higher quality samples.
    
    \item It also supports operations such as image-inpainting, colorization, and super-resolution.
  \end{itemize}
\end{itemize}

\section{Background}

\begin{itemize}
  \item Let $\ve{x}$ denote a data sample and $p_{\data}(\ve{x})$ denote the probability distribution of the data.
  
  \item In a diffusion model, the data distribution $p_{\data}$ is corrupted with noise. Its evolution is governed by the stochastic differential equation (SDE)
  \begin{align}
    \dee \ve{x}_t = \ves{\mu}(\ve{x}_t,t)\, \dee t + \sigma(t)\, \dee\ve{w}_t \label{eqn:probability-flow-ode}
  \end{align}
  where
  \begin{itemize}
    \item $t \in [0,T]$,
    \item $\ves{\mu}(\ve{x}_t, t)$ is called the {\bf drift coefficient},
    \item $\sigma(t)$ is called the {\bf diffusion coefficient}, and
    \item $\{ \ve{w}_t \}_{t \in [0,T]}$ is the standard Brownian motion.
  \end{itemize}

  \item Let $p_t(\ve{x})$ denote the distribution of $\ve{x}_t$.
  
  \item The boundary condition of the above SDE is $p_0(\ve{x}) = p_{\data}(\ve{x})$.

  \item The above SDE has an ODE whose distribution of $\ve{x}_t$ (also denoted by $p_t(\ve{x})$) coincides with that of the SDE. This ODE is called the {\bf probability flow ODE}. It is given by:
  \begin{align*}
    \dee\ve{x}_t = \bigg( \ves{\mu}(\ve{x}_t, t) - \frac{1}{2} \sigma^2(t) \nabla_{\ve{x}_t} \log p_t(\ve{x}_t) \bigg)\, \dee t
  \end{align*}
  where $\nabla_{\ve{x}_t} \log p_t(\ve{x}_t)$ is called the {\bf score function} of $p_t(\ve{x}_t)$.

  \item The SDE is typically designed so that $p_T(\ve{x})$ is closed to a spherical Gaussian distribution $\pi(\ve{x})$.
  
  \item The paper adopts the formulation of Karras \etal~\cite{Karras:2022} where
  \begin{itemize}
    \item $\ves{\mu}(\ve{x}_t,t) = \ve{0}$, and
    \item $\sigma(t) = \sqrt{2t}$.
  \end{itemize}
  This gives
  \begin{align*}
    p_t(\ve{x}) = p_{\data}(\ve{x}) * \mcal{N}(\ve{x}; \ve{0}, t^2I)
  \end{align*}
  where $*$ denotes the convolution operation.

  \item If we make $T$ large enough so that $T^2 \gg \Var(\ve{x}_0)$. We have that $p_T(\ve{x}) \approx \pi(\ve{x}) = \mcal{N}(\ve{0}, T^2I)$.
  
  \item To train a diffusion model, we can train a network $\ve{s}_{\ves{\phi}}(\ve{x},t)$, called the {\bf score model}, to estimate $\nabla_{\ve{x}_t} \log p_t(\ve{x})_t$. The empirical estimate of the probability flow ODE then becomes
  \begin{align}
    \frac{\dee \ve{x}_t}{\dee t} = - t \ve{s}_{\ves{\phi}}(\ve{x}_t, t), \label{eqn:empirical-probability-flow-ode}
  \end{align}
  which the paper calls the {\bf empirical probability flow ODE}.

  \item To sample a data item, we start by sampling $\hat{\ve{x}}_T \sim \mcal{N}(\ve{0}, T^2I)$. Then, we simulate the empirical probability flow ODE backward in time to $t = 0$ by any ODE solving method such as Euler \cite{Song:2021} and Huen's \cite{Karras:2022}.

  \item To avoid numerical instability, we typically stop at $t = \epsilon$ where $\epsilon$ is a small positive constant and return $\hat{\ve{x}}_{\epsilon}$ as the output instead of $\hat{\ve{x}}_0$.
  
  \item The paper follows Karras \etal\ and uses $T = 80$ and $\epsilon = 0.002$. The pixel values are scaled to the range $[-1,1]$.
\end{itemize}

\section{Consistency Models}

\subsection{Definition}

\begin{itemize}
  \item Given a solution trajectory $\{ \ve{x}_t \}_{t \in [\epsilon,T]}$ of the probability flow ODE, we define the {\bf consistency function} $\ve{f}: \Real^d \times \Real \rightarrow \Real$ as
  \begin{align*}
    \ve{f}(\ve{x}_t, t) = \ve{x}_\epsilon.
  \end{align*}
  
  \item The consistency function has the following properties:
  \begin{itemize}
    \item It is {\bf self-consistent}, meaning that that $\ve{f}(\ve{x}_t, t) = \ve{f}(\ve{x}_{t'}, t')$ for all $t, t' \in [\epsilon, T]$.
    \item With a fixed time argument, $\ve{f}(\cdot, t)$ is invertible.
  \end{itemize}
  
  \item A {\bf consistency model}, denoted by $\ve{f}_{\ves{\theta}}$, is trained to estimate the consistency function $\ve{f}$.
\end{itemize}

\subsection{Parameterization}

\begin{itemize}
  \item For any consistency function, $\ve{f}(\cdot, \epsilon)$ is the identity function. This constraint is called the {\bf boundary condition}.
  
  \item To create a function that respects the boundary condition, the paper chooses to parameterize the consistency model as:
  \begin{align*}
    \ve{f}_{\ves{\theta}}(\ve{x}, t) = c_{\mrm{skip}}(t) \ve{x} + c_{\mrm{out}}(t) \ve{F}_{\ves{\theta}}(\ve{x},t)
  \end{align*}
  where
  \begin{itemize}
    \item $c_{\mrm{skip}}(t)$ is a differentiable function such that $c_{\mrm{skip}}(\epsilon) = 1$,
    \item $c_{\mrm{out}}(t)$ is a differentiable function such that $c_{\mrm{out}}(\epsilon) = 0$,
    \item $\ve{F}_{\ves{\theta}}(\ve{x},t)$ is a free-form neural network.
  \end{itemize}

  \item The paper chooses
  \begin{align*}
    c_{\mrm{skip}}(t) &= \frac{\sigma_{\data}^2}{(t - \epsilon)^2 + \sigma_{\data}^2}, \\
    c_{\mrm{out}}(t) &= \frac{\sigma_{\data} (t - \epsilon)}{\sqrt{\sigma_{\data}^2 + t^2}}.
  \end{align*}
\end{itemize}

\subsection{Sampling}

\begin{itemize}
  \item If we have a well-trained consistency model, we can generate a sample in one step by simply sampling $\hat{\ve{x}}_t \sim \mcal{N}(\ve{0},T^2I)$ and then compute $\ve{f}_{\ves{\theta}}(\hat{\ve{x}}_T, T)$.
  
  \item Alternatively, we can sample in multiple steps. For this, we assume that we are given a sequence of time $\tau_1 > \tau_2 > \dotsb > \tau_{N}$. Then, we may run the following algorithm.
  \begin{itemize}
    \item[] $\hat{\ve{x}}_{T} \sim \mcal{N}(\ve{0},T^2I)$.
    \item[] $\ve{x} \leftarrow \ve{f}_{\ves{\theta}}(\hat{\ve{x}}_T, T)$
    \item[] {\bf for} $n = 1$ to $N$ {\bf do}
    \item[] \begin{itemize}
      \item[] Sample $\ves{\xi} \sim \mcal{N}(\ve{0},I)$.
      \item[] $\hat{\mathbf{x}}_{\tau_n} \leftarrow \ve{x} + \sqrt{ \tau_n^2 - \epsilon^2 } \ves{\xi}$
      \item[] $\ve{x} \leftarrow \ve{f}_{\ves{\theta}}(\hat{\mathbf{x}}_{\tau_n}, \tau_n)$
    \end{itemize}    
    \item[] {\bf end for}
    \item[] {\bf return} $\ve{x}$.
  \end{itemize}

  \item The paper find the times for the above algorithm with a greedy algorithm. The time is pinpointed one at a time using ternary search.
  
  \item Note that the above algorithm allows for many zero-shot data editing tasks such as inpainting, colorization, super-resolution, and SDEdit \cite{Meng:2021}.
\end{itemize}

\section{Training via Distillation}

\begin{itemize}
  \item First, we subdivide the time interval $[\epsilon, T]$ into $N-1$ intervals with $\epsilon = t_1 < t_2 < \dotsm < t_{N-1} < t_N = T$.
  
  \item The paper follows Karras \etal\ and uses $$t_i = \epsilon^{1/\rho} + \frac{i-1}{N-1} (T^{1/\rho} - \epsilon^{1/\rho})$$ with $\rho = 7$ \cite{Karras:2022}.
  
  \item When $N$ is sufficiently large, we can obtain accurate estimate of $\ve{x}_{t_n}$ from $\ve{x}_{t_{n+1}}$ by running one step of an ODE solver. With the Euler solver, this is given by
  \begin{align*}
    \hat{\ve{x}}_{t_n}^{\ves{\phi}} := \ve{x}_{t_{n+1}} + (t_{n} - t_{n+1}) t_{n+1} \ve{\Phi}(\ve{x}_{t_{n+1}}, t_{n+1}; \ves{\phi})
  \end{align*}
  where $\ves{\Phi}(\cdot, \cdot; \ves{\phi})$ denotes the update performed by a one-step ODE solver.
  
  \item If we use the ODE
  \begin{align*}
    \frac{\dee\ve{x}_t}{\dee t} = -t \ve{s}_{\ves{\phi}}(\ve{x}_t, t)
  \end{align*}
  inspired by Karras \etal, we have that the estimate is given by
  \begin{align*}
    \hat{\ve{x}}_{t_n}^{\ves{\phi}} := \ve{x}_{t_{n+1}} - (t_{n} - t_{n+1}) t_{n+1} \ve{s}_{\ves{\phi}}(\ve{x}_{t_{n+1}}, t_{n+1}).
  \end{align*}

  \item The examples used to train a consistency model is a tuple of the form $(\ve{x}_{t_n}^{\ves{\phi}}, \ve{x}_{t_{n+1}})$. Such an example can be generated by the following procedure:
  \begin{itemize}
    \item Sampling $\ve{x}_0 \sim p_{\data}$.
    \item Set $\ve{x}_{t_{n+1}} \leftarrow \ve{x}_0 + t_{n+1}\ves{\xi}$ where $\ves{\xi} \sim \mcal{N}(\ve{0}, I)$.
    \item Compute $\hat{\ve{x}}_{t_n}^{\ves{\phi}} \leftarrow \ve{x}_{t_{n+1}} - (t_{n} - t_{n+1}) t_{n+1} \ve{s}_{\ves{\phi}}(\ve{x}_{t_{n+1}}, t_{n+1})$.
  \end{itemize}

  \item A consistency model is trained to minimize the output differences between $\ve{x}_{t_{n+1}}$ and $\ve{x}_{t_n}^{\ves{\phi}}$ according to the {\bf consistency distillation loss}:
  \begin{align*}
    \mcal{L}_{\mrm{CD}}^N(\ves{\theta},\ves{\theta}^-;\ves{\phi}) := E_{\substack{\ve{x} \sim p_{\data},\\n \sim \mcal{U}(\{ 1, \dotsc, N-1\}),\\ \ve{x}_{n+1} \sim \mcal{N}(\ve{x},t_{n+1}^1I)}}\Big[\lambda(t_n) d(\ve{f}_{\ves{\theta}}(\ve{x}_{t_{n+1}}, t_{n+1}), \ve{f}_{\ves{\theta}^-}(\hat{\ve{x}}_{t_n}^{\ves{\phi}}, t_{n}))\Big].
  \end{align*}
  Here,
  \begin{itemize}
    \item $\mcal{U}(\{ 1, \dotsc, N-1\})$ is the uniform distribution on $\{ 1, 2, \dotsc, N-1 \}$,
    \item $\lambda(t_n)$ is the postivie weighting function,
    \item $\ves{\theta}^-$ denotes a running average of the past values of $\ves{\theta}$ during the course of the optimization, and
    \item $d(\cdot, \cdot)$ is a metric function (that does not have to necessary satisfy the triangle inequality).
  \end{itemize}

  \item For the metric function, the paper considered using
  \begin{itemize}
    \item the $\ell_2$ distance, $d(\ve{x},\ve{y}) = \| \ve{x} - \ve{y} \|_2$,
    \item the $\ell_1$ distance, $d(\ve{x},\ve{y}) = \| \ve{x} - \ve{y} \|_1$, and
    \item the LPIPS distance \cite{Zhang:2018}.
  \end{itemize}

  \item The paper found that $\lambda(t_n) = 1$ performs well across all datasets.
  
  \item Note that, while training, we deal with two separate networks.
  \begin{itemize}
    \item $\ve{f}_{\ves{\theta}}$ is called the {\bf online network}.
    \item $\ve{f}_{\ves{\theta}^-}$ is caled the {\bf target network}.
  \end{itemize}
  
  \item The running average $\ves{\theta}^-$ is computed with exponential moving average. That is, given a decay rate $0 \leq \mu < 1$, we performed the following update.
  \begin{align*}
    \ves{\theta}^- \leftarrow \mrm{stopgrad}(\mu\ves{\theta}^- + (1 - \mu)\ves{\theta}).
  \end{align*}

  \item Here's the training algorithm.
  \begin{itemize}
    \item[] {\bf while} not convergent {\bf do}
    \item[] \begin{itemize}
      \item[] Sample $\ve{x} \sim p_{\data}$.
      \item[] Sample $n \sim \mcal{U}(\{ 1, 2, \dotsc, N-1\})$.
      \item[] Sample $\ve{x}_{t_{n+1}} \sim \mcal{N}(\ve{x}, t_{n+1}^2 I)$.
      \item[] $\hat{\ve{x}}_{t_n}^{\ves{\phi}} \leftarrow \ve{x}_{t_{n+1}} + (t_{n} - t_{n+1})\ves{\Phi}(\ve{x}_{t_{n+1}}, t_{n+1}; \ves{\phi})$
      \item[] $\mcal{L}(\ves{\theta},\ves{\theta}^-;\ves{\phi}) \leftarrow \lambda(t_n) d(\ve{f}_{\ves{\theta}}(\ve{x}_{t_{n+1}}, t_{n+1}), \ves{f}_{\ves{\theta}^-}(\hat{\ve{x}}_{t_n}^{\ves{\phi}}, t_{n}))$
      \item[] Update $\ves{\theta}$ with $\nabla_{\ves{\theta}} \mcal{L}(\ves{\theta},\ves{\theta}^-;\ves{\phi})$.
      \item[] $\ves{\theta}^{-} \leftarrow \mrm{stopgrad}(\mu \ves{\theta}^- + (1-\mu)\ves{\theta})$
    \end{itemize}
    \item[] {\bf end while}
  \end{itemize}

  \item The training procecure is similiar to momentum-based contrastive learning \cite{Grill:2020, He:2019}. Using the running average can greatly stablize the training process and imporve the final performance of the consistency model.  
  
  \item The paper shows that the following theorem is true.
  \begin{theorem}
    Let 
    \begin{itemize}
      \item $\Delta t := \max_{n \in \{1, 2, \dotsc, N-1\}}\{ t_{n+1} - t_{n} \}$, and
      \item $\ve{f}(\cdot, \cdot; \ves{\phi})$ be the consistency function of the empirical probability flow ODE \eqref{eqn:empirical-probability-flow-ode}.
    \end{itemize}
    Assume that $\ve{f}_{\ves{\theta}}$ satisfies the Lipschitz condition:
    \begin{align*}
      \| \ve{f}_{\ves{\theta}}(\ve{x},t) - \ve{f}_{\ves{\theta}}(\ve{y},t) \|_2 \leq L \| \ve{x} - \ve{y} \|_2
    \end{align*}
    for some positive constant $L$ and for all $\ve{x}$, $\ve{y}$, and $t \in [\epsilon, T]$. Assume further that, for all $n \in \{ 1, 2, \dotsc, N-1 \}$, the ODE solver called at $t_{n+1}$ has local error uniformly bounded by $O((t_{n+1} - t_{n})^{p_1})$ with $p geq 1$. Then, if $\mcal{L}_{\mrm{CD}}^{\ves{\phi}}(\ves{\theta}, \ves{\theta}; \ves{\phi}) = 0$, we have that
    \begin{align*}
      \sup_{n,\ve{x}} \| \ve{f}_{\ves{\theta}}(\ve{x}, t) - \ve{f}_{\ves{\theta}}(\ve{x}, t; \ves{\phi}) \|_2 = O((\Delta t)^p).
    \end{align*}
  \end{theorem}

  \item The consistency distillation loss can be extended to hold for infinitely many time steps. However, it requires Jacobian vector product and require forward-mode automatic differentiation for efficient implementation, which may not be well-supported in some deep learning frameworks.
\end{itemize}

\section{Training in Isolation}

\begin{itemize}
  \item Consistency models can also be trained from scratch without a pre-trained diffusion model.
  
  \item When training with distillation (which we shall now refer to as ``consistency distillation''), we need $\ve{s}_{\ves{\phi}}(\ve{x},t)$ to approximate the score $\nabla_{\ve{x}} \log p_t(\ve{x})$. However, if we were to train a consistency model from scatch, we do not have this score network any more.
  
  \item However, because $\ve{x}_t \sim \mcal{N}(\ve{x}_0, t^2 I)$, we have that, by Tweedie's formula,
  \begin{align*}
    E[\ve{x}_0 | \ve{x}_t] = \ve{x}_t + t^2 \nabla_{\ve{x}_t} \log p_t(\ve{x}_t).
  \end{align*}
  As a result,
  \begin{align*}
    \nabla_{\ve{x}_t} \log p_t(\ve{x}_t) = E\bigg[ -\frac{\ve{x}_0 - \ve{x}_t}{t^2} \bigg| \ve{x}_t \bigg].
  \end{align*}
  In other words, we can use $-(\ve{x}_0 - \ve{x}_t) / t^2$ as an unbiased estimate of $\nabla_{\ve{x}_t} \log p_t(\ve{x}_t)$. Here, $\ve{x}_0 \sim p_{\data}$ and $\ve{x}_t \sim \mcal{N}(\ve{x}_0, t^2I)$. This gives
  \begin{align*}
    \hat{\ve{x}}_{n}^{\ves{\phi}} 
    &= \ve{x}_{t_{n+1}} - (t_{n} - t_{n+1}) t_{n+1} \frac{\ve{x}_0 - \ve{x}_{t_{n+1}}}{t_{n+1}^2} \\
    &= \ve{x}_0 + t_{n+1}\ves{\xi} - (t_n - t_{n+1}) \frac{\ve{x}_0 - \ve{x}_0 - t_{n+1}\ves{\xi}}{t_{n+1}} \\
    &= \ve{x}_0 + t_{n+1}\ves{\xi} + t_n \ves{\xi} - t_{n+1} \ves{\xi}\\
    &= \ve{x}_0 + t_{n}\ves{\xi}
  \end{align*}  
  where $\ves{\xi}$ is a noise vector distributed according to $\mcal{N}(\ve{0},I)$ such that $\ve{x}_t = \ve{x}_0 + t_{n+1} \ves{\xi}$.

  \item With the above estimate, the consistency distillation loss becomes the {\bf consistency training loss}:
  \begin{align*}
    \mcal{L}^N_{\mrm{CT}}(\ves{\theta}, \ves{\theta}^-) = E_{\substack{\ve{x} \sim p_{\data}, \\ n \sim \mcal{U}(\{1, 2, \dotsc, N-1 \}), \\ \ves{\xi} \sim \mcal{N}(\ve{0}, I)}} \Big[ d\big(\ve{f}_{\ves{\theta}}(\ve{x} + t_{n+1}\ves{\xi}, t_{n+1}), f_{\ves{\theta}^-}(\ve{x} + t_{n} \ves{\xi}, t_n)\big) \Big].
  \end{align*}

  \item The training algorithm is given by
  \begin{itemize}
    \item[] {\bf while} not convergent {\bf do}
    \item[] \begin{itemize}
      \item[] Sample $\ve{x} \sim p_{\data}$.
      \item[] Sample $n \sim \mcal{U}(\{ 1, 2, \dotsc, N-1\})$.
      \item[] Sample $\ves{\xi} \sim \mcal{N}(\ve{0}, I)$.
      \item[] $\mcal{L}(\ves{\theta},\ves{\theta}^-) \leftarrow \lambda(t_n) d(\ve{f}_{\ves{\theta}}(\ve{x} + t_{n+1}\ves{\xi}, t_{n+1}), \ves{f}_{\ves{\theta}^-}(\ve{x} + t_{n} \ves{\xi}, t_{n}))$
      \item[] Update $\ves{\theta}$ with $\nabla_{\ves{\theta}} \mcal{L}(\ves{\theta},\ves{\theta}^-)$.
      \item[] $\ves{\theta}^{-} \leftarrow \mrm{stopgrad}(\mu \ves{\theta}^- + (1-\mu)\ves{\theta})$
    \end{itemize}
    \item[] {\bf end while}
  \end{itemize}

  \item The following theorem characterize what the algorithm can achieve.
  \begin{theorem}
    Let $\Delta t := \max_{n \in \{1, 2, \dotsc, N-1\}}\{ t_{n+1} - t_{n} \}$. Assume that
    \begin{itemize}
      \item the metric $d$ and the target network $\ve{f}_{\ves{\theta}^-}$ are both twice continuously differentiable with bounded second derivatives,
      \item the weighting function $\lambda(\cdot)$ is bounded, 
      \item $E[\| \nabla_{\ve{x}} \log p_t(\ve{x}) \|_2^2] < \infty$, and
      \item if we use the Euler ODE solver, the pre-trained score model maches the ground truth (i.e., $\ve{s}_{\ves{\phi}}(\ve{x},t) = \nabla_{\ve{x}} p_t(\ve{x})$ for all $\ve{x}$ and $t$).
    \end{itemize}
    Then,
    \begin{align*}
      \mcal{L}^N_{\mrm{CD}}(\ves{\theta}, \ves{\theta}^-; \ves{\phi}) = \mcal{L}^N_{\mrm{CT}}(\ves{\theta},\ves{\theta}^-) + o(\Delta t).
    \end{align*}
    Moreover, $\mcal{L}_{\mrm{CT}}^N(\ves{\theta},\ves{\theta}^-) \geq O(\Delta t)$ if $\inf_{N} \mcal{L}_{\mrm{CD}}^N(\ves{\theta}, \ves{\theta}^-;\ves{\phi})> 0$.
  \end{theorem}

  \item From the above theorem, note that the loss $\mcal{L}^N_{\mrm{CT}}(\ves{\theta},\ves{\theta}^-) \geq O(\Delta t)$ is greater than the remainder $\mcal{L}^N_{\mrm{CD}}(\ves{\theta}, \ves{\theta}^-; \ves{\phi}) -  \mcal{L}^N_{\mrm{CT}}(\ves{\theta},\ves{\theta}^-) = o(\Delta t)$, and the loss itself will dominate the as $N \rightarrow \infty$ and $\Delta t \rightarrow 0$.
  
  \item For improved performance, the paper proposes increasing $N$ during training according to a schedule function $N(\cdot)$ that depends on the training iteration.
  \begin{itemize}
    \item When $N$ is small, it facilitates faster convergence at the beginning of training (i.e., less ``variance''). However, the resulting model would have more ``bias''.
    \item When $N$ is a large, the model has more ``variance'' but less ``bias''. This is desirable at the end of training, where variance should have been suppressed in earlier phases.
    \item As a result, $N$ should increase as training progresses.
  \end{itemize}

  \item The paper also found that $\mu$ should change along with $N$ according to a schedule function $\mu(\cdot)$.
  
  \item The paper uses the following $N(\cdot)$ and $\mu(\cdot)$ functions.
  \begin{align*}
    N(k) &= \bigg\lceil \sqrt{\frac{k}{K}((s_1 + 1)^2 - s_0^2) + s_0^2 + 1}  \bigg\rceil  + 1 \\
    \mu(k) &=  \exp\bigg( \frac{s_0 \log \mu_0}{N(k)} \bigg)
  \end{align*}
  where
  \begin{itemize}
    \item $k$ is the number of training iterations completed to far,
    \item $K$ is the total number of iterations,
    \item $s_0$ is the intiial discretization steps,
    \item $s_1 > s_0$ is the final discretization steps,
    \item $\mu_0 > 0$ denotes the EMA decay rate at the beginning of model training.
  \end{itemize}
  So, $N(k)$ increases and $\mu(k)$ decreases as training progresses.

  \item The consistency training loss can also be extended to the continous case where $N \rightarrow \infty$ and $\ves{\theta}^- = \mrm{stopgrad}(\ves{\theta})$. However, like the consistency distillation loss, it requires forward-mode differentiation.
\end{itemize}

\section{Experiments}

\begin{itemize}
  \item Datasets
  \begin{itemize}
    \item CIFAR-10
    \item ImageNet $64 \times 64$
    \item LSUN Bedrom $256 \times 256$
    \item LSUN Cat $256 \times 256$
  \end{itemize} 
  \item Metrics
  \begin{itemize}
    \item FID
    \item IS
    \item Precision and recall \cite{Kynkaanniemi:2019}
  \end{itemize}
\end{itemize}

\subsection{Training Consistency Models}

\begin{itemize}
  \item The experiments in the section were performed to understand
  \begin{itemize}
    \item the effect of the metric function $d$ ($\ell_1$, $\ell_2$, and LPIPS),
    \item the ODE solver in consistency distillation,
    \item the effect of the number of discretization steps $N$ in consistency distillation, and
    \item the effect of the schedules function $N(\cdot)$ and $\mu(\cdot)$ in consistency training.
  \end{itemize} 

  \item The experiments were performed with only the CIFAR-10 dataset.
  
  \item Consistency distillation experiments.
  \begin{itemize}
    \item The paper evaluated $N$ values from the set $\{ 9, 12, 18, 36, 50, 60, 80, 129 \}$. 
    \item The ODE solvers were the Euler solver and the Heun's 2nd order solver \cite{Karras:2022}.
    \item The consistency models have the same architecture as the pre-trained diffusion model and were initialized from the diffusion model.
  \end{itemize}

  \item Consistency distillation results.
  \begin{itemize}
    \item LPIPS outperformed both $\ell_1$ and $\ell_2$ by a large margin.
    \item Heun ODE solver and $N = 18$ are the best choices, consistent with what Karras \etal\ recommended \cite{Karras:2022}.
    \item With the same $N$, Huen's solver uniformly outperforms the Euler solver. 
  \end{itemize}

  \item In the consistency training experiments, the models are initialized randomly.
  
  \item Consistency training results.
  \begin{itemize}
    \item Convergence of training is highly sensitive to $N$. Smaller $N$ leads to faster convergence bu worse sample. Higher $N$ leads to slower convergence but better samples.
    \item Adaptive schedules for $N$ and $\mu$ significantly improves convergence speed and sample quality.
  \end{itemize}
\end{itemize}

\subsection{Few-Step image Generation}

\begin{itemize}
  \item The paper compared images generated by CT and CD with the following distillation baselines:
  \begin{itemize}
    \item Progressive distillation (PD) \cite{Salimans:2022},
    \item Straight distillation \cite{Luhman:2021}, and
    \item DFNO \cite{Zhang:2022}.
  \end{itemize}
  Distilled models were trained from Karras \etal's models \cite{Karras:2022}.

  \item Observations with respective to PD and CD.
  \begin{itemize}
    \item Using the LPIPS metrics improves PD results compared to the $\ell_2$ distance in the original paper.
    \item Both PD and CD improve as more sampling steps are used.
    \item CD uniformly outperforms PD across all datasets, sampling steps, and metric functions, except one case.
  \end{itemize}

  \item CD outperforms all the other two distillation approaches.
  
  \item CIFAR-10 dataset at one-step generation.
  \begin{itemize}
    \item The best model with regards to FID ($1.85$) is StyleGAN-XL \cite{Sauer:2022}.
    \item The best model with regards to IS ($9.83$) is StyleGAN2 with adaptive discrimination augmentation \cite{Karras:2019, Karras:2020}. 
    \item CD achieved FID of $3.55$ and IS of $9.48$, which numerically cannot beat the best models yet.
    \item CT performed even worse than CT.
  \end{itemize}

  \item For 1-step ImageNet $64 \times 64$ generation, CD and CT did not beat BigGAN-deep \cite{Brock:2018} on any metrics.
  
  \item For 1-step LSUN Bedroom $256 \times 256$ generation, CD and CT still loses to StyleGAN2 on FID, but CD won on precision and recall, and CT won on precision ony.
  
  \item For 1-step LSUN Cat $256 \times 256$ generation, CD and CT both lose StyleGan2 on FID, but CD won on precision, but not recall.
  
  \item The metrics for CD and the GANs are quite close. I think this might be because consistency models do not have a way to trade diversity for fidelity like GANs yet?
\end{itemize}

\subsection{Zero-Shot Image Editing}

\begin{itemize}
  \item The paper demonstrated that CD models can be used to colorize images, super-resolution images, do SDEdits, and inpaintings, interpolations, and denoising using the multi-step sampling algorithm.
\end{itemize}

\bibliographystyle{alpha}
\bibliography{ddpm-consistency-model}  
\end{document}