\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\pmb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}

\title{Rotating Objects with Deep Feedforward Networks}

\begin{document}
  \maketitle

  Given an image of an object, we would like to generate an image of that object rotated around some axis, mostly vertical. This problem is often called ``view synthesis'' because we can think of it as changing the view transformation of the camera.

  In this document, we will discuss a series of paper that culminates in the paper by Park \etal~\cite{Park:2017}. We start with the paper by Tatarchenko \etal~\cite{Tatarchenko:2016}, which is probably the first and simplest paper that solves this problem with a feedforward network. While the paper can infer rough shapes from input images, the textures are too blurry to call the results high quality.

  We then take a detour to discuss the Spatial Transformer Network (STN) paper \cite{Jaderberg:2015}, which enables simple image transformtion (translating, scaling, rotating, cropping, and warping) inside a network. The paper itself does not solve the view synthesis problem, but its spetial transformation unit is a tool that can be used to implement optical flow inside a feed forward network. The technique is used by the Appearance Flow Network (AFN) paper to copy pixels of the input image to the synthesized view \cite{Zhou:2016}. AFN produces textures of higher quality than those produced by Tatarchecko \etal, but they are still not accurate enough.

  Lastly, Park \etal's Transformation-grounded View Synthesis Network (TVSN) extends AFN by adding a visibility mask and a second network to fill the holes left by occluded pixels and fix any artefacts caused by appearance flow. IMHO, the paper is interesting because it combines two ideas---reusing pixels through appearance flow and using GANs to generate fine-grained details---to produce better result.

  \section{A Straightforward Solution}

  \begin{itemize}
  	\item The input to the network is a pair $(\ve{x}, \ve{\theta})$ where $\ve{x}$ is an image of an object and $\theta$ is the desired viewpoint. Here, $\ve{\theta}$ is a vector $(\theta^{\mrm{az}, \theta^{\mrm{el}}})$ of azimuthal and elevation angles, respectively.

  	\item The output of the network is an image $\ve{y}$ where $\ve{y}$ is the image with the object in it viewed from viewpoint specified by $\ve{\theta}$. The paper also produces $\ve{d}$,the depth map of $\ve{y}$. However, we are not interested in the depth map part.

  	\item Note the the viewpoint associated with $\ve{x}$ is not given to the network; it must infer this itself.

  	\item The network has a encoder-decoder architecture.
  	\begin{itemize}
  		\item The encoder consists of three parts, which we will call $A$, $B$, and $C$.
  		\begin{itemize}
  			\item $A$ is a 6-layered purely convolutional network that converts the input image $\ve{x}$ to a vector $\ve{a} \in \Real^{1024}$.

  			\item $B$ consists of 2 fully connected layers. It converts the viewpoint $\ve{\theta}$ to a vector $\ve{b} \in \Real^{64}$.

  			\item $C$ also consists of 2 fully connected layers. It converts the concatenation of $\ve{a}$ and $\ve{b}$ to a vector $\ve{z} \in \Real^{1024}$. This is the hidden representation of the desired view.  		
  		\end{itemize}
  		Writing out the encoder as an equation, we have:
  		\begin{align*}
  			\ve{z} &= C(A(\ve{x}) \oplus B(\ve{\theta}))
  		\end{align*}
  		where $\oplus$ denotes vector concatenation.

  		\item The decode is a $6$-layered purely convolutional network that converts $\ve{z}$ to a $128 \times 128$ $3$-channel image.
  	\end{itemize}

  	\item Unfortunately, the paper \emph{does not} specify the loss function it uses. (WTF!) From the code,\footnote{\url{https://github.com/lmb-freiburg/mv3d/blob/master/nobg_dm.py}} it seems that it uses $L^2$ loss for the image and the $L^1$ for the depth map.

  	\item The training data is prepared from the ShapeNet dataset. The authors use $7496$ cars and $6742$ chairs. Each object is rendered from $36$ azimuthal angles and $5$ evaluation angles, so each object has $180$ viewing directions.

  	\item As started earlier, the paper achieves good overall shapes, but the textures are too blurry to be called high quality reconstructions.
  \end{itemize}

  \section{Spatial Transformer Network}

  We take a break from view synthesis to discuss a generally useful technique.

  \begin{itemize}
  	\item A \textbf{spatial transformer} is a differentiable module which applies a spatial transformation to a feature map during a single forward pass.
  	\begin{itemize}
  	  \item The transformation is conditioned on the particular input, producing a single output feature map. In other words, each example given to the transformation gets its own associated output.

  	  \item For multi-channel inputs, the same transformation is performed to each channel independently. 
  	\end{itemize}

  	\item A spetial transformer has three parts:
  	\begin{itemize}
  		\item The \textbf{localization network} takes in the input feature map and outputs the parameters of the spatial transformation that should be applied to the feature map.

  		\item The \textbf{grid generator} generates a set of points where the input map should be sampled from the parameters of the transformation outputted by the localization network.

  		\item The \textbf{sampler} samples the input map according to the grid points to produce the output. 
  	\end{itemize}

  	\item The localization network:
  	\begin{itemize}
  		\item Takes as input the feature map $U \in \Real^{C \times H \times W}$.
  		
  		\item Outputs $\theta$, the parameters of the transformation $\mc{T}_\theta$ to be applied to the feature map.

  		\item It can take any form but should include a final regression layer to produce the parameters $\theta$.
  	\end{itemize}

 	\item The spatial transformation outputted by the localization network can take many forms. One of them is the 2D affine transformation:
 	\begin{align*}
 		A_\theta = \begin{bmatrix}
 			\theta_{11} & \theta_{12} & \theta_{13} \\
 			\theta_{21} & \theta_{22} & \theta_{23}
 		\end{bmatrix},
 	\end{align*}
 	which has six parameters.

 	\item Note that the transformation transforms a \emph{target point} in the image plane of the output to a \emph{source point} to the image plane of the input. As such, the points transformed are always regular grid points in the output image plane. For example, the point-wise transformation according to $A_\theta$ is given by:
 	\begin{align*}
 		\begin{bmatrix}
 			x^s \\ y^s
 		\end{bmatrix}
 		= A_\theta \begin{bmatrix}
 			x^t \\ y^t \\ 1
 		\end{bmatrix}
 	    = \begin{bmatrix}
 			\theta_{11} & \theta_{12} & \theta_{13} \\
 			\theta_{21} & \theta_{22} & \theta_{23}
 		\end{bmatrix}
 		\begin{bmatrix}
 			x^t \\ y^t \\ 1
 		\end{bmatrix}
 	\end{align*}
 	where $(x^s, y^s)$ denotes the output source point and $(x^t, y^t)$ denotes a target point.

 	\item The paper used normalized coordinates. In other words, $-1 \leq x^t, y^t, x^s, y^s \leq 1$ when the points are within the bounds of the input or the output.

 	\item Let the output pixel lies on a regular grid $G = \{ G_i \}$ where $G_i = (x^t_i, y^t_i)$. Naturally, let $(x^s_i, y^s_i) := \mc{T}_\theta(G_i)$ be the corresponding source point generated by the spatial transformation.

 	\item Let $V$ denote the output map, and let $V^c_i$ denote the value of Channel $c$ at $G_i$. We define $V^c_i$ to be the convolution of the input map with a sampling kernel $k$ centered at $G_i$:
 	\begin{align*}
 		V_i^c := \sum_{n}^H \sum_{m}^W U_{nm}^c k(x_i^s - m; \Phi_x) k(y_i^s -n; \Phi_y)
 	\end{align*}
 	where $\Phi_x$ and $\Phi_y$ are parameters of the sampling kernel $k$.

 	\item Any kernels can be used as long as the subgradients can be defined for any $x_i^s$ and $y_i^s$ values.

 	\item Example kernels:
 	\begin{itemize}
 		\item The \emph{neareast neighbor kernel}:
 		\begin{align*}
 		 	V_i^c := \sum_{n}^H \sum_{m}^W U_{nm}^c \delta(\lfloor x_i^s + 0.5 \rfloor - m) \delta(\lfloor y_i^s + 0.5 \rfloor - n)
 		\end{align*}
 		\item The \emph{bilinear sampling kernel}:
 		\begin{align*}
 			V_i^c := \sum_{n}^H \sum_{m}^W U_{nm}^c \max(0, 1 - |x_i^s - m|) \max(0, 1 - |y_i^s - n|)
 		\end{align*}
 		The paper \cite{Jaderberg:2015} has formula for partial derivatives with respect to all the parameters.
 	\end{itemize}

 	\item The spatial transformer can be inserted into any part of a neural network. Potential benefits include:
 	\begin{itemize}
 		\item It can speed up subsequent modules because it can downsample the feature maps.

 		\item It can increase the performance of the overall network because it enables the network to learn to transform the feature maps.

 		\item For some tasks, the output of the localization network can be fed to subsequent modules for further processing.

 		\item Multiple spatial transformers can be used in parallel to process multiple objects separately.
 	\end{itemize}
  \end{itemize}

  \section{Appearance Flow Network (AFN)}

  \begin{itemize}
  	\item The insight of the paper is to use the pixels of the input view to construct the output view. This approach works if the object is rotated by not too much or if the newly visible parts are similar to the ones already visible.

  	\item The input to the network is a source image $\ve{I}_s$ of an object and a relative transformation $T$, which is a generally a rotation.

  	\item The output is the image $\ve{I}_t$ of the object in $\ve{I}_s$ transformed by $T$.

  	\item Let $g$ denotes our desired network, the paper seeks to minimize the $L^p$ loss between the real target image and the one produced by the network:
  	\begin{align*}
  		\mc{L} := E_{(\ve{I}_s, T, \ve{I}_t) \sim p_{\mrm{data}}} [ \| \ve{I}_t - g(\ve{I}_s, T) \|_p ].
  	\end{align*}

  	\item Internally, the network first produces a dense flow field $\ve{f}$. For each pixel $i$, the value of $\ve{f}^{(i)}$ is a $2$-dimentional vector $(x^{(i)}, y^{(i)})$, specifying the position from the source image to sample from. Using bilinear interpolation, the output at Pixel $i$ is given by:
  	\begin{align*}
  		g^{(i)}(\ve{I}_s, T) 
  		:= \sum_{q \in \mrm{neigbors}(x^{(i)}, y^{(i)})}
  		I^{(q)}_s 
  		\max(0, 1-|x^{(i)} - x^{(q)}|)
  		\max(0, 1-|y^{(i)} - y^{(q)}|),
  	\end{align*}
  	and this is directly lifted from \cite{Zhou:2016}.

  	\item The network that produces the flow field $\ve{f}$ has the encoder-decoder artchitecture. The details are as follows:
  	\begin{itemize}
  		\item The flow field generator network has 3 parts:
  		\begin{itemize}
	  		\item The \emph{input view encoder} transforms the input image to a $4096$-dimensional vector. It uses 5 convolutional layers followed by 2 fully connected layers.

	  		\item The \emph{viewpoint transformation encoder} transforms the given viewpoint to a $256$-dimensional vector. It uses 2 fully connected layers.

	  		\item The \emph{synthesis decoder} concatenates the output of the last two parts and transfrom the result to a dense flow field. It uses 2 fully connected layers to transformed the $(4096+256)$-vector to a $4096$-vector. Then, the vector is passed through 6 (upsampling) convolution layers to generate the dense flow field.
  		\end{itemize}
  	\end{itemize}

  	\item The paper also has a network that predicts the foreground mask of the target image. It has the same architecture as the flow-field network above but produces a binary image. The network is trained with the cross entropy loss.

  	\item The technique can be extended to take advantage of multiple input views. 
  	\begin{itemize}
  		\item This is done by having the network produce a confidence map ${C}$ as an extra channel to the output. 

  		\item Given $m$ input images, we use the network to compute $m$ output images $g_1(\ve{I}_s, T)$, $\dotsc$, $g_m(\ve{I}_s, T)$ and $m$ confidence maps $\ve{C}_1$, $\dotsc$, $\ve{C}_m$.

  		\item The final output image is the weighted average of the outputs, using the confidence values as the weights:
  		\begin{align*}
  			g^{(i)}(\ve{I}_s, T) 
  			:=
  			\sum_{j=1}^m \bigg( \frac{\ve{C}^{(i)}_j}{ \sum \ve{C}^{(i)}_j} \bigg) g^{(i)}_j(\ve{I}_s, T).
  		\end{align*}
  	\end{itemize}

  	\item The textures generated by this technique is of higher quality than that of the Tatarchenko \etal~paper. However, since it only copy pixels from visible parts of the model, the textures of the occluded parts are not very accurate.
  \end{itemize}

  \section{Transformation-Grouded View Synthesis Network (TVSN)}

  \begin{itemize}
  	\item The paper's premise is to decompose view systhesis into a three-step process:
  	\begin{itemize}
  		\item Move pixels in the input view that are also visible in the output view to their new positions.

  		\item Remove the remaining pixels from the image.

  		\item Previously unseen pixels are hallucinated into existence.
  	\end{itemize}

  	\item The paper's network consists of two subnetworks:
  	\begin{itemize}
  		\item The \textbf{Disocclusion-aware Appearance Flow Network} (DOAFN) takes as input:
  		\begin{itemize}
  			\item the input view image, and
  			\item the desired relative transformation ($20^\circ$ to $340^\circ$ in increments of $20^\circ$, encoded a $17$-vector.
		\end{itemize}
		It outputs:
		\begin{itemize}
			\item A latent representation of the input image coupled with the transformation.

			\item An image of the new view where only the pixels that are shared with the input view are moved to their new positions, and the missing new pixels are left blank.
		\end{itemize}

		\item The \textbf{View Completion Network} (VNF) takes as input the outputs of the previous network and outputs the image of the new view. It has two jobs:
		\begin{itemize}
			\item Generate pixels that the previous network did not.
			\item Fix any artefects in the previous network's image.
		\end{itemize}
  	\end{itemize}
  \end{itemize}

  \subsection{Disocclusion-aware Appearance Flow Network (DOAFN)}

  \begin{itemize}
  	\item The DOAFN is very similar to the AFN. It has an encoder-decoder architecture.

  	\item Details of the encoder:
  	\begin{itemize}
  		\item Seven convolutional layers convert the input view image into a $2048$ dimensional latent vector.

  		\item Two fully connected layers transform the one-hot vector of the relative view to a $256$ dimensional latent vector.

  		\item The above two latent vectors are concatenated and then transformed by two fully connected layers into a $2048$-vector.  		
  	\end{itemize}

  	\item Details of the decoder:
  	\begin{itemize}
  		\item The decoder is a purely convolutional network consisting of 7 layers. However, the last layer has three heads, producing three outputs:
  		\begin{itemize}
  			\item The dense flow field $\ve{F}$.
  			\item The visibility mask $\ve{M}_{\mrm{vis}}$ that tells which pixels in the new view comes from the old view.
  			\item The background mask $\ve{M}_{\mrm{bg}}$ that tells which background pixels in the input view would remain unchanged in the output view.
  		\end{itemize}

  		\item The output of the first convolutional layer is an image block of size $512 \times 4 \times 4$ is taken as the latent representation of the rotated object. It is fed as an input to the VFN.
  	\end{itemize}

  	\item The final image that the DOAFN outputs is created from $\ve{F}$, $\ve{M}_{\mrm{vis}}$, and $\ve{M}_{\mrm{bf}}$.
  	\begin{itemize}
  		\item First, the flow field is used to bilinearly sample from the source image $\ve{I}_s$. Let us call this sampled image $\ve{I}_{\mrm{afn}}$.

  		\item The sampled image is mulitplied to the visibility mask. The resuilt is then addded to the extracted background:
  		\begin{align*}
  			\ve{I}_{\mrm{doafn}} = \ve{I}_{\mrm{afn}} \otimes \ve{M}_{\mrm{vis}} + \ve{I}_s \otimes \ve{M}_{\mrm{bg}}.
  		\end{align*}
  	\end{itemize}

  	\item The DOAFN can be trained independently without the rest of the network. The training data are generated as follows:
  	\begin{itemize}
  		\item The groundtruth background mask can be obtained from the rendering of the object. However, one needs to be careful that this is the intersection of the background masks of \emph{both} the input view and the output view. We can use the cross entropy loss with it.

  		\item To be the most correct, the visiliby map can be generated by shadow mapping. We may assume that there's a light source located at the camera position of the original view, the visible pixels in the new view are those that receive light when viewed from the new view. However, the paper determines visible pixels by whether the (old) normal vector is facing towards the camera in the new view. This is not entirely correct, though. Again, we can use the cross entropy loss with this.

  		\item The image $\ve{I}_{\mrm{afn}} \otimes \ve{M}_{\mrm{vis}}$ can be generated from the 3D models. We can use any $L^p$ loss with it.

  		\item Again, it was not clear to me what loss functions are used to trained the DOAFN. This was another WTF moment in this series of paper.
  	\end{itemize}

  	\item The paper trains DOAFN first before the VFN.
  \end{itemize}

  \subsection{View Completion Network (VFN)}

  \begin{itemize}
  	\item The VFN's architecture is that of U-Net \cite{Ronneberger:2015}. The only difference is that the $512 \times 4 \times 4$ latent representation from the DOAFN is inserted in one of the bottleneck layers. See the paper for more details. 

  	\item The paper says the architectural choice has the following advantages:
  	\begin{itemize}
  		\item Since the network is conditioned on the latent representation of DOAFN, it can generate content that have consistent attributes with the input view.

  		\item Since the output image of DOAFN is already in the target view, the pixels in the final output image are aligned with the output images. Hence, we can effectively use U-Net style skip connections.
  	\end{itemize}

  	\item Perhaps the most elaborate part of the VFN is the loss function.
  	\begin{itemize}
  		\item There is an \textbf{advesarial loss} term. While training the VFN, we also train a discriminator $D$ that distinguished between the real outputs and the outputs generated by the whole network, which we will call $G$. The loss of the discriminator is the standard GAN loss:
  		\begin{align*}
  			\mc{L}_D := 
  			-E [\log D(\ve{I}_t)] 
  			-E [\log (1 - D(G(\ve{I}_s, T)))].
  		\end{align*}  		
  		The adversarial loss for the generator is given by:
  		\begin{align*}
  			\mc{L}_{\mrm{adv}} := -E [\log D(G(\ve{I}_s, T))].
  		\end{align*}
  		Note that the expections are based on the random variable ${(\ve{I}_s, T, \ve{I}_t) \sim p_{\mrm{data}}}$.

  		\item The paper uses \textbf{feature matching} to stabilize training \cite{Salisman:2016}. Let $F_D$ denote the vector formed by the activations of the first three layers of the discriminator. The paper train the generator to minimize:
  		\begin{align*}
  		 	\mc{L}_{\mrm{fm}} := E \Big[ \| F_D(\ve{I}_t) - F_D(G(\ve{I}_s, T)) \|_2^2 \Big].
  		\end{align*} 
  		Note, however, this is not the exact feature matching loss in the Salisman \etal~paper because the expectation is pulled output the $L^2$ norm. This loss is more similar to the perceptual loss below.

  		\item It uses \textbf{perceptual loss} to match the image contents. Let $\ve{F}_{\mrm{vgg}}$ denote the vector formed by the activations of the first three layers of VGG16, and  The perceptual loss is given by:
  		\begin{align*}
  			\mc{L}_{\mrm{per}} := E \Big[ \| F_{\mrm{vgg}}(\ve{I}_t) - F_{\mrm{vgg}}(G(\ve{I}_s,T)) \|_2^2 \Big].
  		\end{align*}


  		\item Lastly, the paper also minizes the $L^1$ \textbf{difference} between the generated output and the ground truth and the \textbf{total variation loss} of the generated output.
  		\begin{align*}
  			\mc{L}_{\mrm{diff}} 
  			&:= E [ \| \ve{I}_t - G(\ve{I}_s, T) \|_1 ]\\
  			\mc{L}_{\mrm{tv}}
  			&:= E \bigg[ \sum_{i,j} ( G^{i+1,j}(\ve{I}_s, T) - G^{i,j}(\ve{I}_s, T) )^2 + ( G^{i,j+1}(\ve{I}_s, T) - G^{i,j}(\ve{I}_s, T) )^2 \bigg]
  		\end{align*}

  		\item The complete loss is given by:
  		\begin{align*}
  			\mc{L}_{\mrm{vfn}}
  			:= \mc{L}_{\mrm{adv}} + \alpha \mc{L}_{\mrm{fm}} + \beta \mc{L}_{\mrm{per}} + \gamma \mc{L}_{\mrm{diff}}  + \lambda \mc{L}_{\mrm{tv}}.
  		\end{align*}
  		The paper uses $\alpha = 1$, $\beta = 0.001$, $\gamma = 1$, and $\lambda = 0.0001$.
  	\end{itemize}

  	\item After we training the DOAFN, we train the VFN with the DOAFN fixed. Then, we fine-tune both networks end-to-end. The paper observes though that fine-tuning does not results in much improvement.

  	\item Insights from the Results section:
  	\begin{itemize}
  		\item Better textures can be assessed with $L_1$ difference from the ground truth.

  		\item All methods, even the baselines, achieved similar SSIM values.

  		\item If one replaces DOAFN with just simple AFN in the full network, it seems that both methods perform similarly with respect to the $L_1$ and SSIM metrics. However, AFN's artefacts can propagate to the final image while the use of DOAFN rules this out with the visibility map.

  		\item It seems the VFN needs both the adversarial loss and the VGG16 perceptual loss to achieve high quality results. VGG16 loss alone yields blurry output, and adversarial loss alone yields color and details inconsistencies. 
  	\end{itemize}

  \end{itemize}

  \bibliographystyle{acm}
  \bibliography{rotating-objects}  
\end{document}