<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Neural Scene Representation for Computer Graphics Applications</title>

    <!-- Bootstrap -->
    <link href="../../../css/bootstrap.min.css" rel="stylesheet">
    <link href="../../../css/theme.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: {autoNumber: "AMS"} } 
    });
    </script>
    <script type="text/javascript"
            src="../../../MathJax/MathJax.js?config=TeX-AMS_HTML-full">
    </script>

    <script type="text/javascript" src="../../../js/jquery-3.4.1.min.js"></script>    
    <script type="text/javascript" src="../../../js/bigfoot.min.js"></script>    

    <link rel="stylesheet" type="text/css" href="../../../css/bigfoot-default.css">    

</head>
<body>
<div class="container">
    <span style="visibility: hidden;">
        \(
        \def\sc#1{\dosc#1\csod}
        \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \newcommand{\iprod}{\mathbin{\lrcorner}}
        \)
    </span>

    <br>
    <h1>Neural Scene Representations for Computer Graphics Applications</h1>

    <p>There is a bunch of recent works on representing scenes and 3D objects implicitly with neural networks. This note is written as I read some of these papers.</p>
    <hr>


    <h2>1. &nbsp; Neural Radiance Fields</h2>

    <ul>
        <li>This is an ECCV 2020 paper by Mildenhall et al. <a href="https://www.matthewtancik.com/nerf">[LINK]</a></li>
        <li>This paper deals with the problem of <b>view synthesis.</b>
        <ul>
            <li>We are given multiple photographs of a scene, and we assume that the camera parameters for each photograph is also given.</li>

            <li>The goal is to render the scene from a new view point, given a new camera parameter.</li>
        </ul>
        </li>

        <li>The paper represent a scene with a continous 5D function.
        <ul>
            <li>The input is a 3D position $(x,y,z)$ and a direction $(\theta, \phi)$.</li>
            <li>The function outputs two things:
            <ul>
                <li>The material density $\sigma$ at point $(x,y,z)$.</li>
                <li>The RGB radiance from point $(x,y,z)$ in the direction $(\theta,\phi)$.</li>
            </ul>
            </li>
        </ul>
        This function is called the <b>neural radiance field</b> (NeRF).
        </li>        

        <li>The paper optimizes an MLP (without any convolutional layers) to represent the function.</li>

        <li>NeRF can be rendered by any volume rendering algorithm.
        <ul>
            <li>The paper uses standard ray marching.</li>
        </ul>
        </li>

        <li>NeRF has two obvious advantages:
        <ul>
            <li>Because it encodes volume density, it can automatically handle translucency.</li>

            <li>Because radiance is also a function of direction, it can represent specular reflections.</li>
        </ul>
        </li>        

        <li>Because the rendering process is differentiable, we have an end-to-end optimization process that fits a neural representation to photographs.
        <ul>
            <li>The loss is just a simple L2 difference.</li>
        </ul>
        </li>

        <li>Two important tricks.
        <ul>
            <li>The 5D coordinate input is transformed into a sequence of sine and cosine values (similar to the <a href="https://arxiv.org/abs/1706.03762">positional encoding used in transformers</a>) before being fed to the network. This enables the network to represent high frequency details.</li>

            <li>To reduce the number of sample points along a ray, the paper proposes a two-level hierachical sampling algorithm.</li>
        </ul>
        </li>
    </ul>

    <h3>1.1 &nbsp; Rendering Algorithm</h3>

    <ul>
        <li>We start with a review of volume rendering. More details can be found in <a href="../../gfx/volume-rendering/volume-rendering.pdf">another note of mine</a>. In particular, see Section 2.1 and 2.2.</li>

        <li>In the setting of NeRF, we have a volume consisting of absorbing materials that is also emitting light. There is no scattering. NeRF encodes:
        <ul>
            <li>The absorption cross section $\sigma(\ve{x})$ where $\ve{x} \in \Real^3$.</li>

            <li>The radiance per density $\ve{c}(\ve{x},\omega)$ where $\omega \in \mathbb{S}^2$. With this, the source term in the radiative transfer equation can be written as:
            \begin{align*}
                Q(\ve{x},\omega) = \sigma(\ve{x},\omega) \ve{c}(\ve{x},\omega).
            \end{align*}
            </li>
        </ul>
        </li>

        <li>Let $L(\ve{x},\omega)$ denote the radiance going out of $\ve{x}$ in direction $\omega$. We have that the directional derivative of $L$ is given by:
        \begin{align*}
            \omega \cdot \nabla L(\ve{x}, \omega) 
            = \frac{\dee L(\ve{x} + s\omega, \omega)}{\dee s}
            = -\sigma(\ve{x},\omega)L(\ve{x},\omega) + \sigma(\ve{x},\omega) \ve{c}(\ve{x},\omega).
        \end{align*}
        </li>

        <li>The solution to the above equation is given as follows.
        <ul>
            <li>Let $\omega_0$ be a direction.</li>

            <li>Let $\ve{x}_0$ be a point in $\Real^3$ such that, at and beyond $\ve{x}_0$ in the direction $-\omega_0$, there is no light being emitted in direction $\omega_0$.
            <ul>
                <li>In other words, $\ve{x}_0$ may be a point on the boundary of the scene.</li>

                <li>By this assumption, we have that $L(\ve{x}_0,\omega_0) = 0$.</li>
            </ul>            
            </li>

            <li>Then, for any $s_0 > 0$, we have that:
            \begin{align*}
                L(\ve{x}_0 + s_0 \omega_0,\omega_0)
                = \int_{0}^{s_0} T(\ve{x}_0 + s\omega_0 \rightarrow \ve{x}_0 + s_0 \omega_0) \sigma(\ve{x}_0 + s\omega_0, \omega_0)\ve{c}(\ve{x}_0 + s\omega_0, \omega_0)\, \dee s
            \end{align*}
            where
            \begin{align*}
                T(\ve{x}_0 + s\omega_0 \rightarrow \ve{x}_0 + s_0\omega_0) 
                = \exp\bigg( - \int_s^{s_0} \sigma(\ve{x}_0 + u\omega_0)\, \dee u \bigg).
            \end{align*}
            is the <b>transmittance</b> between $\ve{x}_0 +s\omega_0$ and $\ve{x}_0 + s_0\omega_0$.
            </li>
        </ul>
        </li>

        <li>We now discuss how to approximate the above integral with ray marching.</li>

        <li>
            Suppose we have a camera ray $\ve{r}(s) = \ve{o} + s \omega$ that intersects the scene's bounding box in $s$-interval $[s_n, s_f]$.
        </li>

        <li>We would like to compute $L(\ve{o},-\omega) = L(\ve{o}+s_n \omega, -\omega)$.</li>

        <li>A quantity that will be important is the transmisstancde $T(\ve{o} + s \omega \rightarrow \ve{o} + s_n \omega)$ where $s > s_n$. Let us abbrevitate this by just $T(s)$.</li>

        <li>We partition $[s_n,s_f]$ into $N$ evenly-spaced bins, and then draw one sample uniformly at random from within each bin:
        \begin{align*}
            s_i \sim \mathrm{Uniform}\bigg( \bigg[ s_n + \frac{i-1}{N}(s_f - s_n), t_n + \frac{i}{N}(s_f - s_n) \bigg] \bigg).
        \end{align*}
        </li>

        <li>We then evaluate the NeRF at $\ve{o} + s_i\omega$. Let 
        \begin{align*}
        \sigma_i &= \sigma(\ve{o} + s_i\omega), \\
        \ve{c}_i &= \ve{c}(\ve{o} + s_i\omega, -\omega), \\
        \delta_i &= s_i - s_{i-1}.
        \end{align*}
        where $s_0 = s_n$. Also, let $\sigma_0 = 0$.
        </li>

        <li>The mental picture is that we have now partition in interval $[s_n, s_f]$ into $N+1$ subinternals:
        \begin{align*}
            [s_0, s_1), [s_1, s_2), \dotsc, [s_{N-1}, s_N), [s_N, s_{N+1}]
        \end{align*}
        where $s_{N+1} = s_f$. For the purpose of approximating the integral, we assume that the $i$th interval has uniform absorption cross section $\sigma_{i-1}$.
        </li>

        <li>With the above mental picture, we have $T(s_i)$ is given by:
        \begin{align*}
            T(s_i) \approx T_i = \exp\bigg( -\sum_{j=1}^i \delta_i \sigma_{i-1} \bigg).
        \end{align*}
        Moreover, for any $s$ such that $s_{i} \leq s_{i+1}$, we have that
        \begin{align*}
            T(s) 
            &= \exp\bigg(-\int_{s_0}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &= \exp\bigg(-\int_{s_0}^{s_i} \sigma(\ve{o} + u \omega )\, \dee u -\int_{s_i}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &= \exp\bigg(-\int_{s_0}^{s_i} \sigma(\ve{o} + u \omega )\, \dee u\bigg) \exp\bigg( -\int_{s_i}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &= T(s_i) \exp\bigg( -\int_{s_i}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &\approx T_i \exp(-\sigma_i(s - s_i)).
        \end{align*}
        </li>

        <li>Now,
        \begin{align*}
            L(\ve{o},-\omega)
            &= \int_{s_0}^{s_{N+1}} T(s) \sigma(\ve{o} + s\omega, -\omega) \ve{c}(\ve{o} + s\omega, -\omega)\, \dee s \\
            &= \sum_{i=0}^{N} \int_{s_{i}}^{s_{i+1}} T(s) \sigma(\ve{o} + s\omega, -\omega) \ve{c}(\ve{o} + s\omega, -\omega)\, \dee s \\
            &\approx \sum_{i=0}^{N} \int_{s_{i}}^{s_{i+1}} T_i \exp(-\sigma_i(s - s_i)) \sigma_i \ve{c}_i \, \dee s \\
            &= \sum_{i=0}^{N} T_i \ve{c}_i \int_{s_i}^{s_{i+1}} \sigma_i \exp(-\sigma_i(s-s_i))\, \dee s.
        \end{align*}    
        Let $t = s - s_i$, we have
        \begin{align*}
        \int_{s_i}^{s_{i+1}} \sigma_i  \exp(-\sigma_i(s-s_i))\, \dee s
        &= \int_0^{\delta_{i+1}} \sigma_i e^{-\sigma_i t}\, \dee t
        = [ - e^{\sigma_i t} ]_{0}^{\delta_{i+1}}
        = 1 - e^{-\sigma_i \delta_{i+1}}.
        \end{align*}
        As a result,
        \begin{align*}
            L(\ve{o},-\omega)
            &\approx \sum_{i=0}^N T_i \ve{c}_i (1 - e^{-\sigma_i \delta_{i+1}}).
        \end{align*}
        Because $\sigma_0 = 0$, we can rewrite the above expression as:
        \begin{align*}
            L(\ve{o},-\omega)
            &\approx \sum_{i=1}^N T_i \ve{c}_i (1 - e^{-\sigma_i \delta_{i+1}})
            = \sum_{i=1}^N T_i (1 - e^{-\sigma_i \delta_{i+1}}) \ve{c}_i,
        \end{align*}
        which is very similar to the expression given in the paper.
        </li>
    </ul>

    <h3>1.2 &nbsp; Hierarchical Sampling</h3>
    
    <ul>
        <li>Instead of using one network, the paper optimizes two networks together in order to use them to perform hierarchical sampling.</li>

        <li>The two networks have the same structure. One is called the "coarse" network, and the other is called the "fine" network.</li>

        <li>Given a ray $\ve{r}(s) = \ve{o} + s\omega$, we compute two estimates of the radiance.</li>

        <li>The first estimate use the ray marching algorithm above with the coarse network. For this, we choose $N_c$ samples and compute:
        \begin{align*}
            \hat{\ve{C}}_c(\ve{r})
            &= \sum_{i=1}^{N_c} T_i  (1 - e^{-\sigma_i \delta_{i+1}}) \ve{c}_i.
        \end{align*}        
        </li>       

        <li>Letting $w_i = T_i  (1 - e^{-\sigma_i \delta_{i+1}})$, we can write $\hat{\ve{C}}_c(\ve{r})$ as a weighted sum of the $\ve{c}_i$'s:
        \begin{align*}
        \hat{\ve{C}}_c(\ve{r})
            &= \sum_{i=1}^{N_c} w_i \ve{c}_i.
        \end{align*}
        </li>

        <li>We normalize the weights by computing:
        \begin{align*}
            \hat{w_i} = \frac{w_i}{\sum_{i=1}^{N_c} w_i}.
        \end{align*}
        The weights give a piecewise-constasnt PDF along the ray. (It is unclear what intervals the paper uses for this PDF. I presume that it is the original equally-sized intervals.)
        </li>

        <li>From the above PDF, the paper samples $N_f$ samples. It then uses the $N_c + N_f$ samples to estimate the radiance with the fine network. The resulting value $\hat{\ve{C}}_f(\ve{r})$ is the output radiance value.</li>

        <li>The sampling strategy above will put more samples in areas where visible materials are present.</li>

        <li>The paper uses $N_c = 64$ and $N_f = 128$ in their experiments.
        <ul>
            <li>This kind of mean that the rendering cannot be real time.</li>
        </ul>
        </li>
    </ul>

    <h3>1.3 &nbsp; Network Architecture and Optimization</h3>

    <h4>1.3.1 &nbsp; Positional Encoding</h4>
    <ul>
        <li>The input to the network is the 5-tuple $(x,y,z,\theta,\phi).$
        <ul>
            <li>Each comoponent is normalized to lie in the interval $[-1,1]$. (I don't think it is necessary to do this to $\theta$ and $\phi$ though.)</li>
        </ul>
        </li>

        <li>The paper found that operating directly on these coordinates makes it hard for the network to learn high-frequency features.</li>

        <li>For each compontent $p$ of the input tuple, the network maps it to:
        \begin{align*}
            \gamma(p)
            = \begin{bmatrix}
                \sin(2^0\pi p) \\
                \cos(2^0\pi p) \\
                \sin(2^1\pi p) \\
                \cos(2^1\pi p) \\
                \vdots \\
                \sin(2^{L-1}\pi p) \\
                \cos(2^{L-1}\pi p) \\
            \end{bmatrix}
        \end{align*}        
        </li>

        <li>The paper uses $L = 10$ for each of the $xyz$-components, an $L = 4$ for each of the $\theta\phi$-compnents.</li>

        <li>The mappings are then passed to the next part of the network.</li>
    </ul>

    <h4>1.3.2 &nbsp; The Main Network Body</h4>

    <ul>
        <li>The positional encoding of the 3D position $\gamma(\ve{x}) \in \Real^{60}$ is first processed with an MLP with 8 fully connected layers.
        <ul>
            <li>Each layer having 256 neurons, except the last one, which has 257 neurons.</li>

            <li>One component of the output of the last layer is the density. The rest is a 256-dimensional feature vector.</li> 
        </ul>

        <li>The 256-dimensional feature vector is then concatenated with $\gamma(\theta,\phi) \in \Real^{16}$.</li>

        <li>The above vector is then passed to another fully-connected layer with 128 neurons and ReLU activation. The output is then passed to another fully connected layer with 3 neurons to produce the RGB color. (The paper does not write which activation function is used here.)</li>
    </ul>

    <h4>1.3.3 &nbsp; Optimization</h4>

    <ul>
        <li>From the camera settings of the input photographs, we generate rays $\ve{r}$ that look into the pixels of the photos. We then optimize the following loss:
        \begin{align*}
            \mathcal{L} = \sum_{\ve{r}} \Big[ \| \hat{\ve{C}}_c(\ve{r}) - \ve{C}(\ve{r}) \|^2 + \| \hat{\ve{C}}_f(\ve{r}) - \ve{C}(\ve{r}) \|^2 \Big]
        \end{align*}
        where $\ve{C}(\ve{r})$ denote the ground truth radiance of the ray, obtained from the color of the corresponding pixel in the photograph.
        </li>

        <li>The paper uses a batch size of $4096$ rays.</li>

        <li>The optimizating algorithm is Adam with $\beta_1 = 0.9, \beta = 0.999$. The learing rate starts at $5 \times 10^{-4}$ and decays exponentially to $5 \times 10^{-5}$ over the course of the optimization.</li>

        <li>A typical scene takes around 100k to 300k iterations to converge. This is about 1-2 days on a NVIDIA V100 GPU.</li>
    </ul>

    <hr>
    <div class="page-header"></div>
    <p>Last modified: 2021/03/28</p>    
</div>

<script type="text/javascript">
$.bigfoot();
</script>

</body>
</html>
