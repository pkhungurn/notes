<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Neural Scene Representation for Computer Graphics Applications</title>

    <!-- Bootstrap -->
    <link href="../../../css/bootstrap.min.css" rel="stylesheet">
    <link href="../../../css/theme.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: {autoNumber: "AMS"} } 
    });
    </script>
    <script type="text/javascript"
            src="../../../MathJax/MathJax.js?config=TeX-AMS_HTML-full">
    </script>

    <script type="text/javascript" src="../../../js/jquery-3.4.1.min.js"></script>    
    <script type="text/javascript" src="../../../js/bigfoot.min.js"></script>    

    <link rel="stylesheet" type="text/css" href="../../../css/bigfoot-default.css">    

</head>
<body>
<div class="container">
    <span style="visibility: hidden;">
        \(
        \def\sc#1{\dosc#1\csod}
        \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \newcommand{\iprod}{\mathbin{\lrcorner}}
        \)
    </span>

    <br>
    <h1>Neural Scene Representations for Computer Graphics Applications</h1>

    <p>There is a bunch of recent works on representing scenes and 3D objects implicitly with neural networks. This note is written as I read some of these papers.</p>
    <hr>


    <h2>1 &nbsp; Neural Radiance Fields</h2>

    <ul>
        <li>This is an ECCV 2020 paper by Mildenhall et al. <a href="https://www.matthewtancik.com/nerf">[LINK]</a></li>
        <li>This paper deals with the problem of <b>view synthesis.</b>
        <ul>
            <li>We are given multiple photographs of a scene, and we assume that the camera parameters for each photograph is also given.</li>

            <li>The goal is to render the scene from a new view point, given a new camera parameter.</li>
        </ul>
        </li>

        <li>The paper represent a scene with a continous 5D function.
        <ul>
            <li>The input is a 3D position $(x,y,z)$ and a direction $(\theta, \phi)$.</li>
            <li>The function outputs two things:
            <ul>
                <li>The material density $\sigma$ at point $(x,y,z)$.</li>
                <li>The RGB radiance from point $(x,y,z)$ in the direction $(\theta,\phi)$.</li>
            </ul>
            </li>
        </ul>
        This function is called the <b>neural radiance field</b> (NeRF).
        </li>        

        <li>The paper optimizes an MLP (without any convolutional layers) to represent the function.</li>

        <li>NeRF can be rendered by any volume rendering algorithm.
        <ul>
            <li>The paper uses standard ray marching.</li>
        </ul>
        </li>

        <li>NeRF has two obvious advantages:
        <ul>
            <li>Because it encodes volume density, it can automatically handle translucency.</li>

            <li>Because radiance is also a function of direction, it can represent specular reflections.</li>
        </ul>
        </li>        

        <li>Because the rendering process is differentiable, we have an end-to-end optimization process that fits a neural representation to photographs.
        <ul>
            <li>The loss is just a simple L2 difference.</li>
        </ul>
        </li>

        <li>Two important tricks.
        <ul>
            <li>The 5D coordinate input is transformed into a sequence of sine and cosine values (similar to the <a href="https://arxiv.org/abs/1706.03762">positional encoding used in transformers</a>) before being fed to the network. This enables the network to represent high frequency details.</li>

            <li>To reduce the number of sample points along a ray, the paper proposes a two-level hierachical sampling algorithm.</li>
        </ul>
        </li>
    </ul>

    <h3>1.1 &nbsp; Rendering Algorithm</h3>

    <ul>
        <li>We start with a review of volume rendering. More details can be found in <a href="../../gfx/volume-rendering/volume-rendering.pdf">another note of mine</a>. In particular, see Section 2.1 and 2.2.</li>

        <li>In the setting of NeRF, we have a volume consisting of absorbing materials that is also emitting light. There is no scattering. NeRF encodes:
        <ul>
            <li>The absorption cross section $\sigma(\ve{x})$ where $\ve{x} \in \Real^3$.</li>

            <li>The radiance per density $\ve{c}(\ve{x},\omega)$ where $\omega \in \mathbb{S}^2$. With this, the source term in the radiative transfer equation can be written as:
            \begin{align*}
                Q(\ve{x},\omega) = \sigma(\ve{x},\omega) \ve{c}(\ve{x},\omega).
            \end{align*}
            </li>
        </ul>
        </li>

        <li>Let $L(\ve{x},\omega)$ denote the radiance going out of $\ve{x}$ in direction $\omega$. We have that the directional derivative of $L$ is given by:
        \begin{align*}
            \omega \cdot \nabla L(\ve{x}, \omega) 
            = \frac{\dee L(\ve{x} + s\omega, \omega)}{\dee s}
            = -\sigma(\ve{x},\omega)L(\ve{x},\omega) + \sigma(\ve{x},\omega) \ve{c}(\ve{x},\omega).
        \end{align*}
        </li>

        <li>The solution to the above equation is given as follows.
        <ul>
            <li>Let $\omega_0$ be a direction.</li>

            <li>Let $\ve{x}_0$ be a point in $\Real^3$ such that, at and beyond $\ve{x}_0$ in the direction $-\omega_0$, there is no light being emitted in direction $\omega_0$.
            <ul>
                <li>In other words, $\ve{x}_0$ may be a point on the boundary of the scene.</li>

                <li>By this assumption, we have that $L(\ve{x}_0,\omega_0) = 0$.</li>
            </ul>            
            </li>

            <li>Then, for any $s_0 > 0$, we have that:
            \begin{align*}
                L(\ve{x}_0 + s_0 \omega_0,\omega_0)
                = \int_{0}^{s_0} T(\ve{x}_0 + s\omega_0 \rightarrow \ve{x}_0 + s_0 \omega_0) \sigma(\ve{x}_0 + s\omega_0, \omega_0)\ve{c}(\ve{x}_0 + s\omega_0, \omega_0)\, \dee s
            \end{align*}
            where
            \begin{align*}
                T(\ve{x}_0 + s\omega_0 \rightarrow \ve{x}_0 + s_0\omega_0) 
                = \exp\bigg( - \int_s^{s_0} \sigma(\ve{x}_0 + u\omega_0)\, \dee u \bigg).
            \end{align*}
            is the <b>transmittance</b> between $\ve{x}_0 +s\omega_0$ and $\ve{x}_0 + s_0\omega_0$.
            </li>
        </ul>
        </li>

        <li>We now discuss how to approximate the above integral with ray marching.</li>

        <li>
            Suppose we have a camera ray $\ve{r}(s) = \ve{o} + s \omega$ that intersects the scene's bounding box in $s$-interval $[s_n, s_f]$.
        </li>

        <li>We would like to compute $L(\ve{o},-\omega) = L(\ve{o}+s_n \omega, -\omega)$.</li>

        <li>A quantity that will be important is the transmisstancde $T(\ve{o} + s \omega \rightarrow \ve{o} + s_n \omega)$ where $s > s_n$. Let us abbrevitate this by just $T(s)$.</li>

        <li>We partition $[s_n,s_f]$ into $N$ evenly-spaced bins, and then draw one sample uniformly at random from within each bin:
        \begin{align*}
            s_i \sim \mathrm{Uniform}\bigg( \bigg[ s_n + \frac{i-1}{N}(s_f - s_n), t_n + \frac{i}{N}(s_f - s_n) \bigg] \bigg).
        \end{align*}
        </li>

        <li>We then evaluate the NeRF at $\ve{o} + s_i\omega$. Let 
        \begin{align*}
        \sigma_i &= \sigma(\ve{o} + s_i\omega), \\
        \ve{c}_i &= \ve{c}(\ve{o} + s_i\omega, -\omega), \\
        \delta_i &= s_i - s_{i-1}.
        \end{align*}
        where $s_0 = s_n$. Also, let $\sigma_0 = 0$.
        </li>

        <li>The mental picture is that we have now partition in interval $[s_n, s_f]$ into $N+1$ subinternals:
        \begin{align*}
            [s_0, s_1), [s_1, s_2), \dotsc, [s_{N-1}, s_N), [s_N, s_{N+1}]
        \end{align*}
        where $s_{N+1} = s_f$. For the purpose of approximating the integral, we assume that the $i$th interval has uniform absorption cross section $\sigma_{i-1}$.
        </li>

        <li>With the above mental picture, we have $T(s_i)$ is given by:
        \begin{align*}
            T(s_i) \approx T_i = \exp\bigg( -\sum_{j=1}^i \delta_i \sigma_{i-1} \bigg).
        \end{align*}
        Moreover, for any $s$ such that $s_{i} \leq s_{i+1}$, we have that
        \begin{align*}
            T(s) 
            &= \exp\bigg(-\int_{s_0}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &= \exp\bigg(-\int_{s_0}^{s_i} \sigma(\ve{o} + u \omega )\, \dee u -\int_{s_i}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &= \exp\bigg(-\int_{s_0}^{s_i} \sigma(\ve{o} + u \omega )\, \dee u\bigg) \exp\bigg( -\int_{s_i}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &= T(s_i) \exp\bigg( -\int_{s_i}^{s} \sigma(\ve{o} + u \omega )\, \dee u \bigg) \\
            &\approx T_i \exp(-\sigma_i(s - s_i)).
        \end{align*}
        </li>

        <li>Now,
        \begin{align*}
            L(\ve{o},-\omega)
            &= \int_{s_0}^{s_{N+1}} T(s) \sigma(\ve{o} + s\omega, -\omega) \ve{c}(\ve{o} + s\omega, -\omega)\, \dee s \\
            &= \sum_{i=0}^{N} \int_{s_{i}}^{s_{i+1}} T(s) \sigma(\ve{o} + s\omega, -\omega) \ve{c}(\ve{o} + s\omega, -\omega)\, \dee s \\
            &\approx \sum_{i=0}^{N} \int_{s_{i}}^{s_{i+1}} T_i \exp(-\sigma_i(s - s_i)) \sigma_i \ve{c}_i \, \dee s \\
            &= \sum_{i=0}^{N} T_i \ve{c}_i \int_{s_i}^{s_{i+1}} \sigma_i \exp(-\sigma_i(s-s_i))\, \dee s.
        \end{align*}    
        Let $t = s - s_i$, we have
        \begin{align*}
        \int_{s_i}^{s_{i+1}} \sigma_i  \exp(-\sigma_i(s-s_i))\, \dee s
        &= \int_0^{\delta_{i+1}} \sigma_i e^{-\sigma_i t}\, \dee t
        = [ - e^{\sigma_i t} ]_{0}^{\delta_{i+1}}
        = 1 - e^{-\sigma_i \delta_{i+1}}.
        \end{align*}
        As a result,
        \begin{align*}
            L(\ve{o},-\omega)
            &\approx \sum_{i=0}^N T_i \ve{c}_i (1 - e^{-\sigma_i \delta_{i+1}}).
        \end{align*}
        Because $\sigma_0 = 0$, we can rewrite the above expression as:
        \begin{align*}
            L(\ve{o},-\omega)
            &\approx \sum_{i=1}^N T_i \ve{c}_i (1 - e^{-\sigma_i \delta_{i+1}})
            = \sum_{i=1}^N T_i (1 - e^{-\sigma_i \delta_{i+1}}) \ve{c}_i,
        \end{align*}
        which is very similar to the expression given in the paper.
        </li>
    </ul>

    <h3>1.2 &nbsp; Hierarchical Sampling</h3>
    
    <ul>
        <li>Instead of using one network, the paper optimizes two networks together in order to use them to perform hierarchical sampling.</li>

        <li>The two networks have the same structure. One is called the "coarse" network, and the other is called the "fine" network.</li>

        <li>Given a ray $\ve{r}(s) = \ve{o} + s\omega$, we compute two estimates of the radiance.</li>

        <li>The first estimate use the ray marching algorithm above with the coarse network. For this, we choose $N_c$ samples and compute:
        \begin{align*}
            \hat{\ve{C}}_c(\ve{r})
            &= \sum_{i=1}^{N_c} T_i  (1 - e^{-\sigma_i \delta_{i+1}}) \ve{c}_i.
        \end{align*}        
        </li>       

        <li>Letting $w_i = T_i  (1 - e^{-\sigma_i \delta_{i+1}})$, we can write $\hat{\ve{C}}_c(\ve{r})$ as a weighted sum of the $\ve{c}_i$'s:
        \begin{align*}
        \hat{\ve{C}}_c(\ve{r})
            &= \sum_{i=1}^{N_c} w_i \ve{c}_i.
        \end{align*}
        </li>

        <li>We normalize the weights by computing:
        \begin{align*}
            \hat{w_i} = \frac{w_i}{\sum_{i=1}^{N_c} w_i}.
        \end{align*}
        The weights give a piecewise-constasnt PDF along the ray. (It is unclear what intervals the paper uses for this PDF. I presume that it is the original equally-sized intervals.)
        </li>

        <li>From the above PDF, the paper samples $N_f$ samples. It then uses the $N_c + N_f$ samples to estimate the radiance with the fine network. The resulting value $\hat{\ve{C}}_f(\ve{r})$ is the output radiance value.</li>

        <li>The sampling strategy above will put more samples in areas where visible materials are present.</li>

        <li>The paper uses $N_c = 64$ and $N_f = 128$ in their experiments.
        <ul>
            <li>This kind of mean that the rendering cannot be real time.</li>
        </ul>
        </li>
    </ul>

    <h3>1.3 &nbsp; Network Architecture and Optimization</h3>

    <h4>1.3.1 &nbsp; Positional Encoding</h4>
    <ul>
        <li>The input to the network is the 5-tuple $(x,y,z,\theta,\phi).$
        <ul>
            <li>Each comoponent is normalized to lie in the interval $[-1,1]$. (I don't think it is necessary to do this to $\theta$ and $\phi$ though.)</li>
        </ul>
        </li>

        <li>The paper found that operating directly on these coordinates makes it hard for the network to learn high-frequency features.</li>

        <li>For each compontent $p$ of the input tuple, the network maps it to:
        \begin{align*}
            \gamma(p)
            = \begin{bmatrix}
                \sin(2^0\pi p) \\
                \cos(2^0\pi p) \\
                \sin(2^1\pi p) \\
                \cos(2^1\pi p) \\
                \vdots \\
                \sin(2^{L-1}\pi p) \\
                \cos(2^{L-1}\pi p) \\
            \end{bmatrix}
        \end{align*}        
        </li>

        <li>The paper uses $L = 10$ for each of the $xyz$-components, an $L = 4$ for each of the $\theta\phi$-compnents.</li>

        <li>The mappings are then passed to the next part of the network.</li>
    </ul>

    <h4>1.3.2 &nbsp; The Main Network Body</h4>

    <ul>
        <li>The positional encoding of the 3D position $\gamma(\ve{x}) \in \Real^{60}$ is first processed with an MLP with 8 fully connected layers.
        <ul>
            <li>Each layer having 256 neurons, except the last one, which has 257 neurons.</li>

            <li>One component of the output of the last layer is the density. The rest is a 256-dimensional feature vector.</li> 
        </ul>

        <li>The 256-dimensional feature vector is then concatenated with $\gamma(\theta,\phi) \in \Real^{16}$.</li>

        <li>The above vector is then passed to another fully-connected layer with 128 neurons and ReLU activation. The output is then passed to another fully connected layer with 3 neurons to produce the RGB color. (The paper does not write which activation function is used here.)</li>
    </ul>

    <h4>1.3.3 &nbsp; Optimization</h4>

    <ul>
        <li>From the camera settings of the input photographs, we generate rays $\ve{r}$ that look into the pixels of the photos. We then optimize the following loss:
        \begin{align*}
            \mathcal{L} = \sum_{\ve{r}} \Big[ \| \hat{\ve{C}}_c(\ve{r}) - \ve{C}(\ve{r}) \|^2 + \| \hat{\ve{C}}_f(\ve{r}) - \ve{C}(\ve{r}) \|^2 \Big]
        \end{align*}
        where $\ve{C}(\ve{r})$ denote the ground truth radiance of the ray, obtained from the color of the corresponding pixel in the photograph.
        </li>

        <li>The paper uses a batch size of $4096$ rays.</li>

        <li>The optimizating algorithm is Adam with $\beta_1 = 0.9, \beta = 0.999$. The learing rate starts at $5 \times 10^{-4}$ and decays exponentially to $5 \times 10^{-5}$ over the course of the optimization.</li>

        <li>A typical scene takes around 100k to 300k iterations to converge. This is about 1-2 days on a NVIDIA V100 GPU.</li>
    </ul>
    <hr>

    <h2>2 &nbsp; Dynamic NeRF [Gafni et al. 2020]</h2>

    <ul>
        <li>Here's <a href="https://gafniguy.github.io/4D-Facial-Avatars/">the project page</a>.</li>
        
        <li>Problem specification.
        <ul>
            <li><b>Input:</b> A monocular video of a human talking in front of a static camera.</li>
            <li><b>Output:</b> A NeRF that satisfies the following contracts.
            <ul>
                <li><b>Inputs</b>
                <ul>
                    <li>A 3D position $(x,y,z).$</li>
                    <li>A direction $\ve{d} = (\theta,\phi)$.</li>
                    <li>A pose parameter $\delta \in \Real^{76}$.</li>
                    <li>A per-frame latent code $\gamma \in \Real^{32}$.</li>
                </ul>
                </li>

                <li><b>Outputs</b>
                <ul>
                    <li>A volume density $\sigma$ at $(x,y,z)$.</li>
                    <li>An RGB radiance $\ve{c}$ emiited per (differential) mass from $(x,y,z)$ in direction $(\theta,\phi)$.</li>                    
                </ul>
                The outputs are of the human in the video frame indicated by $\gamma$, taking the pose specified by $\delta$.
                </li>
            </ul>
            In other words, this is a rigged NeRF of the human in the video.
            </li>
        </ul>
        </li>

        <li>The pose parameter $\delta$ is the one used by a 3D morphable model that is used by the <a href="https://arxiv.org/abs/2007.14808">Face2Face paper</a>.</li>

        <li>The latent code $\gamma$ is there to compensate for the fact that $\delta$ might not be able to capture all variations.
        <ul>
            <li>For certain, it does not capture shoulder movements or hair movements, which can be present in the video.</li>
        </ul>
        </li>

        <li>Network architecture.
        <ul>
            <li>The architecture is very similar to that of the NeRF paper. They just add the extra parameters as input.</li>

            <li>Look at the picture:
            <a href="dynamic-nerf-architecture.png"><img src="dynamic-nerf-architecture.png" width="600" /></a>
            </li>

            <li>Also, the paper also uses the "coarse" and the "fine" networks, just like how the NeRF paper does it.
            <ul>
                <li>However, this time $N_c = N_f = 64$.</li>
            </ul>
            </li>            
        </ul>
        </li>

        <li>Training.
        <ul>
            <li>Given a video with $M$ frames, let $I_i$ denote the $i$th frame where $i \in \{1, 2, \dotsc, M\}$.</li>

            <li>Let $\gamma_i$ denote the latent code for the $i$th frame. The paper says that these latent codes are learnable parameters.</li>

            <li>For each frame, the paper uses the 3D morphable model to regress the pose parameter $\delta_i$ and the transformation matrix $P_i$ associated with the head rotation.</li>

            <li>Let $\Theta$ be the parameters of a network. The lost associated with the $i$th frame is given by:
            \begin{align*}
                \mathcal{L}_i(\Theta) = \sum_{j \in \mathrm{pixels}} \| C(\ve{r}_j; \Theta, P_i, \delta_i, \gamma_i) - I_i[j] \|^2
            \end{align*}
            where
            <ul>
                <li>$\ve{r}_j$ is the camera ray associated with Pixel $j$. The ray is transformed into the object space by $P_i^{-1}$ first before being fed to the underlying NeRF.</li>
                
                <li>$C(\ve{r}_j; \Theta, P_i, \delta_i, \gamma_i)$ is the radiance seen through the ray $\ve{r}_j$ given the relevant parameters.</li>
            </ul>
            </li>

            <li>To facilitate foreground-background segmentation, the paper modifies the radiance computation procedure a little.
            <ul>
                <li>In addition to the video, the paper also takes a snapshot of the background (i.e., the scene without a person talking).</li>

                <li>The function $C$ is modified so that the last sample along each rays evaluates to the associated background pixel's color.</li>
            </ul>
            </li>

            <li>The overall loss is the sum of the losses of the coarse and the fine networks over the frames.
            \begin{align*}
                \mathcal{L}_{\mathrm{total}}
                = \sum_{i=1}^M \Big( \mathcal{L}_i(\Theta_{\mathrm{coarse}}) + \mathcal{L}_i(\Theta_{\mathrm{fine}}) \Big).
            \end{align*}
            </li>

            <li>In real training, however, the paper does not use the whole images in a batch. It instead samples 2048 rays such that 95% of the rays intersect the head's bounding box.</li>

            <li>The opimization algorithm was Adam with learning rate $5 \times 10^{-4}$.</li>

            <li>Each model is trained for 400k iterations.</li>

            <li>Input image size was 512.</li>

            <li>The paper mentions that it needs a large number of training images---more than those of static scene reconstruction---to be able to generalize on the poses.</li>
        </ul>
        </li>
    </ul>
    <hr>

    <h2>3 &nbsp; Portrait NeRF [Gao et al. 2020]</h2>

    <ul>
        <li><a href="https://portrait-nerf.github.io/">Link to the project page</a>.</li>

        <li>This paper presents a way to generate (non-posable) NeRF from a single photograph.
        <ul>
            <li>The input photograph contains a face looking straight ahead. It covers the upper head, hairs, and torso.</li>
        </ul>
        </li>

        <li>The paper looks at the problem as an instance of meta-learning.
        <ul>
            <li>The paper pretrains a NeRF with portrait images obtained from light stage captures.</li>

            <li>At test time, we are given a portrait image and use it to finetune the above NeRF.</li>

            <li>In meta-learning lingo:
            <ul>
                <li>The problem of synthesizing view from a camera pose is called a <b>query</b>.</li>

                <li>Light stage captures are called the <b>labels</b>.</li>

                <li>Training a subject specific NeRF is called a <b>task</b>.</li>

                <li>At test time, we are given a single label (the frontal capture), and we need to optimize the <b>testing task</b>, which must be able to answer multiple queries.</li>
            </ul>
            </li>

            <li>The paper learns a single NeRF and uses it to initialize the weight of the NeRF that is going to be optimize at test time. It does so by solving multiple tasks given the labels. These are called <b>meta-training tasks</b>.
            <ul>
                <li>That is, we learn a single MLP for multple subjects.</li>
            </ul>
            </li>
        </ul>
        </li>
    </ul>

    <h3>3.1 &nbsp; Overview</h3>

    <ul>
        <li>The method contains two stages:
        <ul>
            <li><b>Pretraining</b>
            <ul>
                <li>We train a coordinate-based NeRF $f_\Theta$ on subjects captured from a light stage.</li>

                <li>The end project is a model parameter $\Theta^*_p$.</li>
            </ul>
            </li>

            <li><b>Testing</b>
            <ul>
                <li>We initialize the NeRF with $\Theta^*_p$.</li>
                <li>We finetune it on the frontal view of the test subject.</li>
            </ul>
            </li>
        </ul>
        </li>

        <li>To address the variety of size and translation among different subjects, the input to the NeRF is defined in a <b>canonical face coordinate</b> (i.e., the object space). Camera rays are transformed into these coordinates before being fed to the NeRF.</li>
    </ul>

    <h3>3.2 &nbsp; Training Data</h3>

    <ul>
        <li>The paper uses a light stage to cpature multiple subjects.</li>

        <li>For each subject:
        <ul>
            <li>The paper construct a 3D mesh of the subject.</li>

            <li>It then renders a 5 $\times$ 5 training views. These views form a uniform grid on the solid angles with $25^\circ$ vertical field-of-view and $15^\circ$ horizonal field-of-view and with the center being the view looking straight to the subject's face.            
            </li>

            <li>The set containing only the center view is called the <b>support set</b> and is denoted by $\mathcal{D}_s$.</li>

            <li>The set containing the other 24 views are called the <b>query set</b> and is denoted by $\mathcal{D}_q$.</li>
        </ul>
        </li>        
    </ul>

    <h3>3.3 &nbsp; Pretraining</h3>

    <ul>
        <li>Let there be $K$ subjects in the training data. The subjects are indexed by $m \in \{0,1,\dotsc, K-1\}$.</li>

        <li>The paper starts with a NeRF with randomly initialized parameter $\Theta_{p,-1}$.</li>

        <li>It then loops for the subjects, one by one, from Subject $m = 0$ to Subject $m = K-1$.
        <ul>
            <li>Now, consider Subject $m$.</li>

            <li>We train two NeRF, whose parameters are denoted by $\Theta_m$ and $\Theta_{p,m}$.
            <ul>
                <li>$\Theta_m$ is optimized with both $\mathcal{D}_s$ and $\mathcal{D}_q$. We discard it after the iteration. Its only use is to generate gradients to update $\Theta_{p,m}$.</li>

                <li>$\Theta_{p,m}$ is optimized with only $\mathcal{D}_q$. We use it in the next iteration.</li>
            </ul>
            </li>

            <li>First, we optimize $\Theta_m$ with data from $\mathcal{D}_s$.
            <ul>
                <li>We initialize $\Theta_m^0 \gets \Theta_{p,m-1}$.</li>

                <li>For $N_s$ iterations, we perform the gradient update:
                \begin{align*}
                    \Theta_m^{t+1} 
                    \gets \Theta_m^t - \alpha \frac{\partial \mathcal{L}(\mathcal{D}_s, \Theta^t_m)}{\partial \Theta_m^t}.
                \end{align*}
                where $\mathcal{L}(\mathcal{D}_s, \Theta^t_m)$ is the L2 loss between the view predicts by the NeRF with parameters $\Theta^t_m$, and $\alpha$ is the learning rate.
                </li>

                <li>Let $\Theta_m^*$ denote the parameter at the end of the $N_s$th iteration, $\Theta^{N_s}_m$.</li>
            </ul>
            </li>

            <li>Second, we optimize $\Theta_m$ and $\Theta_{p,m}$ jointly.
            <ul>
                <li>We initialize $\Theta_m^0 \gets \Theta_m^*$ and $\Theta_{p,m}^0 \gets \Theta_{p,m-1}$.</li>

                <li>For $N_q$ iterations, we perform the following gradient updates:
                \begin{align*}
                    \Theta_{m}^{t+1} &\gets \Theta_m^t - \beta \frac{\partial \mathcal{L}(\mathcal{D}_q, \Theta^t_m)}{\partial \Theta_m^t} \\
                    \Theta_{p,m}^{t+1} &\gets \Theta_{p,m}^t - \beta \frac{\partial \mathcal{L}(\mathcal{D}_q, \Theta^t_m)}{\partial \Theta_m^t}.
                \end{align*}
                where $\beta$ is the learning rate.
                </li>

                <li>At the end, we set $\Theta_{p,m} \gets \Theta_{p,m}^{N_q}$.</li>
            </ul>            
            </li>
        </ul>
        In the end, the pretrained parameters $\Theta_p^*$ is $\Theta_{K-1}^*$.
        </li>
    </ul>

    <h3>3.4 &nbsp; Finetuning</h3>

    <ul>
        <li>Finetuning at test time is equivalent to performing the $N_s$ gradient updates as discussed the last section.</li>
    </ul>

    <h3>3.5 &nbsp; Canonical Face Space</h3>

    <ul>
        <li>For each subject $m$ in the training data, we approximate its geometry with a 3D morphable model. Say, the morphable model for Subject $m$ is $F_m$.</li>

        <li>We average the geometry to obtain an average mesh $\overline{F}$.</li>

        <li>We use the correspondence between $F_m$ and $\overline{F}$ to find a rigid transform $\ve{x} \mapsto s_m R_m \ve{x} + \ve{t}_m$ where $s_m$ is a scaling factor, $R_m$ is a rotation matrix, and $\ve{t}_m$ is translation vector.
        <ul>
            <li>This optimization is done by computing an SVD. (More details in the paper.</li>
        </ul>
        </li>

        <li>The above rigid transformation is applied to the incoming ray before feeding it to the underlying NeRf.</li>

        <li>At test time, the rigid transformation can be computed in the same way.</li>
    </ul>

    <hr>
    <div class="page-header"></div>
    <p>Last modified: 2021/03/28</p>    
</div>


<script type="text/javascript">
$.bigfoot();
</script>

</body>
</html>
