<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Image Translation through Landmarks</title>

    <!-- Bootstrap -->
    <link href="../../../css/bootstrap.min.css" rel="stylesheet">
    <link href="../../../css/theme.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: {autoNumber: "AMS"} } 
    });
    </script>
    <script type="text/javascript"
            src="../../../MathJax/MathJax.js?config=TeX-AMS_HTML-full">
    </script>

    <script type="text/javascript" src="../../../js/jquery-3.4.1.min.js"></script>    
    <script type="text/javascript" src="../../../js/bigfoot.min.js"></script>    

    <link rel="stylesheet" type="text/css" href="../../../css/bigfoot-default.css">    

</head>
<body>
<div class="container">
    <span style="visibility: hidden;">
        \(
        \def\sc#1{\dosc#1\csod}
        \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \)
    </span>

    <br>
    <h1>Image Translation through Landmarks</h1>
    <hr>

    <p>This article covers three papers:</p>

    <ul>
        <li>
        Jakab et al.'s <i>Unsupervised Learning of Object Landmarks through Conditional Image Generation</i> (NuerIPS 2018) <a href="https://papers.nips.cc/paper/7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generation">[LINK]</a>
    </li>
    <li>
        Siarohin et al.'s <i>Animating Arbitrary Objects via Deep Motion Transfer</i> (CVPR 2019) 
        <a href="http://www.stulyakov.com/papers/monkey-net.html">[LINK]</a>
    </li>
    <li>
        Siarohin et al.'s <i>First Order Motion Model for Image Animation</i> (NIPS 2019) <a href="https://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation">[LINK]</a>
    </li>    
    </ul>

    <p>Ultimately, what is achieved is the task of animating an image using the movement that comes from a video sequence. The last paper is notatble because it generates a very convincing animation using an interesting method.</p>

    <p>At the core of these papers is a way to extract landmark in images without labelled data. This, in general, should be useful in my future research.</p>

    <h2>Jakab et al. (NeurIPS 2018)</h2>

    <ul>
        <li>The problem the paper tries to solve is landmark detection: given an image, output coordinates of points that correspond to landmars (the nose, the eyes, and the mouth of a face) in the image.</li>

        <li>The paper does so without requiring labelled images.</li>

        <li>The training data the paper uses are pairs of images of the same object. The images are different due to:
            <ul>
                <li>The images might be taken from different viewpoints.</li>
                <li>The object is deformable, and it has different shapes in the two images.</li>
                <li>The second image is obtained from the first image by some kind of image transformations such as warping.</li>
            </ul>
        </li>

        <li>The training data can be obtained through several means:
            <ul>
                <li>Take two different frames from a video.</li>
                <li>Programmatically warping images.</li>
            </ul>
        </li>

        <li>The key insight of the paper is that positions of landmarks should be useful to the task of translating the first image in the pair to the second. So, the paper attempts to solve the following proxy task: <i>Given two images $\ve{x}$ and $\ve{x}'$ of the same object, extract the landmark positions of $\ve{x}'$ and then try to regenerate $\ve{x}'$ from $\ve{x}$ and the landmark positions of $\ve{x}'$.</i>
        </li>

        <li>More specifically:
            <ul>
                <li>Let $\mathcal{X} = \mathbb{R}^{C \times H \times W}$ denote the set of images.</li>

                <li>Let $\ve{x}, \ve{x}' \in \mathcal{X}$. Call $\ve{x}$ the <i>source image</i> and $\ve{x}'$< the <i>target image</i>.</li>

                <li>Let $\Omega$ denote the domain of the pixel. In other words, $\Omega = [0,H] \times [0,W]$.</li>
                
                <li>Let $\mathcal{Y} = \Omega^K$ denote the set of $K$-tuple of points in $\Omega$. These are the domain of the landmarks.</li>

                <li>We are interested in learning a mapping $\Phi(\ve{x}) = \ve{y} \in \mathcal{Y}$ that maps images to $K$ landmark positions.</li>

                <li>For convenience, let $\ve{y} = (u_1, u_2, \dotsc, u_K)$ where $u_i \in \Omega$. Let us further denote $u_i$ by $(u^{(1)}_i, u^{(2)}_i)$.</li>

                <li>To learn $\Phi$ in an unsupervised manner, we learn another mappin $\Psi: \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{X}$ such that $\Psi(\ve{x}, \ve{y}') = \Psi(\ve{x}, \Phi(\ve{x}')) = \ve{x}'$.
                    <ul>
                        <li>In other words, $\Psi$ reconstructs the target image $\mathcal{x}'$ from the source image $\ve{x}$ and the detected landmarks $\ve{y}' = \Phi(\ve{x}')$ of the target image.</li>
                    </ul>
                </li>
            </ul>
        </li>

        <li>The paper learns $\Phi$ and $\Psi$ joinly to solve the following minimization problem:
            \begin{align*}
                \min_{\Phi, \Psi} E_{\ve{x}, \ve{x}'} [\mathcal{L}(\ve{x}', \Psi(\ve{x}, \Phi(\ve{x}')))].
            \end{align*}
        </li>

        <li>To force $\Phi$ to extract landmarks, the paper forces the network to output $K$ positional outputs with a three step process:
            <ol>
                <li>The network is forced to output $K$ heatmaps. Let us denote the $k$th heatmap by $S_k(\ve{x})$. The pixel value at coordinate $u$ of the heapmap is denoted by $S_k(\ve{x})[u] = S_k(\ve{x})[u^{(1)}, u^{(2)}]$.</li>

                <li>The network is then computes the position of the lardmark by (1) turning the heatmaps into a probability distribution over $\Omega$ by applying the softmax function and (2) computing the expected value of the positions with this probability distribution:
                    \begin{align*}
                    u_k = \frac{\int_\Omega u \exp(S_k(\ve{x})[u]) \ \dee u}{\int_\Omega \exp(S_k(\ve{x})[u]) \ \dee u}.
                    \end{align*}
                While we use integrals to formulate the above expression, they become sums over the pixels in real implementations.
                </li>

                <li>Lastly, the network converts the positions into renderings of a 2D Gaussian with a fixed standard deviation $\sigma$:
                    \begin{align*}
                    \Phi_k(\ve{x})[u] = \frac{1}{\sqrt{2\pi}\sigma} \exp\bigg( \frac{-\| u - u_k \|^2}{2 \sigma^2} \bigg).
                    \end{align*}
                Note that the end result is the tensor $\Phi(\ve{x}) \in \mathbb{R}^{K \times H \times W}$. This is equivalent to the coordinates and is more readily usable by the network $\Psi$.
                </li>
            </ol>
        </li>

        <li>The loss function for the generator network is the perceptual content loss:
        \begin{align*}
            \mathcal{L}(\ve{x}', \tilde{\ve{x}}')
            &= \sum_l \alpha_l \| \Gamma_l(\ve{x}') - \Gamma_l(\tilde{\ve{x}}') \|_2^2
        \end{align*}
        where $\Gamma_l$ denots the $l$-th used subnetwork of VGG-19, and $\alpha_l$ is the weight of the loss term involving $\Gamma_l$.
            <ul>
                <li>The paper uses the <tt>input</tt>, <tt>conv1_2</tt>, <tt>conv2_2</tt>, <tt>conv3_2</tt>, <tt>conv4_2</tt>, and <tt>conv5_2</tt>.</li>

                <li>The weights $\alpha_l$s are updated online during training using the technique in <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Photographic_Image_Synthesis_ICCV_2017_paper.pdf">Chen and Koltun paper</a>.</li>
            </ul>
        </li>

        <li>Now, for the details of the network architectures:
            <ul>
                <li><b>The landmark detector network ($\Phi$).</b>
                    <ul>
                        <li>The networks consist of a series of blocks.</li>
                        <li>Each block has two convolutional layers. It also doubles the number of channels of the previous block.</li>
                        <li>All layers use a $3 \times 3$ kernel.
                            <ul>
                                <li>The exception is the very first layer, which uses a $7 \times 7$ kernel.</li>
                            </ul>
                        </li>
                        <li>The first layer of each block downsamples the image by a factor of two with a stride 2 convolution.
                            <ul>
                                <li>Again, the exception is the very first layer, which does not downsample.</li>
                            </ul>
                        </li>
                        <li>I think the first layer of each block also doubles the number of channels. The second layer probably preserves the dimension of the input.
                            <ul>
                                <li>The very first layer, though, just change the number of channels from 3 to 32.</li>
                            </ul>
                        </li>
                        <li>The downsampling blocks continue until the feature tensor has spetial dimension of $16 \times 16$.
                            <ul>
                                <li>Let $H = W = 2^n$. Then, there must be $n-3$ blocks (because the first layer does not downsample). The output tnesor is of dimension $16 \times 16 \times (32 \cdot 2^{n-3}).$</li>
                            </ul>
                        </li>
                        <li>The train of blocks is then followed by a $1 \times 1$ convolution that changes the number of channels to $K$.</li>
                        <li>The above tensor is then used to render 2D Gaussian with $\sigma = 0.1$ on $K$ $16 \times 16$ images.</li>
                    </ul>
                </li>
                <li><b>The image generation network ($\Psi$).</b>
                    <ul>
                        <li>The image generation network receives two inputs.
                            <ul>
                                <li>The source image $\ve{x}$.</li>
                                <li>The detected landmarks $\mathcal{y}' = \Phi(\ve{x}')$ in the form of rendered images of Gaussians.</li>
                            </ul>
                        </li>

                        <li>First, the network converts the source image $\ve{x}$ into a feature tensor $\ve{z}$ of size $16 \times 16 \times C$ where $C = 32 \cdot 2^{n-3}$. This is done using a new network with the same architecture as the series of blocks in the landmark detector network.</li>

                        <li>Then, $\ve{z}$ and $\ve{y}'$ are concatenated in the channel dimension. The output is then pass to a decoder network.</li>

                        <li>The decoder network consists of a sequence of blocks. Each block has two convolutional layers.</li>

                        <li>The input to each block, except the very first block, is upsampled by a factor of 2 using bilinear interpolation.</li>

                        <li>The each block halves the number of channels. I don't know which convolutional layer does it, but I would presume the first one.
                            <ul>
                                <li>However, the number of channels is maintained to be at least 32 until the end of the sequence.</li>
                            </ul>
                        </li>

                        <li>The sequence of blocks continues until the spetial dimension matches that of the input.</li>

                        <li>Lastly, a final convolution layer changes the number of channels to $3$ so as to produce an image.</li>

                        <li>Btw, almost all layers use the $3 \times 3$ convolutional filter.</li>
                    </ul>
                </li>
            </ul>
        </li>
    </ul>

    <h2>Siarohin et al. (CVPR 2019)</h2>

    <ul>
        <li>While Jakab et al. focuses on landmark detection, this paper focus on the motion transfer problem. It uses detected landmarks to pass information about the object's pose.</li>

        <li>The input consists of a <i>source image</i> whose movement is to be copied from that of a <i>driving video</i>.</li>

        <li>Training data for Monkey-Net are two frames from videos of the same objects, just like the Jakab et al. paper.</li>

        <li>The network the paper discribes, called Monkey-Net (MOviNg KEYpoint), has three main modules:
        <ul>
            <li>The <b>Keypoint Detector</b> takes the source image and a frame from the driving video and extract sparse keypoints (landmarks) for the two images.</li>

            <li>The <b>Dense Motion Prediction Network</b> translates the sparse keypoints into an optical flow map.</li>

            <li>The <b>Motion Transfer Network</b> combines the source image and the optical flow map to produce the target image.</li>
        </ul>
        </li>

        <li>More formally:
        <ul>
            <li>Let $\ve{x}$ and $\ve{x}'$ be two frames of dimension $3 \times H \times W$.</li>

            <li>Let $\mathcal{X}$ be the domain of the video frames. In other words, $\mathcal{X} = \Real^{3 \times H \times W}.$</li>

            <li>Let $\mathcal{U}$ denote the $H \times W$ lattice (i.e., the set of pixels).</li>

            <li>The paper jointly learns a keypoint detector $\Delta$ together with a generator network $G$ so that $G$ should be able to reconstruct $\ve{x}'$ from $\Delta(\ve{x})$, $\Delta(\ve{x}')$, and $\ve{x}$.</li>

            <li>The paper adds a third network $M$ that estimates the optical flow $\mathcal{F} \in \Real^{2 \times H \times W}$ which tells us, if we were to reconstruct $\ve{x}'$ from $\ve{x}$, where would one copy each pixel of $\ve{x}'$ from. $M$ computes the optical flow from $\Delta(\ve{x})$, $\Delta(\ve{x}')$, and $\ve{x}$.</li>

            <li>The paper adds $M$ for two reasons:
            <ul>
                <li>This forces $\Delta$ to predict keypoint locations that capture not only the object structure but also motion.</li>

                <li>The Motion Transfer Network is implemented with standard encoder-decoder architecture. As a result, it cannot deal with large motions well.</li>
            </ul>
            </li>
        </ul>
        </li>
    </ul>

    <h3>Keypoint Detection Network</h3>
    <ul>
        <li>The paper uses U-Net to compute $K$ heatmaps $\tilde{H}_k \in [0,1]^{H \times W}$, one for each keypoint.
            <ul>
            <li>The last layer of this part uses softmax to turn a raw feature map output to a probability distribution over $\mathcal{U}$.</li>
            </ul>
        </li>

        <li>The paper the computes the keypoint locations by computing the expected value of the pixel coordinates according to the probability in the heatmap:
        \begin{align*}
            h_k = \sum_{p \in \mathcal{U}} \tilde{H}_k[p] p.
        \end{align*}
        However, the paper also computes the covariance matrix of the Gaussian:
        \begin{align*}
            \Sigma_k = \sum_{p \in \mathcal{U}} \tilde{H}_k[p] (p - h_k) )( p - h_k)^T.
        \end{align*}
        The paper does so because it hopes the covariance matrix would give information about the keypoint's orientation.
        </li>

        <li>Then, for each heatmap, a 2D Gaussian with mean $\ve{h}_k$ and covariance $\Sigma_k$ is rendered:
        \begin{align*}
            H_k[p] = \frac{1}{\alpha} \exp(- (p - h_k)^T \Sigma_k^{-1}(p - h_k))
        \end{align*}
        where $\alpha$ is a normalization constant.
        </li>

        <li>The above process is applied to both $\ve{x}$ and $\ve{x}$', resulting in two sets of $K$ keypoint heatmaps $H = \{ H_k \}_{k=1}^K$ and $H' = \{ H_k' \}_{k=1}^K$</li>
    </ul>
    
    <h3>Motion Transfer Network</h3>
    
    <ul>
        <li>The motion transfer network reconstructs $\ve{x}'$ from $\ve{x}$, $\Delta(\ve{x}) = H$, and $\Delta{\ve{x}'} = H'$.</li>

        <li>The overall architecture of the network is a U-Net, but with a twist.</li>

        <li>First, $\ve{x}$ is passed to a standard encoder composed of convolutions and average pooling. Let $\ve{\xi}_r \in \Real^{C_r \times H_r \times W_r}$ denote the output of the $r$th block of then encoder network (which has $R$ encoding blocks).</li>

        <li>The paper then uses the optical flow map $\mathcal{F}$ of align the features $\ve{\xi}_r$ with $\ve{x}'$. In other words, it uses bilinear sampling function $f_w$ to "warp" the feature map:
        \begin{align*}
            \ve{x}_r' = f_w(\ve{\xi}_r, \mathcal{F}).
        \end{align*}
        In order to carry out this operation, $\mathcal{F}$ is downsampled to $H_r \times W_r$ via nearest neighbor interpolation.
        </li>

        <li>These $\ve{\xi}'_r$ would then be shipped through the above the valley of the U shape to the decoder part of the U-Net instead of $\ve{\xi}_r$.</li>

        <li>However, encoding motion with $\mathcal{F}$ alone led to problems when training the network because the parameters of $\Delta$ are faraway from the loss function. To remedy this, the input to the decoder also includes $\dot{H} = H' - H$.
        <ul>
            <li>Similar to $\ve{\xi}'_r$ the paper computes $\dot{H}_r$ by downsampling $\dot{H}$ to $H_r \times W_r$.</li>

            <li>The skip connection carries the concatenation between $\ve{\xi}'_r$ and $H_r$.</li>
        </ul>
        </li>
    </ul>

    
    <h3>Dense Motion Prediction Network</h3>
    <ul>
        <li>The network's job is to predict $\mathcal{F}$ from $\ve{x}$, $\Delta(\ve{x})$ and $\Delta(\ve{x}')$.</li>

        <li>The paper adopts a part-based formulation. It makes the assumption that each keypoint is located on an object that is locally rigid.</li>

        <li>In other words, consider the $k$th keypoint. It is located at $h_k$ in $\ve{x}$ and $h_k'$ in $\ve{x}'$. As result, the (inverse) optical flow map around $h_k'$ should be $\dot{h}_k = h_k - h_k'$.</li>

        <li>More concretely, the paper estimate masks $M_k \in \Real^{H \times W} for $0 \leq k \leq K$. The <i>coarse</i> optical flow is given by:
        \begin{align*}
        \mathcal{F}_{\mathrm{coarse}} = M_0 \otimes \rho([0,0]) + \sum_{k=1}^{K} M_k \otimes \rho(\dot{h}_k).
        \end{align*}
        Here,
        <ul>
            <li>$\otimes$ denotes element-wise multiplication.</li>
            <li>$\rho(\cdot)$ is the function that outputs a $\Real^{2 \times H \times K}$ image obtained by repeating the vector that is the input of the function over the entire image.</li>
        </ul>
        Note that $\rho([0,0])$ is the <i>backgound</i> flow image where nothing moves, and $M_0$ is the mask for the background.
        </li>

        <li>The network also predicts another flow $\mathcal{F}_{\mathrm{residual}}$, which rerves to augments the coarse optical flow, from scratch.</li>

        <li>The final optical flow is given by:
        \begin{align*}
        \mathcal{F} = \mathcal{F}_{\mathrm{coarse}} + \mathcal{F}_{\mathrm{residual}}.
        \end{align*}
        </li>

        <li>The architecture of this network is still a U-Net.</li>

        <li>However, there's a misalignment between the input and the output of U-Net. The input $\ve{x}$ is align with itself, but the output $\mathcal{F}$ is aligned with $\ve{x}'$. This misalignment can degrade U-Net performance.</li>

        <li>Hence, instead of providing $\ve{x}$ to the network, the network instead warps $\ve{x}$ using the constant optical flow map $\rho(\dot{h}_k)$:
        \begin{align*}
            \ve{x}_k = f_w(\ve{x}, \rho(\dot{h}_k)).
        \end{align*}
        Each of $\ve{x}_k$ gives an image of $\ve{x}$ which is locally aligned with $\ve{x}'$ around the keypoint $h'_k$. The paper then feeds all the $\ve{x}_k$s, concatenated with $\dot{H}$, as input to the U-Net.
        </li>
    </ul>

    <h3>Training</h3>

    <ul>
        <li>All the networks are trained end-to-end as if they are one network.</li>

        <li>The loss used is a combination of an adversarial and the feature matching loss.</li>

        <li><b>Adversarial Loss</b>
        <ul>
            <li>The discriminator $D$ takes as input $H'$ concatenated with either the real image $\ve{x}'$ or the generated image $\hat{\ve{x}}'$.</li>

            <li>The adversarial loss is the least-square GAN loss (LSGAN):
            \begin{align*}
                \mathcal{L}^D_{\mathrm{gan}} 
                &= E_{\ve{x}'} [(D(\ve{x}', H') - 1)^2] + E_{\ve{x},\ve{x}'} [D(\hat{\ve{x}}', H')^2] \\
                \mathcal{L}^G_{\mathrm{gan}} 
                &= E_{\ve{x},\ve{x}'} [(D(\hat{\ve{x}}', H') - 1)^2]
            \end{align*}
            </li>

            <li>The keypoint locations $H'$ are provided to the discriminator to help it focus on moving parts and not on the background.</li>

            <li>When training the generator, gradient should not be propagated through $H'$ to prevent the generator from generating spurious landmark locations to fool the discriminator.</li>
        </ul>
        </li>

        <li><b>Fetaure Matching Loss</b>
        <ul>
            <li>The feature matching loss encourages the output images $\hat{\ve{x}}'$ and $\ve{x}'$ to yield similar feature representations.</li>

            <li>The features used are the intermediate layers of the discriminator $D$.
            \begin{align*}
            \mathcal{L}_{\mathrm{rec}} E_{\ve{x}, \ve{x}'} \bigg[ \sum_{i} \| D_i(\hat{\ve{x}}', H') - D_i(\ve{x}',H') \|_1 \bigg]
            \end{align*}
            where $D_i$ denots the output of the $i$th layer of the discriminator. $D_0$ denotes the discriminator input.
            </li>
            
            <li>It seems that the paper uses all the layers of the discriminator except the last for the feature matching loss.</li>
        </ul>

        <li>The total loss is given by:
        \begin{align*}
        \mathcal{L}_{\mathrm{tot}} = \lambda_{\mathrm{rec}} \mathcal{L}_{\mathrm{rec}} + \mathcal{L}^G_{\mathrm{gan}}.
        \end{align*}
        The paper uses $\lambda_{\mathrm{rec}} = 10$.
        </li>
    </ul>

    <h3>Generation Procedure</h3>

    <ul>
        <li>At test time, the network receives a driving video and an image.</li>

        <li>To generate the $t$th frame, we use the keypoint estimation network $\Delta$ to estimate:
        <ul>
            <li>the keypoints $\{h_k^s\}$ of the source image,</li>
            <li>the keypoints $\{h_k^1\}$ of the first frame of the driving video, and</li>
            <li>the keypoints $\{h_k^t\}$ of the $t$th frame of the driving video.</li>
        </ul>
        </li>

        <li>The source keypoints are then transfered using the displacements between $\{h_k^t\}$ and  $\{h_k^1\}$:
        \begin{align*}
            {h_k^s}' = h_k^s - (h_k^t - h_k^1).
        \end{align*}
        </li>

        <li>Both $\{h_k^t\}$ and $\{{h_k^s}'\}$ are then encoded as heatmaps. Then, the heatmaps are then passed to the dense motion prediction network and the motion transfer network, respectively.</li>

        <li>One limitation of the system is that the first frame of the video and the source image must be similar.</li>
    </ul>
    
    <h2>Siarohin et al. (NeurIPS 2019)</h2>

    <ul>
        <li>The paper extends the previous paper in two ways:
            <ul>
                <li>Each landmark is augmented with a <i>local affine transformation.</i></li>

                <li>The network also models occlusions.</li>
            </ul>
        </li>

        <li>The source image is denoted by $\ve{S} \in \Real^{3 \times H \times W}$.</li>

        <li>The driving video is denoted by $\mathcal{D}$, and a frame of this video is denoted by $\ve{D}$. Each frame is an element of $\Real^{3 \times H \times W}$.</li>

        <li>The network is composed to two main modules:
        <ul>
            <li>The <b>motion estimation module</b> predicts a dense motion field (i.e., backward optical flow) from a frame $\ve{D}$ to the source image $\ve{S}$.
            <ul>
                <li>It is used to align the features of $\ve{S}$ to those of $\ve{D}$.</li>

                <li>The output is modeled by a function $\hat{\mathcal{T}}_{\ve{S} \leftarrow \ve{D}}: \Real^2 \rightarrow \Real^2$ that maps each pixel location in $\ve{D}$ to a pixel location in $\ve{S}$. This is used to align features computed from $\ve{S}$ to those of $\ve{D}$.</li>

                <li>The network also outputs an occlusion mask $\hat{\mathcal{O}}_{\ve{S} \leftarrow \ve{D}}$, which indicates which parts of $\ve{D}$ can be constructed by copying pixels of the source image and which parts have to be inpainted.</li>
            </ul>
            </li>

            <li>The <b>image generation module</b> warps the source image according to $\hat{\mathcal{T}}_{\ve{S} \leftarrow \ve{D}}$ and inpaint pixels that are occluded according to $\hat{\mathcal{O}}_{\ve{S} \leftarrow \ve{D}}$.</li>
        </ul>
        </li>

        <li>To predict $\hat{\mathcal{T}}_{\ve{S} \leftarrow \ve{D}}$, the paper proceeds in two steps:
        <ol>
            <li>An encoder-decoder network approximates a <i>coarse</i> transformation $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$.
            <ul>
                <li>The paper assumes there is an abstract reference frame $\ve{R}$. It then estimate two transformations:
                <ul>
                    <li>from $\ve{R}$ to $\ve{S}$: $\mathcal{T}_{\ve{S} \leftarrow \ve{R}}$.</li>
                    <li>from $\ve{R}$ to $\ve{D}$: $\mathcal{T}_{\ve{D} \leftarrow \ve{R}}$.</li>
                </ul>
                This frame is an abstract concept that is never explicitly computed. It is there to make it possible to process $\ve{D}$ and $\ve{S}$ independently. With the frame, we can write $\mathcal{T}_{\ve{S} \leftarrow \ve{D}} = \mathcal{T}_{\ve{S} \leftarrow \ve{R}} \circ \mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1}.$
                </li>

                <li>Both $\mathcal{T}_{\ve{S} \leftarrow \ve{R}}$ and $\mathcal{T}_{\ve{D} \leftarrow \ve{R}}$ are estimated by a collection of affine transformations around $K$ keypoints.</li>

                <li>The locations of the keypoints in $\ve{D}$ and $\ve{S}$ are separately predicted by an encoder-decoder network.</li>

                <li>The motion in each neighborhood around a keypoint is approximated using an affine transformation.
                <ul>
                    <li>So, the encoder-decoder network also outputs the parameters of the affine transformation in addition to the keypoint locations.</li>
                </ul>
                </li>

                <li>The $\mathcal{T}_{\ve{S} \leftarrow \ve{R}}$ and $\mathcal{T}_{\ve{D} \leftarrow \ve{R}}$, aproximated as above, are then combined to yield a coarse transformation $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$, which is still modeled by $K$ local affine transformations.</li>
            </ul>
            </li>
            
            <li>A dense motion network uses $\mathcal{T}_{\ve{S} \leftarrow \ve{R}}$ to synthesize to the dense motion field $\hat{\mathcal{T}}_{\ve{S} \leftarrow \ve{D}}$. This network also outputs the occlusion mask $\hat{\mathcal{O}}_{\ve{S} \leftarrow \ve{D}}.$</li>
        </ol>
        </li>
    </ul>

    <h3>Computing the Coarse Transformation</h3>

    <ul>
        <li>Computing $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$ involves computing $\mathcal{T}_{\ve{S} \leftarrow \ve{R}}$ and $\mathcal{T}_{\ve{R} \leftarrow \ve{D}}$.</li>

        <li>Given a frame $\ve{X}$, the transformation $\mathcal{T}_{\ve{X} \leftarrow \ve{R}}$ is approximated with first order Taylor series expansions of $K$ keypoints $p_1$, $p_2$, $\dotsc$, $p_K$ where the keypoints are in the frame $\ve{R}$. We have that, around $p_k$:
        \begin{align*}
            \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p)
            &= \sum_{k=1}^K \bigg[ \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p_k)
            + \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg) (p-p_k) + o(\| p - p_k \|) \bigg].
        \end{align*}
        As a result, the motion function $\mathcal{T}_{\ve{X} \leftarrow \ve{R}}$ is represented (perhaps piecewisely) by its values at $p_k$s and the Jacobians around these values:
        \begin{align*}
            \mathcal{T}_{\ve{X} \leftarrow \ve{R}}
            \approx
            \Bigg\{ \bigg\{ \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p_k), \frac{\dee}{\dee p} \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg\}, \dotsc, \bigg\{ \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p_K), \frac{\dee}{\dee p} \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p) \bigg|_{p=p_K} \Bigg\}.
        \end{align*}
        </li>

        <li>However, we also need to estimate $\mathcal{T}_{\ve{R}\leftarrow\ve{X}} = \mathcal{T}_{\ve{X}\leftarrow\ve{R}}^{-1}.$ To do so, we need to assume that $\mathcal{T}_{\ve{X}\leftarrow\ve{R}}$ is locally bijective in the neighborhood of each keypoint (which is standard practice).</li>

        <li>Coming back to $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$:.
        <ul>
            <li>For convenience, let $z_k$ denote the $k$th keypoint in the $\ve{D}$ frame. In other words, $z_k = \mathcal{T}_{\ve{D}\leftarrow\ve{R}}(p_k).$</li>

            <li>We now need to estimate $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$ around each $z_k$.</li>

            <li>We rewrite $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$ as $\mathcal{T}_{\ve{S} \leftarrow \ve{R}} \circ \mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1}$.</li>

            <li>This allows us to estimate $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$ in two steps:
            <ul>
                <li>Estimate $\mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1} = \mathcal{T}_{\ve{R} \leftarrow \ve{D}}$ around each $z_k$.</li>

                <li>Estimate $\mathcal{T}_{\ve{S} \leftarrow \ve{R}}$ around $p_k = \mathcal{T}_{\ve{R} \leftarrow \ve{D}}(z_k)$.</li>

                <li>Using the first order Taylor series expansion, we have that, near $z_k$:
                \begin{align*}
                \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z)
                &= \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z_k)
                + \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z) \bigg|_{z=z_k} \bigg) (z-z_k) + o(\| z - z_k \|).
                \end{align*}
                </li>

                <li>For the $0$th order term, we have:
                \begin{align*}
                \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z_k)
                &= \mathcal{T}_{\ve{S} \leftarrow \ve{R}} \circ \mathcal{T}_{\ve{R} \leftarrow \ve{D}}(z_k) \\
                &= \mathcal{T}_{\ve{S} \leftarrow \ve{R}} \circ \mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1}(z_k) \\
                &= \mathcal{T}_{\ve{S} \leftarrow \ve{R}} \circ \mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1} \circ \mathcal{T}_{\ve{D} \leftarrow \ve{R}} (p_k) \\
                &= \mathcal{T}_{\ve{S} \leftarrow \ve{R}} (p_k).
                \end{align*}
                </li>

                <li>The first order term is approximated as:
                \begin{align*}
                \frac{\dee}{\dee z} \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z) \bigg|_{z=z_k}
                &= \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)
                \bigg( \frac{\dee}{\dee z} \mathcal{T}_{\ve{R} \leftarrow \ve{D}}(z) \bigg|_{z=z_k} \bigg) \\
                &= \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)
                \bigg( \frac{\dee}{\dee z} \mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1}(z) \bigg|_{z=z_k} \bigg).
                \end{align*}
                Because we have assume that $\mathcal{T}_{\ve{R} \leftarrow \ve{D}}$ is locally bijective around $z_k$, the inverse function theorem tells us that:
                \begin{align*}
                \frac{\dee}{\dee z} \mathcal{T}_{\ve{R} \leftarrow \ve{D}}(z) \bigg|_{z=z_k}
                = \frac{\dee}{\dee z} \mathcal{T}_{\ve{D} \leftarrow \ve{R}}^{-1}(z) \bigg|_{z=z_k}
                = \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{D} \leftarrow \ve{R}}(z) \bigg|_{p=p_k} \bigg)^{-1}.
                \end{align*}
                So,
                \begin{align*}
                \frac{\dee}{\dee z} \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z) \bigg|_{z=z_k}
                &= \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)
                \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{D} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)^{-1}.
                \end{align*}
                </li>

                <li>
                As a result,
                \begin{align*}
                \mathcal{T}_{\ve{S} \leftarrow \ve{D}}(z)
                \approx
                \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p_k)
                + \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)
                \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{D} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)^{-1}
                (z - \mathcal{T}_{\ve{D} \leftarrow \ve{R}}(p_k)).
                \end{align*}
                </li>
            </ul>
            </li>
        </ul>
        </li>

        <li>
        Now, the problem now becomes how to compute $\mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p_k)$ and the Jacobians around $p_k$.
        <ul>
            <li>$\mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p_k)$ is computed using the same algorithm as in Siarohin et al.'s CVPR 2019 paper.
            <ul>
                <li>For each keypoint $k$, the encoder-decoder network computes a heatmap.</li>

                <li>The heatmap is then turned into a probability distribution by applying the softmax function.</li>

                <li>The expected value of the pixel positions according to the above probability distribution gives the location $p_k$.</li>
            </ul>
            </li>

            <li>To compute the Jacobian $\frac{\dee}{\dee p} \mathcal{T}_{\ve{X}\leftarrow \ve{R}}(p)|_{p=p_k}$:
            <ul>
                <li>Note first that it is a $2 \times 2$ matrix, which contains 4 entries.</li>

                <li>The encoder-decoder network also outputs 4 more heatmaps, corresponding to each entry of the matrix.</li>

                <li>An entry is computed as the weighted average of the corresponding heatmap.
                <ul>
                    <li>The weight is the probability distribution used to computed the keypoint location.</li>
                </ul>
                </li>
            </ul>
            </li>    
        </ul>
        </li>
    </ul>

    <h3>Computing the Dense Optical Flow</h3>

    <ul>
        <li>The dense (backward) optical flow $\hat{\mathcal{T}}_{\ve{S} \leftarrow \ve{D}}$ is computed with a convolutional network $P$, which takes as input:
        <ul>
            <li>the source image $\ve{S}$, and</li>
            <li>a collection of local affine transformations $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$.</li>
        </ul>
        </li>

        <li>$P$ has U-Net architecture. Therefore, it is important for the input image $\ve{S}$ to be aligned with the frame of the output, which is that of $\ve{D}$.</li>

        <li>To create inputs that are aligned with $\ve{D}$, the paper uses the $K$ affine transformations in $\mathcal{T}_{\ve{S} \leftarrow \ve{D}}$ to compute $K$ source images $\ve{S}^1$, $\ve{S}^2$, $\dotsc$, $\ve{S}^K$. The paper also use $\ve{S}^0 = \ve{S}$ as the background.</li>

        <li>For each keypoint $p_k$, the paper also compute a heatmap $\ve{H}_k$ which encodes the keypoints in the form that is easily usable by $P$. It is defined as the different between two rendering of Gaussians centering at the keypoints:
        \begin{align*}
        \ve{H}_k(z) 
        &= \exp\bigg( \frac{-\|\mathcal{T}_{\ve{D} \leftarrow \ve{R}}(p_k) -z\|^2}{\sigma^2} \bigg) - \exp\bigg( \frac{-\|\mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p_k) -z\|^2}{\sigma^2} \bigg).
        \end{align*}
        The paper uses $\sigma = 0.1$.
        </li>

        <li>The heatmaps $\{\ve{H}_k\}$ and the transformed images $\{\ve{S}_k\}$ are concatenated and give as input of $P$.</li>

        <li>Now, $P$ outputs $K+1$ masks $\ve{M}_0$, $\ve{M}_1$, $\dotsc$, $\ve{M}_{K}$.</li>

        <li>The final dense motion field is given by:
        \begin{align*}
        \hat{\mathcal{T}}_{\ve{D} \leftarrow \ve{S}}
        &= M_0 \otimes z + \sum_{k=1}^K \ve{M}_k \otimes \Big( \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p_k) + J_k\big(z - \mathcal{T}_{\ve{D}\leftarrow \ve{R}}(p_k) \big) \Big)
        \end{align*}
        where
        \begin{align*}
        J_k = \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{S} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)
        \bigg( \frac{\dee}{\dee p} \mathcal{T}_{\ve{D} \leftarrow \ve{R}}(p) \bigg|_{p=p_k} \bigg)^{-1}.
        \end{align*}
        </li>

        <li>One difference I noticed between this NIPS 2019 paper and the previous CVPR 2019 paper is that there's no residual term ($\mathcal{F}_{\mathrm{residual}}$).</li>

        <li>Also, $P$ also output the occlusion map $\hat{\mathcal{O}}_{\ve{S} \leftarrow \ve{D}}$. This comes naturally by adding a layer to the output of $P$.</li>
    </ul>

    <h3>Output Image Generation</h3>

    <ul>
        <li>This time, the image generation network $G$ is not a U-Net. The paper uses the encoder-decoder architecture of <a href="https://arxiv.org/pdf/1603.08155.pdf">Johnson et at.'s paper.</a>
        <ul>
            <li>It has two downsampling blocks, six residual blocks, and two up-sample blocks.</li>
        </ul>
        </li>

        <li>The source image $\ve{S}$ is fed through the two downsampling blocks. Let $\ve{\xi} \in \Real^{C' \times H', \times W'}$ be the output of the last downsampling block.</li>

        <li>The paper warps each $\ve{\xi}$ with $\hat{\mathcal{T}}_{\ve{D} \leftarrow \ve{S}}$ (downsampled with nearest neighbor sampling to $H
        ' \times W'$ resolution) in order to make the features align with $\ve{D}$. Moreover, the paper also multiplies the output with the mask $\hat{\mathcal{O}}_{\ve{S} \leftarrow \ve{D}}$ to mask out the feature map regions that should be inpainted. In other words, we compute:
        \begin{align*}
            \ve{\xi}' = \hat{\mathcal{O}}_{\ve{S} \leftarrow \ve{D}} \otimes f_w(\ve{\xi}, \hat{\mathcal{T}}_{\ve{D} \leftarrow \ve{S}}).
        \end{align*}
        Then, $\ve{\xi}'$ is then fed to the next stage of the neural network.
        </li>
    </ul>

    <h3>Training Losses</h3>

    <ul>
        <li>The paper uses two losses:
        <ul>
            <li>Perceptual content loss based on <a href="https://arxiv.org/pdf/1603.08155.pdf">Johnson et al.'s paper</a>.</li>

            <li>Equivariance constraint loss.</li>
        </ul>
        </li>

        <li>For the perceptual loss: 
        <ul>
            <li>The implementation is ased on the implementation by <a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf">Wang et al.</a></li>

            <li>Let $\ve{D}$ denote a frame of the driving video. Let $\hat{\ve{D}}$ denote the generated frame.</li>

            <li>The reconstruction loss is given by:
            \begin{align*}
                \mathcal{L}_{\mathrm{rec}}(\hat{\ve{D}}, \ve{D})
                = \sum_{i=1}^I \frac{1}{P_i} \| N_i(\hat{\ve{D}}) - N_i(\ve{D}) \|_1.
            \end{align*}
            where $N_i$ is the $i$th used layer of the VGG-19 network, and $P_i$ denote the number of elements in the output of the $i$th layer. The paper does not tell what layers of VGG-19 it uses, but it seems there are 5 used layers.
            </li>

            <li>The paper also proposes applying the loss on downsampled versions of $\hat{\ve{D}}$ and $\ve{D}$. There are four resolutions, so there are 20 loss terms in generate.</li>
        </ul>
        </li>
    </ul>

    <h4>Imposing Equivariance Constraint</h4>

    <ul>
        <li>The equivariance constraint forces the keypoint extractor to predict consistent keypoints with respect to known geometric transformations.
        <ul>
            <li>In other words, if image $\ve{X}$ undergoes a known transformation $T$, then the keypoints extracted from $\ve{X}$ should also undergo the same transformation too.</li>
        </ul>
        </li>

        <li>However, the keypoint extractor of the paper does not only output keypoint locations but also the Jacobians. The formulation of the equivariance constraint must be extended to Jacobians in order for it to be able to be enforced.</li>

        <li>Assume that the input image $\ve{X}$ undergoes a known spatial transformation $\mathcal{T}_{\ve{X} \leftarrow \ve{Y}}$.
        <ul>
            <li>This can be an affine transformation or a thin-place spline deformation.</li>

            <li>After this transformation, we obtain a new image $\ve{Y}$.</li>
        </ul>
        </li>

        <li>Using the keypoint estimation network, we get a set of local approximations for $\mathcal{T}_{\ve{X} \leftarrow \ve{R}}$ and $\mathcal{T}_{\ve{Y} \rightarrow \ve{R}}$.</li>

        <li>The equivariance constraint can now be written as:
        \begin{align*}
            \mathcal{T}_{\ve{X} \leftarrow \ve{R}}
            &\equiv \mathcal{T}_{\ve{X} \leftarrow \ve{Y}} \circ \mathcal{T}_{\ve{Y} \leftarrow \ve{R}}.
        \end{align*}
        This means that:
        \begin{align}
            \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p_k)
            &\equiv \mathcal{T}_{\ve{X} \leftarrow \ve{Y}} \circ \mathcal{T}_{\ve{Y} \leftarrow \ve{R}}(p_k) \label{point-constraint} \\
            \bigg( \frac{\dee}{\dee p}  \ve{T}_{\ve{X} \leftarrow \ve{R}}(p) |_{p = p_k} \bigg)
            &\equiv \bigg( \frac{\dee}{\dee p}  \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p) |_{p = \mathcal{T}_{\ve{Y}\leftarrow\ve{R}}(p_k)} \bigg) \bigg( \frac{\dee}{\dee p}  \mathcal{T}_{\ve{Y} \leftarrow \ve{R}}(p) |_{p = p_k} \bigg) \label{jacobian-constraint}
        \end{align}
        </li>

        <li>The constraint \eqref{point-constraint} can be enforced by computing the $L_1$ difference between the keypoint locations of $\ve{X}$ and the transformed keypoint locations of $\ve{Y}$.</li>

        <li>The constraint \eqref{jacobian-constraint} needs more care. If we just minimize the difference between the matrices on both sizes, it would force the Jacobian to zero and lead to numerical problems. So, the paper enforces the following equivalent constraint instead:
        \begin{align*}
            I 
            &\equiv 
            \bigg( \frac{\dee}{\dee p}  \ve{T}_{\ve{X} \leftarrow \ve{R}}(p) |_{p = p_k} \bigg)^{-1}
            \bigg( \frac{\dee}{\dee p}  \mathcal{T}_{\ve{X} \leftarrow \ve{R}}(p) |_{p = \mathcal{T}_{\ve{Y}\leftarrow\ve{R}}(p_k)} \bigg) 
            \bigg( \frac{\dee}{\dee p}  \mathcal{T}_{\ve{Y} \leftarrow \ve{R}}(p) |_{p = p_k} \bigg)
        \end{align*}
        where $I$ is the $2\times 2$ identity matrix.
        </li>

        <li>The paper uses the same weight for the equivariance constraint loss as the weight for the perceptual content loss.</li>

        <li>However, it is unclear how this loss is actually used. The paper does not tell exactly where this loss fits into the whole training scheme. (However, it does specify how to sample random thin-plate spline deformation.)</li>
    </ul>

    <h3>Transferring Motion at Test Time</h3>

    <ul>
        <li>At test time, we have a source image $\ve{S}$ and frames of the driving videos $\ve{D}_1$, $\dotsc$, $\ve{D}_T$. We would like to compute $\ve{S}_t$ for $1 \leq t \leq T$.</li>

        <li>The paper assumes that $\ve{S}$ and $\ve{D}_1$ are roughly aligned. So, we shall write $\ve{S}$ as $\ve{S}_1$.</li>

        <li>Rather than trying to compute $\mathcal{T}_{\ve{S}_1 \leftarrow \ve{D}_t}$, the paper transfer the relative motion between $\ve{D}_1$ and $\ve{D}_t$ to $\ve{S}_1$. In other words:
        \begin{align*}
            \mathcal{T}_{\ve{S}_1 \leftarrow \ve{S}_t}
            \approx \mathcal{T}_{\ve{S}_1 \leftarrow \ve{R}}(p_k) + J_k(z - \mathcal{T}_{\ve{S}_1 \leftarrow \ve{R}} + \mathcal{T}_{\ve{D}_1 \leftarrow \ve{R}} - \mathcal{T}_{\ve{D}_t \leftarrow \ve{R}})
        \end{align*}
        with
        \begin{align*}
            J_k 
            &= \bigg( \frac{\dee}{\dee p}  \ve{T}_{\ve{D}_1 \leftarrow \ve{R}}(p) |_{p = p_k} \bigg)
            \bigg( \frac{\dee}{\dee p}  \ve{T}_{\ve{D}_t \leftarrow \ve{R}}(p) |_{p = p_k} \bigg)^{-1}
        \end{align*}
        </li>

        <li>In other words, the paper only transfer relative motion between frames, which would help preserve the original object's shape.</li>
    </ul>
    
    <hr>
    <div class="page-header"></div>
    <p>Last modified: 2020/02/25</p>    
</div>

<script type="text/javascript">
$.bigfoot();
</script>

</body>
</html>

