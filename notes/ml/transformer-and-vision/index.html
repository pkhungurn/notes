<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Understanding the Transformer</title>

    <!-- Bootstrap -->
    <link href="../../../css/bootstrap.min.css" rel="stylesheet">
    <link href="../../../css/theme.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: {autoNumber: "AMS"} } 
    });
    </script>
    <script type="text/javascript"
            src="../../../MathJax/MathJax.js?config=TeX-AMS_HTML-full">
    </script>

    <script type="text/javascript" src="../../../js/jquery-3.4.1.min.js"></script>    
    <script type="text/javascript" src="../../../js/bigfoot.min.js"></script>    

    <link rel="stylesheet" type="text/css" href="../../../css/bigfoot-default.css">    

</head>
<body>
<div class="container">
    <span style="visibility: hidden;">
        \(
        \def\sc#1{\dosc#1\csod}
        \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \DeclareMathOperator*{\argmax}{arg\,max}
        \DeclareMathOperator*{\argmin}{arg\,min}
        \)
    </span>

    <br>
    <h1>Transformer and Vision</h1>
    <hr>
    
    <p>
        Vision tasks have been dominated by CNNs. However, the landscape has kind of changed 
        when people started using transformers <a href="#fn_vaswani_2017">[Vaswani et al. 2017]</a> to do vision tasks. 
        This note was written as I read papers related to the user of transformers in vision tasks. It also contains
        a section on the MLP mixer <a href="#fn_tolstikhin_2021">[Tolstikhin et al. 2021]</a>, a new architecture inspired 
        by the transformer.
    </p>

    <p>
        In particular, here are some papers I'm interested in reading.
        <ul>
            <li>
                Dosovitskiy et al. <b>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</b> ICLR 2021.
                <a href="https://arxiv.org/abs/2010.11929">[arXiv]</a>                
            </li>

            <li>
                Child et al. <b>Generating Long Sequences with Sparse Transformers.</b> 2019. <a href="https://arxiv.org/abs/1904.10509">[arXiv]</a>
            </li>

            <li>
                Kitaev et al. <b>Reformer: The Efficient Transformer.</b> ICRL 2020. <a href="https://arxiv.org/abs/2001.04451">[arXiv]</a>
            </li>

            <li>Wang el al. <b>Linformer: Self-Attention with Linear Complexity.</b> 2020. <a href="https://arxiv.org/abs/2006.04768">[arXiv]</a></li>

            <li>
                Touvron et al. <b>Training data-efficient image transformers & distillation through attention.</b> 2020. 
                <a href="https://arxiv.org/abs/2012.12877">[arXiv]</a>
            </li>

            <li>
                Tolstikhin et al. <b>MLP-Mixer: An all-MLP Architecture for Vision.</b> 2021. <a href="https://arxiv.org/abs/2105.01601">[arXiv]</a>
            </li>

            <li>
                Chen et al. <b>Pre-Trained Image Processing Transformer.</b> 2020. <a href="https://arxiv.org/abs/2012.00364">[arXiv]</a>                
            </li>

            <li>
                Jiang et al. <b>TransGAN: Two Transformers Can Make One Strong GAN.</b> 2021 <a href="https://arxiv.org/abs/2102.07074">[arXiv]</a>
            </li>

            <li>Chen et al. <b>Generative Pretraining from Pixels.</b> ICML 2020. <a href="http://proceedings.mlr.press/v119/chen20s/chen20s.pdf">[PDF]</a></li>
        </ul>
    </p>

    <div class="footnotes">
        <li class="foonote" id="fn_vaswani_2017">
            Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
            Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.
            <b>Attention Is All You Need.</b>
            NIPS 2017.
            <a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">[PDF]</a>
        </li>

        <li class="footnote" id="fn_tolstikhin_2021">
            Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, 
            Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, 
            Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy.
            <b>MLP-Mixer: An all-MLP Architecture for Vision.</b>
            2021. <a href="https://arxiv.org/abs/2105.01601">[arXiv]</a>
        </li>
    </div>
    
    <hr>
    <div class="page-header"></div>
    <p>Last modified: 2021/06/07</p>    
</div>

<script type="text/javascript">
$.bigfoot();
</script>

</body>
</html>