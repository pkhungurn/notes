\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}

\begin{document}
  \maketitle

  This article is written as I read ``Perceptual Losses for Real-Time Style Transfer and Super-Resolution'' by Johnson \etal\ \cite{Johnson:2016}.

  \section{Introduction}

  \begin{itemize}
  	\item An approach to solve the image transformation problem is to train a network in a supervised manner, using a per-pixel loss. Per-pixel loss does not capture perceptual differences between the output and the ground-truth images, but the approach is fast.

  	\item Perceptual loss functions can be implemented by comparing differences between high-level features extracted from pretrained CNNs. High quality images can be generated by minimizing this function. The style transfer paper by Gatys \etal\ is an example that does this \cite{Gatys:2015}. However, this approach is slow.

  	\item The present paper trains a feed-forward network using perceptual loss functions. The network can transform images in real-time and achieve good quality outputs. The authors apply this approach to two problems: style transfer and super-resolution.  
  \end{itemize}

  \section{Method}

  \begin{itemize}
  	\item The system consists of two components:
  	\begin{itemize}
  		\item An image transformation network $f_W$.
  		\begin{itemize}
  			\item Parametermized by wegiths $W$.
  			\item Transforms input image $x$ into output image $\hat{y}$.
  		\end{itemize}
  		\item A loss network $\phi$.
  		\begin{itemize}
  		 	\item Used to define several loss functions $\ell_1$, $\ell_2$, $\dotsc$, $\ell_k$.
  		 	\item Each function $\ell_i$ computes a scalar value $\ell_i(\hat{y},y_i)$, which measures the difference between the output image $\hat{y}$ and a target image $y_i$.
  		\end{itemize} 
  	\end{itemize}

  	\item The network is trained to find the optimal weight $W^*$ that minimizes a weighted sum of the loss funcions:
  	\begin{align*}
  		W^* = \argmin_{W} E_{x, \{y_i\}} \bigg[ \sum_{i=1}^k \lambda_i \ell_i (f_W(x), y_i) \bigg].
  	\end{align*}

  	\item The loss network $\phi$ is used to define a \emph{feature reconstruction loss} $\ell^\phi_{feat}$ and a \emph{style reconstruction loss} $\ell^\phi_{style}$ that measures differences in content and style between images, respectively.

  	\item Each input image has an associated \emph{content target} $y_c$ and \emph{style target} $y_s$.
  	\begin{itemize}
  		\item For style transfer $y_c$ is $x$ itself, and $y_s$ is the image having the style that we want to transfer $x$ to.
  		\item For super-resolution, $x$ is a low-res image. $y_c$ is the high-res image. The style reconstruction loss is not used.
  	\end{itemize}
  \end{itemize}

  \subsection{Image Transformation Networks}

  \begin{itemize}
  	\item The architecture follows the guidelines by the DCGAN paper \cite{Radford:2016}.
  	\begin{itemize}
  		\item No pooling layers.
  		\item Downsampling and upsampling are implemented by strided and fractionally strided convolution layers.      
  	\end{itemize}

    \item The paper designed two network: the style transfer network and the super-resolution network. The style transfer network receives an input image and tranfer it to a fixed style, determine at training time.

    \item The following are the common features between the two networks.
    \begin{itemize}
      \item The networks use 5 residual blocks \cite{He:2015}.
      \item All non-residual convolutional layers are followed by batch normalization and ReLU non-linearity. 
      \item The output layer uses scaled $\tanh$ to make sure the pixels are in the range $[0,255]$.
      \item The first and the last convolutional layers use kernels of size $9 \times 9$. Other convolutional layers use $3 \times 3$ kernels.
    \end{itemize}    

    \item The networks take the following inputs and outputs:
    \begin{itemize}
      \item The style transfer network inputs and outputs are images of size $3 \times 256 \times 256$.
      \item The super-resolution network outputs an image of size $3 \times 288 \times 288$. If the upsampling factor is $f$, then the input of of size $3 \times (288/f) \times (288/f)$.
    \end{itemize}

    \item In the super-resolution network with upsampling factory $f$, the residual blocks are followed by $\log_2 f$ convolutional layers with stride $1/2$.

    \item In the style transfer network, two stride-2 convolutional layers downsample the input. The result is then passed to the residual blocks and then two fractionally strided convolutional layers to upsample it to the original resolution.

    \item Downsampling and then upsampling in the style transfer network has the following benefits:
    \begin{itemize}
      \item Downsampling significantly reduces the cost of evaluating the residual blocks.
      \item Downsampling increases the effective receptive field sizes of each pixel in the input.
    \end{itemize}

    \item The paper argues that shortcut connections in residual blocks are beneficial to image transformation because, in most cases, the output image should share structure with the input image.

    \item The detailed architecture can be found in the supplementary material of the paper.\footnote{\url{https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf}}

  \end{itemize}

  \subsection{Perceptual Loss Function}

  \begin{itemize}
    \item The paper uses a \emph{loss network} $\phi$ to define two perceptual loss functions.

    \item Here, $\phi$ is VGG-16 pretrained for image classification.

    \item Let $\phi_j(x)$ be the activation of the $j$th layer of the network. We restrict ourselves to convolutional layers. Let $C_j \times H_j \times W_j$ be the shape of $\phi_j(x)$.

    \item The first loss function is the \textbf{feature reconstruction loss}:
    \begin{align*}
      \ell^{\phi,j}_{feat} (\hat{y}, y) = \frac{1}{C_j H_j W_j} \| \phi_j(\hat{y}) - \phi_j(y) \|^2_2
    \end{align*}
    The paper demonstrated that finding $\hat{y}$ that minimizes the feature loss function tend to preserve the spetial features while color, texture, and exact shape degrade as we use deeper layers.

    \item The second loss fucntion is the \textbf{style reconstruction loss}. This is defined with the \emph{Gram matrix} $G^{\phi}_j(x)$, which is $C_j \times C_j$  matrix whose elements are given by:
    \begin{align*}
      G_j^\phi(x)_{c, c'} = \frac{1}{C_j H_j W_j} \sum_{h=1}^{H_j} \sum_{w=1}^{W_j} \phi_j(x)_{h,w,c} \phi_j(x)_{h,w,c'}.
    \end{align*}
    The style reconstruction loss the the squared Frobenius norm fo the difference between the Gram matrices of the output and target images:
    \begin{align*}
      \ell_{style}^{\phi,j} = \| G_j^\phi(\hat{y}) - G_j^\phi(y) \|_F^2.
    \end{align*}
    
    \item If we view $\phi_j(x)$ as being a collection of vectors of dimension $C_j$, then $G_j^{\phi}(x)$ is proportional to the uncentered covariance of the $C_j$ dimensional vectors. So, it captures information about which features tend to activate together.

    \item The Gram matrix can be computed efficiently by viewing $\phi_j(x)$ as a matrix $\psi$ of size $C_j \times H_jW_j$. We then have that:
    \begin{align*}
      G_j^{\phi}(x) = \frac{\psi \psi^T}{C_j H_j W_j}.
    \end{align*}

    \item The paper also defines two simple loss functions:
    \begin{itemize}
      \item The \textbf{pixel loss} simply computes the average square difference between corresponding pixels:
      \begin{align*}
        \ell_{pixel}(\hat{y}, y) = \frac{\| \hat{y} - y \|_2^2}{CHW}.
      \end{align*}
      \item The \textbf{total variation regularizer} constrains the image to change smoothly over space:
      \begin{align*}
        \ell_{TV}(\hat{y}) = \sum_{w,h} \big( (\hat{y}_{w,h+1} - \hat{y}_{w,h})^2 + (\hat{y}_{w+1,h} - \hat{y}_{w,h})^2 \big)
      \end{align*}
    \end{itemize}
  \end{itemize}

  \section{Implementation}

  \subsection{Style Transfer}

  \begin{itemize}
    \item Style transfer is defined as finding the image $\hat{y}$ that minimizes the following loss function:
    \begin{align*}
      \hat{y} = \argmin_y \big( 
        \lambda_c \ell^{\phi,j}_{feat}(y, y_c)
        + \lambda_s \ell^{\phi,j}_{style}(y, y_s)
        + \lambda_{TV} \ell_{TV}(y) \big)
    \end{align*}
    where $y_c$ is the target content image, $y_s$ is the target style image, and $\lambda_c$, $\lambda_s$, $\lambda_{TV}$ are scalar hyperparameters.

    \item For the feature reconstruction loss, the paper uses the output of \texttt{relu3\_3} of VGG-16. For the style reconstruction loss, the paper uses the outputs of \texttt{relu1\_2}, \texttt{relu2\_2}, \texttt{relu3\_3}, and \texttt{relu4\_3}.
  \end{itemize}

  \subsection{Super Resolution}

  \begin{itemize}
    \item The author trained models to minimize the feature reconstruction loss at layer \texttt{relu2\_2}.
  \end{itemize}

  \bibliographystyle{acm}
  \bibliography{perceptual-losses}  
\end{document}