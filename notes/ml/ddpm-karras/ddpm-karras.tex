\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{clrscode3e}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\ves}[1]{\boldsymbol{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\data}{\mathrm{data}}
\newcommand{\SNR}{\mathrm{SNR}}

\title{Karras \etal's DDPM Improvements}
\author{Pramook Khungurn}

\begin{document}
\maketitle

This note is written as I read ``Elucidating the Design Space of Diffusion-Based Generative Models'' by Karras \etal~\cite{Karras:2022}. I have to admit that it is very hard for me to read the paper because it requires so much background to understand, and all the derivations are folded into the appendix.

\section{Introduction}

\begin{itemize}
  \item The paper casts a number of previous works on DDPM and score-based generative models into a common framework and suggests improvements to multiple parts of generative models in the framework.
  
  \item The previous works include:
  \begin{itemize}
    \item The stochastic differential equation paper by Song \etal~\cite{Song:2021}.
    \item The improved DDPM paper by Nichol and Dharival~\cite{Nichol:2021}.
    \item The DDIM paper by Song \etal~\cite{Song:DDIM:2020}.
    \item The adaptive step-size SDE solver paper by Jolicoeur-Martineau \etal~\cite{Jolicoeur-Martineau:2021}.
  \end{itemize}

  \item The end results are score improvements on several datasets.
  \begin{itemize}
    \item SOTA FID score on CIFAR-10.
    \item Near-SOTA FID score on ImageNet-64 without retraining.
    \item SOTA FID score on ImageNet-64 with retraining.
  \end{itemize}

  \item I also believe that the Heun's 2nd order solver introduced in this paper has been include as a sampler in Stable Diffusion.
  \begin{itemize}
    \item Actually, this is why I try to read the paper.
  \end{itemize}
\end{itemize}

\section{A Common Framework for Diffusion Models}

\subsection{Preliminary}

\begin{itemize}
  \item Let each data item be a vector in $\Real^d$. We typically denote one with the letter $\ve{x}$.
  
  \item Let $p_{\data}(\ve{x})$ denote the data distribution. Let $\sigma_{\data}$ denotes the standard deviation of the ata.
  
  \item All diffusion model works with the distribution $p(\ve{x};\sigma)$ obtained by adding i.i.d. Gaussian noise of standard deviation $\sigma$ to the data sampled from $p_{\data}$.
  \begin{itemize}
    \item $p(\ve{x}; 0) = p_{\data}(\ve{x})$.
    \item If $\sigma_{\max} \gg \sigma_{\data}$, then $p(\ve{x};\sigma_{\max})$ would be very close to $\mcal{N}(\ve{0}, \sigma_{\max}^2 I)$.
  \end{itemize}

  \item In general, here's how a diffusion model generates a sample.
  \begin{enumerate}
    \item We specify a number of noise levels $0 = \sigma_1 < \sigma_2 < \dotsb < \sigma_T = \sigma_{\max}$.
    \item We first sample a point $\ve{x}_T$ from $\mcal{N}(\ve{0}, \sigma_{\max}^2 I)$ and simply assume that it comes from $p(\ve{x}; \sigma_{\max})$.
    \item Given a point $\ve{x}_t$ that comes from $p(\ve{x}; \sigma_t)$, we revert the noising process inherent in $p(\ve{x}; \sigma_t)$ to produce a point $\ve{x}_{t-1}$ that should come from $p(\ve{x};\sigma_{t-1})$.
    \item Starting from $\ve{x}_T$, we repeat Step 3 until we reach $\ve{x}_0$, which is returned as the output of the sampling process.
  \end{enumerate}
\end{itemize}

\subsection{A General SDE and Its Probability Flow ODE}

\begin{itemize}
  \item In \cite{Song:2021}, Song \etal\ suggests that the noising process (i.e., $p(\ve{x}, \sigma)$) can be modeled by stochastic differential equations.

  \item The sequence $\sigma_1$, $\sigma_2$, $\dotsc$, $\sigma_T$ becomes a continuous function $\sigma(t)$ of time where $t \in [0,T]$.
  \item Here, $\sigma(0) = 0$, and $\sigma(T) = \sigma_{\max}$.
  \item For each tiem $t$, we view $\ve{x}(t)$ which should be distributed according to $p(\ve{x}; \sigma(t))$ as a random variable. This means that $\{ \ve{x}(t) : t \in [0,T] \}$ is a stochastic process.
  \item The evaluation of the above stochastic process is governed by the stochastic differential equation:
  \begin{align*}
    \dee \ve{x} = \ve{f}(\ve{x}(t), t)\, \dee t + g(t)\, \dee \ve{W}
  \end{align*} 
  where $\ve{f}: \Real^d \times \Real \rightarrow \Real^d$ is called the {\bf drift coefficient}, $g(t): \Real \rightarrow \Real$ is called the {\bf diffusion coefficient}, and $\ve{W}(t)$ is the standard $d$-dimensional Brownian motion (aka the Wiener process).
  \item The initial condition is $\ve{x}(0) \sim p_{\data}$.
  \begin{itemize}
    \item To make it simpler, we say that $\ve{x}(0)$ is fixed and has no variance when deriving a solution to SDEs.
  \end{itemize}
  \item The Song \etal\ paper gives two SDEs. One is called the {\it variance-exploding (VE)} SDE, and another the {\it variance-preserving (VP)} SDE. Both SDEs are of the form
  \begin{align*}
    \dee \ve{x} = f(t)\ve{x}\, \dee t + g(t)\, \dee \ve{W}
  \end{align*}
  where $f(\ve{x},t)$ becomes $f(t)\ve{x}$, and $f$ is reduced to a function of signature $\Real \rightarrow \Real$.
  \item The above SDE has an explicit, unique solution. We appeal from the following theorem (Equation 6.2) from \cite{Sarkka:2019}.
  \begin{theorem}
    Consider a {\bf linear SDE} of the form
    \begin{align*}
      \dee \ve{x} = \big(F(t)\ve{x} + \ve{u}(t) \big)\, \dee t + L(t)\, \dee \ve{W} 
    \end{align*}
    where $F: \Real \rightarrow \Real^{d \times d}$, $\ve{u}: \Real \rightarrow \Real^d$, and $L: \Real \rightarrow \Real^{d \times d}$. We have that the solution of this equation is a Gaussian process whose mean and covariance matrix,
    \begin{align*}
      \ves{\mu}(t) &= E[\ve{x}(t)], \\ 
      \Sigma(t) &= E[(\ve{x}(t) - \ves{\mu}(t)) (\ve{x}(t) - \ves{\mu}(t))^T],
    \end{align*}
    satisfy the ODEs
    \begin{align*}
      \frac{\dee\ves{\mu}(t)}{\dee t} &= F(t) \ves{\mu}(t), \\
      \frac{\dee\Sigma(t)}{\dee t} &= F(t) \Sigma(t) + \Sigma(t) F(t)^T + L(t)L(t)^T. 
    \end{align*}
  \end{theorem}
  \item Setting $F(t) = f(t)I$, $\ve{u}(t) = \ve{0}$, and $L(t) = g(t)I$, we have that
  \begin{align*}
    \frac{\dee\ves{\mu}(t)}{\dee t} &= f(t) \ves{\mu}(t), \\
    \frac{\dee\Sigma(t)}{\dee t} &= 2 f(t) \Sigma(t) + g(t)^2 I. 
  \end{align*}
  Assuming that the sample $\ve{x}(0)$ has already been sampled and fixed. The initial condition is $\ves{\mu}(0) = \ve{x}(0)$, and $\Sigma(0) = 0$. Solving the first ODE, we have that
  \begin{align*}
    \ves{\mu}(0) = \ve{x}(0) \exp\bigg( \int_0^t f(u)\, \dee u \bigg).
  \end{align*}
  The paper defines $$s(t) := \exp\bigg( \int_0^t f(u)\, \dee u\bigg),$$ and so $$\ves{\mu}(0) = s(t) \ve{x}(0).$$
  For the second ODE, notice that $\Sigma(t)$ is a diagonal matrix whose entries are all the same. Let the entries have value $v(t)$. We have that
  \begin{align*}
    \frac{\dee v(t)}{\dee t} = 2f(t)v(t) + g(t)^2.
  \end{align*}
  Solving the equation (see Proposition~\ref{thm:linear-sde-variance}), we have that
  \begin{align*}
    v(t) = s(t)^2 \int_0^t \frac{g(u)^2}{s(u)^2}\, \dee u,
  \end{align*}
  assumming that $v(0) = 0$ (because we are under the setting where the sample $\ve{x}(0)$ has alrady been fixed). The paper defines $$\sigma(t) := \sqrt{\int_0^t \frac{g(u)^2}{s(u)^2}\, \dee u},$$
  and so the conditional density of $\ve{x}(t)$ given $\ve{x}(0)$ is given by
  $$p_{t|0}(\ve{x}|\ve{x}_0) = \mcal{N}(\ve{x} ; s(t)\ve{x}_0, s(t)^2 \sigma(t)^2 I ). $$
  The marginal density of $\ve{x}(t)$ is given by
  \begin{align*}
    p_t(\ve{x}) = \int_{\Real^d} p_{t|0}(\ve{x} | \ve{x}_0) p_{\data}(\ve{x}_0)\, \dee \ve{x}_0.
  \end{align*}

  \item According to \cite{Song:2021}, the stochastic process $\ve{x}(t)$, when interpreted as a function of signature $\Real \rightarrow \Real^d$, and the marginal probability distribution $p_t(\ve{x})$ also satisfies the ODE
  \begin{align*}
    \frac{\dee \ve{x}}{\dee t} = f(t)\ve{x} - \frac{1}{2}g(t)^2 \nabla_{\ve{x}} \log p_t(\ve{x})
  \end{align*}
  called the {\bf probability flow ODE}. (Deriving this equation can be done my manipulating the Fokker--Planck equation that $p_t(\ve{x})$ is supposed to satisfy. See my notes on score-based generative models for more information \cite{KhungurnScoreBased}.)
\end{itemize}

\subsection{Making $s(t)$ and $\sigma(t)$ First Class Citizens}

\begin{itemize}
  \item Note that the conditional distribution $p_{t|0}$ is given in terms of $s(t)$ and $\sigma(t)$, which are derived from $f(t)$ and $g(t)$. However, we are more interested in $s(t)$ and $\sigma(t)$ because they tells use directly what the means and the standard deviations are. So, the paper rewrites the probability ODE in terms of $s(t)$ and $\sigma(t)$ instead of $f(t)$ and $g(t)$.

  \item Consider the expression for the marginal distribution of $\ve{x}(t)$.
    \begin{align*}
      p_t(\ve{x}) 
      &= \int_{\Real^d} p_{t|0}(\ve{x}|\ve{x}_0) p_{\data}(\ve{x}_0)\, \dee \ve{x}_0 \\
      &= \int_{\Real^d} \mcal{N}(\ve{x}; s(t)\ve{x}_0, s^2(t)\sigma^2(t) I) p_{\data}(\ve{x}_0)\, \dee \ve{x}_0 \\
      &= \int_{\Real^d} s(t)^{-d} \mcal{N}(\ve{x} / s(t); \ve{x}_0, \sigma^2(t) I) p_{\data}(\ve{x}_0)\, \dee \ve{x}_0 \\
      &= s(t)^{-d}  \int_{\Real^d} \mcal{N}(\ve{x} / s(t); \ve{x}_0, \sigma^2(t) I) p_{\data}(\ve{x}_0)\, \dee \ve{x}_0 \\
      &= s(t)^{-d} \Big[ p_{\data} * \mcal{N}(\ve{0}, \sigma^2(t) I ) \Big] \big( \ve{x} / s(t) \big)
    \end{align*}
    where $*$ is the convolution operation.

  \item $p_{\data} * \mcal{N}(\ve{0}, \sigma^2(t) I )$ is the corrupted version of $p_{\data}$ due to the noise added by the diffusion process. The paper defines
  \begin{align*}
    p(\ve{x}; \sigma) := p_{\data} * \mcal{N}(\ve{0}, \sigma^2 I).
  \end{align*} 
  So,
  \begin{align*}
    p_t(\ve{x}) = \frac{p(\ve{x}/s(t); \sigma(t))}{s(t)^d}.
  \end{align*}

  \item Plugging the expression for $p_t(\ve{x})$ into the probability flow ODE, we have that
  \begin{align*}
    \frac{\dee \ve{x}}{\dee t} &= f(t)\ve{x} - \frac{1}{2}g(t)^2 \nabla_{\ve{x}} \log \frac{p(\ve{x}/s(t); \sigma(t))}{s(t)^d} \\
    \frac{\dee \ve{x}}{\dee t} &= f(t)\ve{x} - \frac{1}{2}g(t)^2 \nabla_{\ve{x}} \log p(\ve{x}/s(t); \sigma(t)).
  \end{align*}
  
  \item We now write $f(t)$ and $g(t)$ in terms of $s(t)$ and $\sigma(t)$.  For $f(t)$, we have that
  \begin{align*}
    \dot{s}(t) &= f(t) s(t) \\
    f(t) &= \frac{\dot{s}(t)}{s(t)}.
  \end{align*}
  For $g(t)$, we have that
  \begin{align*}
    \{ \sigma^2(t) \}' &= \frac{g(t)^2}{s(t)^2} \\
    2\sigma(t)^2 \dot{\sigma}(t) &= \frac{g(t)^2}{s(t)^2} \\
    g(t) &= s(t) \sqrt{2 \sigma(t) \dot{\sigma}(t)}.
  \end{align*}

  \item Plugging in expressions for $f(t)$ and $g(t)$, we have that
  \begin{align}
    \frac{\dee \ve{x}}{\dee t}
    &= \frac{\dot{s}(t)}{s(t)} \ve{x} - s(t)^2 \sigma(t) \dot{\sigma}(t) \nabla_{\ve{x}} \log p(\ve{x}/s(t); \sigma(t)). \label{eqn:scaled-probability-flow-ode}
  \end{align}
\end{itemize}

\subsection{Deterministic Sampling}

\begin{itemize}
  \item A way to sample from $p_{\data}$ is to (1) sample an element from $p_T(\ve{x})$ and (2) simulate Equation~\eqref{eqn:scaled-probability-flow-ode} numerically backward in time from $t = T$ to $t = 0$. The ability to sample from $p_T(\ve{x})$ rests on the fact that $p_{T|0} \sim \mcal{N}(s(T)\ve{x}(0), s(T)^2 \sigma(T)^2I)$. This can be easy in the following situations.
  \begin{enumerate}
    \item $s(T) = 1$, and $\sigma(T) \gg \sigma_{\data}$.\\ In this case, $p_T(\ve{x}) \approx \mcal{N}(\ve{0}, \sigma(T)^2I)$, and we can sample $\ve{x}(T)$ from $\mcal{N}(\ve{0}, \sigma(T)^2I)$.
    \item $s(T) \approx 0$, and $s(T)^2 \sigma(T)^2 = 1$.\\ In this case, $p_T(\ve{x}) \approx \mcal{N}(\ve{0}, I)$, and we can sample $\ve{x}(T)$ from $\mcal{N}(\ve{0}, I)$.
  \end{enumerate}
  In \cite{Song:2021}, the VE formulation corresponds to the first situation, and the VP formulation corresponds to the second situation. 

  \item Note that the sampling algorithm above is not ``deterministic'' per se. There is randomness in the sampling of $\ve{x}(T)$. However, this is the only randomness that is present in the algorithm. We will discuss ``stochastic'' sampling algorithms later. These algorithms use random numbers in each of its iterations.
  
  \item In order to simulate the ODE of Equation~\eqref{eqn:scaled-probability-flow-ode} numerically, we need to be able to compute the {\bf score} $\nabla_{\ve{x}} \log p(\ve{x}/s(t); \sigma^2(t))$. To make the problem more general, we drop the $s(t)^{-1}$ scaling factor and see if there is a way to compute $\nabla_{\ve{x}} \log p(\ve{x}; \sigma)$. A common way to do this is to train a neural network $D_{\ves{\theta}}(\widetilde{\ve{x}}, \sigma)$ such that it outputs $\ve{x}$ given a sample $\widetilde{\ve{x}} \sim p(\ve{x}; \sigma)$. This can be done by minimizing the loss
  \begin{align*}
    \mcal{L}(\ves{\theta}, \sigma) := E_{\ve{x}_0 \sim p_{\data}} E_{\ves{\xi} \sim \mcal{N}(0,I)} \Big[ \big\| D_{\ves{\theta}}(\ve{x}_0 + \sigma \ves{\xi}, \sigma) - \ve{x}_0 \big\|^2 \Big]
  \end{align*}
  Then, for the optimal $\ves{\theta}^*$, we would have that
  \begin{align*}
    \nabla_{\ve{x}} \log p(\ve{x}, \sigma) \approx \frac{D_{\ves{\theta}^*}(\ve{x}, \sigma) - \ve{x}}{\sigma^2}.
  \end{align*}
  The implementation of $D_{\ves{\theta}}(\ve{x}, \sigma)$ might include scaling $\ve{x}$ by the appropriate factor to make the inference easier. It is also common to use a single neural network for all $\sigma$ values, and so we need to think of a way to sample $\sigma$ and a way to assign weight to each $\mcal{L}(\ves{\theta}, \sigma)$. 
  
  \item We will come back to how to train $D(\ve{x},\sigma)$ later, but let us formulate the sampling algorithm right away that involves the probability flow ODE right away.
  
  \item We distinguished between samples from the distribution $p(\ve{x};\sigma(t)) = \mcal{N}(\ve{x}; \ve{x}(0), \sigma(t)^2 I)$, which is not scaled by $s(t)$, and the distribution $p_t(\ve{x}) = \mcal{N}(\ve{x}; s(t)\ve{x}(0), s(t)^2 \sigma(t)^2 I)$, which is scaled by $s(t)$.
    
  \item In particular, we let $\ve{x}$ denote a sample from $p_t(\ve{x})$, and $\hat{\ve{x}}$ denote a sample from $p(\hat{\ve{x}}, \sigma(t))$. It follows that
  \begin{align*}
    \ve{x} = s(t) \hat{\ve{x}}.
  \end{align*}
  The above equation is supposed to be read in the following way: one can sample $\ve{x} \sim p_T$ by sampling $\hat{\ve{x}} \sim \mcal{N}(\ve{x}(0), \sigma^2(t)I)$ and them computing $\ve{x} := s(t)\hat{\ve{x}}$.

  \item We have that
  \begin{align*}
    \nabla_{\ve{x}} \log p(\ve{x}/s(t); \sigma(t))
    &= \nabla_{s(t)\hat{\ve{x}}} \log p(\hat{\ve{x}}; \sigma(t))
    = \frac{1}{s(t)} \nabla_{\hat{\ve{x}}} \log p(\hat{\ve{x}}; \sigma(t)) \\
    &\approx \frac{D_{\ves{\theta}}(\hat{\ve{x}}, \sigma(t)) - \hat{\ve{x}}}{s(t)\sigma^2(t)} \\
    &= \frac{D_{\ves{\theta}}(\ve{x}/s(t), \sigma(t)) - \ve{x}/s(t)}{s(t)\sigma^2(t)}.
  \end{align*}

  \item Subtituting the above expression for the score into Equation~\ref{eqn:scaled-probability-flow-ode}, we have that
  \begin{align}
    \frac{\dee \ve{x}}{\dee t}
    &= \frac{\dot{s}(t)}{s(t)} \ve{x} - s(t)^2 \sigma(t) \dot{\sigma}(t) \frac{D_{\ves{\theta}}(\ve{x}/s(t), \sigma(t)) - \ve{x}/s(t)}{s(t)\sigma^2(t)} \notag \\
    \frac{\dee \ve{x}}{\dee t} 
    &= \frac{\dot{s}(t)}{s(t)} \ve{x} - s(t) \frac{\dot{\sigma}(t)}{\sigma(t)} \bigg( D_{\ves{\theta}}\bigg(\frac{\ve{x}}{s(t)}, \sigma(t)\bigg) - \frac{\ve{x}}{s(t)} \bigg) \notag \\
    \frac{\dee \ve{x}}{\dee t}
    &= \bigg( \frac{\dot{s}(t)}{s(t)} + \frac{\dot{\sigma}(t)}{\sigma(t)} \bigg) \ve{x} - s(t) \frac{\dot{\sigma}(t)}{\sigma(t)} D_{\ves{\theta}}\bigg(\frac{\ve{x}}{s(t)}, \sigma(t)\bigg). \label{eqn:probability-flow-ode-in-practice}
  \end{align}

  \item There are many algorithms for numerically solving an ODE such as Euler method and Runge--Kutta methods. The pseudocode for the Euler method that solves Equation~\eqref{eqn:probability-flow-ode-in-practice} backward in time is given in Algorithm~\ref{alg:euler-deterministic-sampler}. The paper advocates the use of Heun's method, which is a second order integrator. Its pseudocode is given in Algorithm~\ref{alg:heun-deterministic-sampler}.  
  
  \begin{algorithm}[t]
  \begin{codebox}
    \Procname{$\proc{Euler-Sampler}(D_{\ves{\theta}}, \sigma, s, \{ t_0, t_1, \dotsc, t_N \} )$}
    \li Sample $\ve{x}_0 \sim \mcal{N}(\ve{0}, s(t_0)^2 \sigma(t_0)^2 I)$.
    \li \For $i \leftarrow 0, 1, \dotsc, N-1$
    \li \Do
          $\ve{d}_i \leftarrow \Big( 
            \frac{\dot{\sigma}(t_i)}{\sigma(t_i)} + \frac{\dot{s}(t_i)}{s(t_i)}
          \Big) \ve{x}_i
          - s(t_i) \frac{\dot{\sigma}(t_i)}{\sigma(t_i)} D_{\ves{\theta}} \Big( \frac{\ve{x}_i}{s(t_i)}, \sigma(t_i)  \Big)$
    \li   $\ve{x}_{i+1} \leftarrow \ve{x}_i + (t_{i+1} - t_i) \ve{d}_i$
        \End
    \li \Return $\ve{x}_N$.
  \end{codebox}
  \caption{Euler method for sampling from $p_{\data}$ by simulating the probability flow  ODE \eqref{eqn:probability-flow-ode-in-practice}.}  
  \label{alg:euler-deterministic-sampler}
  \end{algorithm}

  \begin{algorithm}[t]
  \begin{codebox}
    \Procname{$\proc{Heun-Sampler}(D_{\ves{\theta}}, \sigma, s, \{ t_0, t_1, \dotsc, t_N \} )$}
    \li Sample $\ve{x}_0 \sim \mcal{N}(\ve{0}, s(t_0)^2 \sigma(t_0)^2 I)$.
    \li \For $i \leftarrow 0, 1, \dotsc, N-1$
    \li \Do
          $\ve{d}_i \leftarrow \Big( 
            \frac{\dot{\sigma}(t_i)}{\sigma(t_i)} + \frac{\dot{s}(t_i)}{s(t_i)}
          \Big) \ve{x}_i
          - s(t_i) \frac{\dot{\sigma}(t_i)}{\sigma(t_i)} D_{\ves{\theta}} \Big( \frac{\ve{x}_i}{s(t_i)}, \sigma(t_i)  \Big)$
    \li   $\ve{x}_{i+1} \leftarrow \ve{x}_i + (t_{i+1} - t_i) \ve{d}_i$
    \li   \If $\sigma(t_{i+1}) \neq 0$ 
    \li    \Then
            $\ve{d}'_{i} \leftarrow \Big( 
              \frac{\dot{\sigma}(t_{i+1})}{\sigma(t_{i+1})} + \frac{\dot{s}(t_{i+1})}{s(t_{i+1})}
            \Big) \ve{x}_{i+1}
            - s(t_{i+1}) \frac{\dot{\sigma}(t_{i+1})}{\sigma(t_{i+1})} D_{\ves{\theta}} \Big( \frac{\ve{x}_{i+1}}{s(t_{i+1})}, \sigma(t_{i+1})  \Big)$
    \li     $\ve{x}_{i+1} \leftarrow \ve{x}_i + \frac{1}{2}(t_{i+1} - t_i)(\ve{d}_i + \ve{d}_i')$
          \End
        \End
    \li \Return $\ve{x}_N$.
  \end{codebox}
  \caption{Heun's method for sampling from $p_{\data}$ by simulating the probability flow  ODE \eqref{eqn:probability-flow-ode-in-practice}.}  
  \label{alg:heun-deterministic-sampler}
  \end{algorithm}

  \item Note that, to use the above algoriths, we must provide a sequence of times $\{ t_0, t_1, \dotsc, t_N \}$ where
  \begin{align*}
    T = t_0 > t_1 > t_2 > \dotsb > t_{N-1} > t_N = 0.
  \end{align*}
\end{itemize}

\subsection{Stochastic Sampling}

\begin{itemize}
  \item The paper goes to a very long derivation (Appendix B.5) that uses heat equation to derive the reverse-time SDE \cite{Anderson:1982} so that it can derive the stochastic sampling algorithm. Moreover, the derived equation only works for the non-scaled version (i.e., $s(t) = 1$ and $f(t) = 0$) of the forward SDE. The stochastic sampling algorithm is also only for the non-scaled case. The paper claims that the derivation for the scaled case is similar and omits the derivation.
  
  \item I found the above treatment annoyhing as it is not complete. I also struggled with the derivation because it uses the heat equation and knowledge about partial differential equations, which I have not studied.
  
  \item So, I will take a shortcut and use the result by Anderson that the  
\end{itemize}

\appendix

\section{Derivations}

\begin{itemize}
  \item \begin{proposition} \label{thm:linear-sde-variance}
    The solution to the initial value problem
    \begin{align*}
      \frac{\dee v(t)}{\dee t} &= 2f(t)v(t) + g(t)^2 \\
      v(0) &= 0      
    \end{align*}
    is
    \begin{align*}
      v(t) = s(t)^2 \int_0^t \frac{g(u)^2}{s(u)^2}\, \dee u
    \end{align*}
    where $s(t) = \exp(\int_0^t f(u)\, \dee u)$.
  \end{proposition}
  \begin{proof}
    \begin{align*}
    \frac{\dee v(t)}{\dee t} &= 2f(t)v(t) + g(t)^2 \\
    \frac{\dee v(t)}{\dee t} - 2f(t)v(t) &= g(t)^2.
    \end{align*}
  \end{proof}
  Let $s(t) = \int_0^t f(u)\, \dee u$.
  Multiplying both sides by $s(t)^{-2} = \exp(-2\int_0^t f(u)\, \dee t)$, we have that
  \begin{align*}
    \exp\bigg( -2 \int_0^t f(u)\, \dee t \bigg) \frac{\dee v(t)}{\dee t}
    - 2f(t) \exp\bigg( -2 \int_0^t f(u)\, \dee t \bigg) v(t)
    &= \exp\bigg( -2 \int_0^t f(u)\, \dee t \bigg) g(t)^2 \\
    \bigg\{ \exp\bigg( -2 \int_0^t f(u)\, \dee t \bigg) v(t) \bigg\}'
    &= \exp\bigg( -2 \int_0^t f(u)\, \dee t \bigg) g(t)^2 \\
    \bigg\{ \frac{v(t)}{s(t)^2} \bigg\}'
    &= \frac{g(t)^2}{s(t)^2} \\
    \frac{v(t)}{s(t)^2} - \frac{v(0)}{s(0)^2} &= \int_0^t \frac{g(u)^2}{s(u)^2}\, \dee u.
  \end{align*}
  Because $v(0) = 0$ and $s(0) = 1$, we have that
  \begin{align*}
    \frac{v(t)}{s(t)^2} &= \int_0^t \frac{g(u)^2}{s(u)^2}\, \dee u \\
    v(t) &= s(t)^2 \int_0^t \frac{g(u)^2}{s(u)^2}\, \dee u
  \end{align*}
  as required.
\end{itemize}

\bibliographystyle{alpha}
\bibliography{ddpm-karras}  
\end{document}