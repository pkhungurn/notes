\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage[amsthm, thmmarks]{ntheorem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{verse}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}

\newcommand{\dee}{\mathrm{d}}
\newcommand{\Dee}{\mathrm{D}}
\newcommand{\In}{\mathrm{in}}
\newcommand{\Out}{\mathrm{out}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\etal}{{et~al.}}
\newcommand{\sphere}{\mathbb{S}^2}
\newcommand{\modeint}{\mathcal{M}}
\newcommand{\azimint}{\mathcal{N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\sseq}{\subseteq}
\newcommand{\ov}[1]{\overline{#1}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{Variational Autoencoder}
\author{}

\begin{document}
  \maketitle

  This document is written as I read ``Tutorial on Variational Autoencoders'' by Carl Doersch \cite{doersch:2016}.

  \section{Introduction}

  \begin{itemize}
  	\item Let $x$ denote a data point. Let $\mathcal{X}$ denotes the space of all data points.

  	\item We assume that the data points are generated from an unknown distribution $P_{gt}$, where ``$gt$'' stands for ``ground truth.''
  	\begin{itemize}
  	  \item Here, $X$ can be an image of, say, a cat. $P_{gt}$ is the distribution of cat images.
  	\end{itemize}

  	\item Given many examples $x_1, x_2, \dotsc, x_n$ sampled from $P_{gt}$, we are interested in learning a probability distribution ``model'' $P$ which can be used to sample a new data point $x$ so that it looks like it is being generated from $P_{gt}$.

  	\item The probability distribution $P$ that a variational autoencoder learns is a \textbf{latent variable model}.

  	\begin{itemize}
  	  \item To generate a sample with a latent variable model, we first sample a \textbf{latent vector} $z$ from a space $\mathcal{Z}$.
  	  \begin{itemize}
  	  	\item $\mathcal{Z}$ is much smaller than $\mathcal{X}$.
  	  	\item We can think of $z$ as being the low dimensional representation of the real data point $x$.
  	  	\item The probability distribution $P(z)$ where $z$ is sampled from is chosen to be the multidimentional normal distribution $\mathcal{N}(0, I)$ where $I$ is the identity matrix whose size equals to the dimention of $\mathcal{Z}$.
  	  \end{itemize}

  	  \item To generate a real data point, we pass $z$ to a deterministic function $\mu_E(\cdot, \theta)$ where $\theta$ is the model's parameter.
  	  \begin{itemize}
  	  	\item In our case, $\mu_E$ is a deep neural networks, and $\theta$ is the networks' parameters.
  	  \end{itemize}

  	  \item The probability distribution of any data point $x$ being generated from our model is given by:
  	  \begin{align*}
  	  	P(x) = \int_{\mathcal{Z}} P(x | z; \theta) P(z)\ \dee z
  	  \end{align*}
  	  where
  	  \begin{align*}
  	  	P(x|z; \theta) = \mathcal{N}\big(x; \mu_D(z,\theta), \sigma^2_D I \big),
  	  \end{align*}
  	  is the Gaussian distribution with mean $\mu_D(z,\theta)$ with element-wise variance of $\sigma^2_D$ with zero cross-element correlations. The subscript ``$D$'' here stands for ``decoder.'' Note also that $\sigma_D$ is a hyperparameter of the algorithm.
  	  \begin{itemize}
  	  	\item Note that, in other treatments of VAE, the element-wise variance is modeled by a neural network $\sigma_D^2(\cdot, \theta)$ instead of being fixed constant like in the treatment by Doersch. In this case, the covariance matrix is given by $\diag(\sigma_D^2(\cdot, \theta))$.
  	  \end{itemize}

  	  \item The choice of using Gaussian distribution seems to come from expediency. It is easily computable and continuous in $\theta$ and $x$. 
  	\end{itemize}

  	\item With the above definition, we would like to find $\theta$ that maximizes the probability that the samples $x_1, x_2, \dotsc, x_n$ are generated by our model:
  	\begin{align*}
  		\argmax_{\theta} \bigg( \prod_{i=1}^n P(x_i) \bigg)
  		&= \argmax_{\theta} \bigg( \sum_{i=1}^n \log P(x_i) \bigg)\\
  		&= \argmax_{\theta} \bigg( \frac{1}{n} \sum_{i=1}^n \log P(x_i) \bigg)
  	\end{align*}
  	
  	\item Note that the expression being argmax'ed in the last line is an unbiased estimate of $E_{x \sim P_{gt}}[\log P(x)].$ In fact, it is this expectation that we want to maximize. Again, in its full form, we want to solve the following problem:
  	\begin{align*}
  	  \argmax_{\theta} E_{x \sim P_{gt}} \bigg[ \log \int_{\mathcal{Z}} P(x|z,\theta) P(z)\ \dee z \bigg]
  	\end{align*}

  	\item The problematic part of the above problem is the integral inside the expectation. It is hard to approximate it accurately.
  	\begin{itemize}
  	  \item We may apply the usual trick: Monte Carlo integration. Sample $z_1, z_2, \dotsc, z_m$ according of $P(z)$ and compute:
  	  \begin{align*}
  	  	\frac{1}{m} \sum_{j=1}^m P(x|z_j, \theta) P(z_j).
  	  \end{align*}
  	  However, we would need a lot of samples to get an accurate estimate because most $P(x|z_j, \theta)$ would be close to zero. We need to throw a lot of darts before we find a $f(z_j,\theta)$ that is close enough to $x$.

  	  \item MCMC might help here, but that is too complicated.
  	\end{itemize}
  \end{itemize}

  \section{Variational Autoencoders}

  \begin{itemize}
  	\item The key idea behind VAE is, for each $x$, we will sample $z$'s that are likely to produce $x$ and compute $P(x)$ just from those.

  	\item We need a new conditional probability distribution $Q(z|x)$, which, when given a data point $x$, generates latent vectors that are likely to produce $x$.
  	\begin{itemize}
  	  \item For VAE, this is given by:
  	  \begin{align*}
  	  	Q(z|x) = \mathcal{N}\big(z; \mu_E(x,\phi), \diag(\sigma^2_E(x,\phi))\big)
  	  \end{align*}
  	  where $\mu_E(\cdot, \phi)$ and $\sigma_E(\cdot, \phi)$ are deep neural networks, and $\phi$ is the network parameters. The subscript ``$E$'' stands for ``encoder.''
  	\end{itemize}

  	\item Consider $\log P(x)$, the log of the probability that a given data point $x$ is generated. Because $P(x)$ does not depend on $z$, we have that:
  	\begin{align*}
  	  \log P(x) = E_{z \sim Q(\cdot|x)} [\log P(x)]
  	\end{align*}
  	By Bayes rule:
  	\begin{align*}
  	  P(x) = \frac{P(x|z) P(z)}{P(z|x)}.
  	\end{align*}
  	So,
  	\begin{align*}
  	  \log P(x) 
  	  &= E_{z \sim Q(\cdot|x)} \bigg[\log \frac{P(x|z) P(z)}{P(z|x)} \bigg] \\
  	  &= E_{z \sim Q(\cdot|x)} \bigg[\log \bigg( \frac{P(x|z) P(z)}{P(z|x)} \frac{Q(z|x)}{Q(z|x)} \bigg) \bigg] \\
  	  &= E_{z \sim Q(\cdot|x)} [\log P(x|z)]
  	  - E_{z \sim Q(\cdot|x)} \bigg[ \log \frac{Q(z|x)}{P(Z)} \bigg] 
  	  + E_{z \sim Q(\cdot|x)} \bigg[\log \frac{Q(z|x)}{P(z|x)} \bigg]\\
  	  &= E_{z \sim Q(\cdot|x)} [\log P(x|z)] 
  	  - \mathcal{D}(Q(z|x)\|P(z)) + \mathcal{D}(Q(z|x)\| P(z|x))
  	\end{align*}
  	where $\mathcal{D}(\cdot\|\cdot)$ denotes the Kullback--Leibler divergence between two probability distributions.

  	Rearranging, we have:
  	\begin{align*}
  	  \log P(x) - \mathcal{D}(Q(z|x)\| P(z|x))
  	  &= E_{z \sim Q(\cdot|x)} [\log P(x|z)] 
  	  - \mathcal{D}(Q(z|x)\|P(z))
  	\end{align*}

  	\item What we would like to maximize is $\log P(x)$. VAE, however, maximizes $\log P(x) - \mathcal{D}(Q(z|x)\| P(z|x))$ by maximizing $E_{z \sim Q(\cdot|x)} [\log P(x|z)] 
  	  - \mathcal{D}(Q(z|x)\|P(z))$ instead.
	
	\item While $\log P(x) - \mathcal{D}(Q(z|x)\| P(z|x))$ is not the same as $\log P(x)$, we hope that $\mathcal{D}(Q(z|x)\| P(z|x))$ is effectively zero because we will be using a high-capacity model for $Q(z|x)$, and so it should be able to approximate any probability distribution. The Doersch paper \cite{doersch:2016} contains a proof that, as the capacity of the model goes to infinity, this term actually goes to $0$ in a 1D case.

  	\item The term $\mathcal{D}(Q(z|x)\|P(z))$ can be computed in closed form because it is a KL divergence between two Gaussian distributions. Abbreviating $\mu_E(x,\phi)$ as $\mu_E(x)$ and $\diag(\sigma^2_E(x,\phi))$ as $\Sigma_E(x)$, we have
  	\begin{align*}
  	  \mathcal{D}(Q(z|x)\|P(z))
  	  &= \mathcal{D}(\mathcal{N}(\mu_E(x),\Sigma_E(x))\|\mathcal{N}(0,I)) \\
  	  &= \frac{1}{2} \big( 
  	  \tr(\Sigma_D(x))
  	  + \mu_E(x)^T \mu_E(x)
  	  - k
  	  + \log \det \Sigma_E(x)
  	  \big)
  	\end{align*}
  	where $k$ is the dimensionality of the distribution.

  	\item The term $E_{z \sim Q(\cdot|x)} [\log P(x|z)] - \mathcal{D}(Q(z|x)\|P(z))$ together can be maximized through gradient descent. We just need a way to compute it in a forward pass that is differentiable.

  	From the previous item, $\mathcal{D}(Q(z|x)\|P(z))$ can be computed just by passing $x$ to $\mu_E$ and $\sigma_E$. However, $E_{z \sim Q(\cdot|x)} [\log P(x|z)]$ is more problematic because it is an expectation.

  	The VAE paper estimates $E_{z \sim Q(\cdot|x)} [\log P(x|z)]$ by taking a sample $x'$ from $\mathcal{N}(\mu_D(x), \Sigma_D(x))$ and compute $\log P(x'|z)$. To generate the sample, there's a special unit that connects the encoder (i.e., $\mu_E$ and $\sigma_E$) to the decoder (i.e., $\mu_D$) that takes as input a $\xi \sim \mathcal{N}(0,I)$. Then, the unit computes:
  	\begin{align*}
  	  x' = \mu_E(x,\phi) + \xi \odot |\sigma_E(x,\phi)|
  	\end{align*}
  	where $\odot$ denotes element-wise multiplication. Note that $\xi$ comes as an input to the whole encoder-decoder network. So, we are not just passsing $x$ in. We pass $(x, \xi)$ in.

  	\item The introduction of the unit to sample from a distribution inside a network by taking extra input is called the \textbf{reparameterization trick}. This can be done easily with the Gaussian distribution as we can generate deterministic networks that compute the distribution paramaters. Then, from these parameters, we can easily sample by passing in random numbers.
  \end{itemize}

  \section{Conditional Variational Autoencoders}

  \begin{itemize}
  	\item CVAE solves the problem of generating a sample $y$ from a distribution conditioned on another input $x$.
  	\begin{itemize}
  	  \item $x$ might be an image that has a hole in it, and we would like $y$ to be a plausible image with the hole filled.
  	\end{itemize}

  	\item To generate $y$ from $x$, we introduce a latent variable $z \sim \mathcal{N}(0,I)$. Then, we say that:
  	\begin{align*}
  		P(y|x,z) = \mathcal{N}(\mu_D(z,x), \sigma^2_D I)
  	\end{align*}
  	where, again, $\mu_D$ is a neural network, and $\sigma_D$ is a hyperparameter.

  	\item Using the same derivation as in the previous section, we have that:
  	\begin{align*}
  		\log P(y|x) - \mathcal{D}(Q(z|y,x)\|p(z|y,x))
  		&= E_{z \sim Q(\cdot|y,x)}\big[ \log P(y|z,x) \big] - \mathcal{D}(Q(z|y,x)\|P(z|x)).
  	\end{align*}
  	Because $z$ is sampled independent of $x$, we have:
  	\begin{align*}
  		\log P(y|x) - \mathcal{D}(Q(z|y,x)\|p(z|y,x))
  		&= E_{z \sim Q(\cdot|y,x)}\big[ \log P(y|z,x) \big] - \mathcal{D}(Q(z|y,x)\|P(z)).
  	\end{align*}
  	From this equation, we can derive the training algorithm.
  \end{itemize}
  \bibliographystyle{plain}
  \bibliography{vae}  
\end{document}